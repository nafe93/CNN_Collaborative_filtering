{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'images', 'images.csv', 'styles', 'styles.csv']\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"D:/Data/Fashion\"\n",
    "print(os.listdir(DATASET_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6044: expected 10 fields, saw 11\\nSkipping line 6569: expected 10 fields, saw 11\\nSkipping line 7399: expected 10 fields, saw 11\\nSkipping line 7939: expected 10 fields, saw 11\\nSkipping line 9026: expected 10 fields, saw 11\\nSkipping line 10264: expected 10 fields, saw 11\\nSkipping line 10427: expected 10 fields, saw 11\\nSkipping line 10905: expected 10 fields, saw 11\\nSkipping line 11373: expected 10 fields, saw 11\\nSkipping line 11945: expected 10 fields, saw 11\\nSkipping line 14112: expected 10 fields, saw 11\\nSkipping line 14532: expected 10 fields, saw 11\\nSkipping line 15076: expected 10 fields, saw 12\\nSkipping line 29906: expected 10 fields, saw 11\\nSkipping line 31625: expected 10 fields, saw 11\\nSkipping line 33020: expected 10 fields, saw 11\\nSkipping line 35748: expected 10 fields, saw 11\\nSkipping line 35962: expected 10 fields, saw 11\\nSkipping line 37770: expected 10 fields, saw 11\\nSkipping line 38105: expected 10 fields, saw 11\\nSkipping line 38275: expected 10 fields, saw 11\\nSkipping line 38404: expected 10 fields, saw 12\\n'\n",
      "c:\\users\\alkzir\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>productDisplayName</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15970</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
       "      <td>15970.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39386</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Peter England Men Party Blue Jeans</td>\n",
       "      <td>39386.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59263</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Titan Women Silver Watch</td>\n",
       "      <td>59263.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21379</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Track Pants</td>\n",
       "      <td>Black</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Manchester United Men Solid Black Track Pants</td>\n",
       "      <td>21379.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53759</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Grey T-shirt</td>\n",
       "      <td>53759.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1855</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Inkfruit Mens Chain Reaction T-shirt</td>\n",
       "      <td>1855.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30805</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Green</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Ethnic</td>\n",
       "      <td>Fabindia Men Striped Green Shirt</td>\n",
       "      <td>30805.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26960</td>\n",
       "      <td>Women</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Purple</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Jealous 21 Women Purple Shirt</td>\n",
       "      <td>26960.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29114</td>\n",
       "      <td>Men</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Socks</td>\n",
       "      <td>Socks</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Pack of 3 Socks</td>\n",
       "      <td>29114.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30039</td>\n",
       "      <td>Men</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Black</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Skagen Men Black Watch</td>\n",
       "      <td>30039.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id gender masterCategory subCategory  articleType baseColour  season  \\\n",
       "0  15970    Men        Apparel     Topwear       Shirts  Navy Blue    Fall   \n",
       "1  39386    Men        Apparel  Bottomwear        Jeans       Blue  Summer   \n",
       "2  59263  Women    Accessories     Watches      Watches     Silver  Winter   \n",
       "3  21379    Men        Apparel  Bottomwear  Track Pants      Black    Fall   \n",
       "4  53759    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n",
       "5   1855    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n",
       "6  30805    Men        Apparel     Topwear       Shirts      Green  Summer   \n",
       "7  26960  Women        Apparel     Topwear       Shirts     Purple  Summer   \n",
       "8  29114    Men    Accessories       Socks        Socks  Navy Blue  Summer   \n",
       "9  30039    Men    Accessories     Watches      Watches      Black  Winter   \n",
       "\n",
       "     year   usage                             productDisplayName      image  \n",
       "0  2011.0  Casual               Turtle Check Men Navy Blue Shirt  15970.jpg  \n",
       "1  2012.0  Casual             Peter England Men Party Blue Jeans  39386.jpg  \n",
       "2  2016.0  Casual                       Titan Women Silver Watch  59263.jpg  \n",
       "3  2011.0  Casual  Manchester United Men Solid Black Track Pants  21379.jpg  \n",
       "4  2012.0  Casual                          Puma Men Grey T-shirt  53759.jpg  \n",
       "5  2011.0  Casual           Inkfruit Mens Chain Reaction T-shirt   1855.jpg  \n",
       "6  2012.0  Ethnic               Fabindia Men Striped Green Shirt  30805.jpg  \n",
       "7  2012.0  Casual                  Jealous 21 Women Purple Shirt  26960.jpg  \n",
       "8  2012.0  Casual                       Puma Men Pack of 3 Socks  29114.jpg  \n",
       "9  2016.0  Casual                         Skagen Men Black Watch  30039.jpg  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_PATH + \"/styles.csv\", error_bad_lines=False)\n",
    "new_df = df.dropna()\n",
    "new_df['image'] = new_df.apply(lambda row: str(row['id']) + \".jpg\", axis=1)\n",
    "new_df = new_df.reset_index(drop=True)\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### View image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figures(figures, nrows = 1, ncols=1,figsize=(8, 8)):\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows,figsize=figsize)\n",
    "    for ind,title in enumerate(figures):\n",
    "        axeslist.ravel()[ind].imshow(cv2.cvtColor(figures[title], cv2.COLOR_BGR2RGB))\n",
    "        axeslist.ravel()[ind].set_title(title)\n",
    "        axeslist.ravel()[ind].set_axis_off()\n",
    "    plt.tight_layout() # optional\n",
    "    \n",
    "def img_path(img):\n",
    "    return DATASET_PATH+\"/images/\"+img\n",
    "\n",
    "def load_image(img, resized_fac = 0.1):\n",
    "    img     = cv2.imread(img_path(img))\n",
    "    w, h, _ = img.shape\n",
    "    resized = cv2.resize(img, (int(h*resized_fac), int(w*resized_fac)), interpolation = cv2.INTER_AREA)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generation of a dictionary of (title, images)\n",
    "figures = {'im'+str(i): load_image(row.image) for i, row in new_df.sample(6).iterrows()}\n",
    "# plot of the images in a figure, with 2 rows and 3 columns\n",
    "plot_figures(figures, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### view distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15faf450f28>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x2880 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,40))\n",
    "new_df.articleType.value_counts().sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = new_df.articleType.value_counts().sort_values() / len(new_df)\n",
    "p_value = p_value.to_dict()\n",
    "p_value_array = [p_value[i] for i in new_df['articleType']]\n",
    "new_df['p_value_articleType'] = p_value_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>productDisplayName</th>\n",
       "      <th>image</th>\n",
       "      <th>p_value_articleType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15970</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
       "      <td>15970.jpg</td>\n",
       "      <td>0.072918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39386</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Peter England Men Party Blue Jeans</td>\n",
       "      <td>39386.jpg</td>\n",
       "      <td>0.013681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59263</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Titan Women Silver Watch</td>\n",
       "      <td>59263.jpg</td>\n",
       "      <td>0.057672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21379</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Track Pants</td>\n",
       "      <td>Black</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Manchester United Men Solid Black Track Pants</td>\n",
       "      <td>21379.jpg</td>\n",
       "      <td>0.006897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53759</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Grey T-shirt</td>\n",
       "      <td>53759.jpg</td>\n",
       "      <td>0.160310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44072</th>\n",
       "      <td>17036</td>\n",
       "      <td>Men</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Casual Shoes</td>\n",
       "      <td>White</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Gas Men Caddy Casual Shoe</td>\n",
       "      <td>17036.jpg</td>\n",
       "      <td>0.064546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44073</th>\n",
       "      <td>6461</td>\n",
       "      <td>Men</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Flip Flops</td>\n",
       "      <td>Flip Flops</td>\n",
       "      <td>Red</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Lotto Men's Soccer Track Flip Flop</td>\n",
       "      <td>6461.jpg</td>\n",
       "      <td>0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44074</th>\n",
       "      <td>18842</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Graphic Stellar Blue Tshirt</td>\n",
       "      <td>18842.jpg</td>\n",
       "      <td>0.160310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44075</th>\n",
       "      <td>46694</td>\n",
       "      <td>Women</td>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Fragrance</td>\n",
       "      <td>Perfume and Body Mist</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Rasasi Women Blue Lady Perfume</td>\n",
       "      <td>46694.jpg</td>\n",
       "      <td>0.013681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44076</th>\n",
       "      <td>51623</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Pink</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Fossil Women Pink Dial Chronograph Watch ES3050</td>\n",
       "      <td>51623.jpg</td>\n",
       "      <td>0.057672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44077 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id gender masterCategory subCategory            articleType  \\\n",
       "0      15970    Men        Apparel     Topwear                 Shirts   \n",
       "1      39386    Men        Apparel  Bottomwear                  Jeans   \n",
       "2      59263  Women    Accessories     Watches                Watches   \n",
       "3      21379    Men        Apparel  Bottomwear            Track Pants   \n",
       "4      53759    Men        Apparel     Topwear                Tshirts   \n",
       "...      ...    ...            ...         ...                    ...   \n",
       "44072  17036    Men       Footwear       Shoes           Casual Shoes   \n",
       "44073   6461    Men       Footwear  Flip Flops             Flip Flops   \n",
       "44074  18842    Men        Apparel     Topwear                Tshirts   \n",
       "44075  46694  Women  Personal Care   Fragrance  Perfume and Body Mist   \n",
       "44076  51623  Women    Accessories     Watches                Watches   \n",
       "\n",
       "      baseColour  season    year   usage  \\\n",
       "0      Navy Blue    Fall  2011.0  Casual   \n",
       "1           Blue  Summer  2012.0  Casual   \n",
       "2         Silver  Winter  2016.0  Casual   \n",
       "3          Black    Fall  2011.0  Casual   \n",
       "4           Grey  Summer  2012.0  Casual   \n",
       "...          ...     ...     ...     ...   \n",
       "44072      White  Summer  2013.0  Casual   \n",
       "44073        Red  Summer  2011.0  Casual   \n",
       "44074       Blue    Fall  2011.0  Casual   \n",
       "44075       Blue  Spring  2017.0  Casual   \n",
       "44076       Pink  Winter  2016.0  Casual   \n",
       "\n",
       "                                    productDisplayName      image  \\\n",
       "0                     Turtle Check Men Navy Blue Shirt  15970.jpg   \n",
       "1                   Peter England Men Party Blue Jeans  39386.jpg   \n",
       "2                             Titan Women Silver Watch  59263.jpg   \n",
       "3        Manchester United Men Solid Black Track Pants  21379.jpg   \n",
       "4                                Puma Men Grey T-shirt  53759.jpg   \n",
       "...                                                ...        ...   \n",
       "44072                        Gas Men Caddy Casual Shoe  17036.jpg   \n",
       "44073               Lotto Men's Soccer Track Flip Flop   6461.jpg   \n",
       "44074             Puma Men Graphic Stellar Blue Tshirt  18842.jpg   \n",
       "44075                   Rasasi Women Blue Lady Perfume  46694.jpg   \n",
       "44076  Fossil Women Pink Dial Chronograph Watch ES3050  51623.jpg   \n",
       "\n",
       "       p_value_articleType  \n",
       "0                 0.072918  \n",
       "1                 0.013681  \n",
       "2                 0.057672  \n",
       "3                 0.006897  \n",
       "4                 0.160310  \n",
       "...                    ...  \n",
       "44072             0.064546  \n",
       "44073             0.020736  \n",
       "44074             0.160310  \n",
       "44075             0.013681  \n",
       "44076             0.057672  \n",
       "\n",
       "[44077 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15fb0b26ef0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,7))\n",
    "sns.distplot(new_df.articleType.value_counts().sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### How we see it's look like Poisson distribution, then we choose random 5000 rows from our dataset to training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21252,\n",
       " 14314,\n",
       " 27981,\n",
       " 19681,\n",
       " 13284,\n",
       " 19415,\n",
       " 30821,\n",
       " 18786,\n",
       " 8972,\n",
       " 14631,\n",
       " 38941,\n",
       " 17187,\n",
       " 42694,\n",
       " 22366,\n",
       " 26340,\n",
       " 12008,\n",
       " 4301,\n",
       " 3888,\n",
       " 6020,\n",
       " 364,\n",
       " 43779,\n",
       " 33803,\n",
       " 31099,\n",
       " 15122,\n",
       " 37483,\n",
       " 40709,\n",
       " 1336,\n",
       " 4048,\n",
       " 18027,\n",
       " 5584,\n",
       " 7875,\n",
       " 34185,\n",
       " 30346,\n",
       " 31867,\n",
       " 37179,\n",
       " 41695,\n",
       " 23082,\n",
       " 29125,\n",
       " 2496,\n",
       " 30692,\n",
       " 26038,\n",
       " 3067,\n",
       " 6930,\n",
       " 11861,\n",
       " 38733,\n",
       " 37254,\n",
       " 14304,\n",
       " 8432,\n",
       " 3522,\n",
       " 26044,\n",
       " 32258,\n",
       " 22257,\n",
       " 31345,\n",
       " 18560,\n",
       " 7990,\n",
       " 761,\n",
       " 32799,\n",
       " 14159,\n",
       " 41201,\n",
       " 15897,\n",
       " 20916,\n",
       " 5869,\n",
       " 597,\n",
       " 24810,\n",
       " 1461,\n",
       " 29982,\n",
       " 20410,\n",
       " 43492,\n",
       " 21865,\n",
       " 12522,\n",
       " 15877,\n",
       " 25853,\n",
       " 2581,\n",
       " 43915,\n",
       " 10294,\n",
       " 36455,\n",
       " 32007,\n",
       " 7605,\n",
       " 33442,\n",
       " 21267,\n",
       " 38948,\n",
       " 24287,\n",
       " 474,\n",
       " 18595,\n",
       " 37990,\n",
       " 13439,\n",
       " 28203,\n",
       " 15587,\n",
       " 12255,\n",
       " 31688,\n",
       " 42989,\n",
       " 26910,\n",
       " 4012,\n",
       " 33773,\n",
       " 13178,\n",
       " 11039,\n",
       " 3454,\n",
       " 28564,\n",
       " 36798,\n",
       " 4310,\n",
       " 10730,\n",
       " 40685,\n",
       " 10074,\n",
       " 36581,\n",
       " 18375,\n",
       " 24838,\n",
       " 21520,\n",
       " 4221,\n",
       " 39289,\n",
       " 17199,\n",
       " 21735,\n",
       " 19545,\n",
       " 20304,\n",
       " 22207,\n",
       " 38492,\n",
       " 37550,\n",
       " 26546,\n",
       " 11321,\n",
       " 3355,\n",
       " 6614,\n",
       " 7039,\n",
       " 23797,\n",
       " 4133,\n",
       " 3058,\n",
       " 42992,\n",
       " 26119,\n",
       " 8423,\n",
       " 17158,\n",
       " 24217,\n",
       " 8014,\n",
       " 3581,\n",
       " 24871,\n",
       " 17170,\n",
       " 3937,\n",
       " 14473,\n",
       " 40428,\n",
       " 35906,\n",
       " 4912,\n",
       " 16602,\n",
       " 6585,\n",
       " 29769,\n",
       " 30376,\n",
       " 34763,\n",
       " 25203,\n",
       " 14849,\n",
       " 19666,\n",
       " 3275,\n",
       " 43094,\n",
       " 6289,\n",
       " 3768,\n",
       " 4425,\n",
       " 7638,\n",
       " 17330,\n",
       " 2342,\n",
       " 7698,\n",
       " 41489,\n",
       " 8245,\n",
       " 17727,\n",
       " 12747,\n",
       " 35277,\n",
       " 39804,\n",
       " 7831,\n",
       " 29601,\n",
       " 12952,\n",
       " 3596,\n",
       " 11529,\n",
       " 23146,\n",
       " 3311,\n",
       " 42591,\n",
       " 10012,\n",
       " 16481,\n",
       " 32175,\n",
       " 23874,\n",
       " 43935,\n",
       " 28842,\n",
       " 35728,\n",
       " 26443,\n",
       " 34140,\n",
       " 34338,\n",
       " 5677,\n",
       " 34711,\n",
       " 29169,\n",
       " 29240,\n",
       " 28257,\n",
       " 34512,\n",
       " 22809,\n",
       " 1938,\n",
       " 37352,\n",
       " 14373,\n",
       " 20438,\n",
       " 31608,\n",
       " 31123,\n",
       " 455,\n",
       " 14352,\n",
       " 38028,\n",
       " 33581,\n",
       " 20419,\n",
       " 25525,\n",
       " 15683,\n",
       " 1933,\n",
       " 1521,\n",
       " 27739,\n",
       " 34661,\n",
       " 26960,\n",
       " 22038,\n",
       " 38039,\n",
       " 7973,\n",
       " 17035,\n",
       " 38611,\n",
       " 27206,\n",
       " 20515,\n",
       " 43162,\n",
       " 22692,\n",
       " 31574,\n",
       " 8812,\n",
       " 21439,\n",
       " 15314,\n",
       " 34525,\n",
       " 41507,\n",
       " 17609,\n",
       " 33468,\n",
       " 34362,\n",
       " 29021,\n",
       " 18481,\n",
       " 35612,\n",
       " 24498,\n",
       " 39395,\n",
       " 3486,\n",
       " 32755,\n",
       " 7674,\n",
       " 14776,\n",
       " 10221,\n",
       " 13434,\n",
       " 34927,\n",
       " 2190,\n",
       " 28472,\n",
       " 32361,\n",
       " 7226,\n",
       " 7725,\n",
       " 40720,\n",
       " 19043,\n",
       " 923,\n",
       " 28592,\n",
       " 5430,\n",
       " 26829,\n",
       " 43228,\n",
       " 28,\n",
       " 19428,\n",
       " 4110,\n",
       " 29635,\n",
       " 32415,\n",
       " 35817,\n",
       " 11062,\n",
       " 35664,\n",
       " 32268,\n",
       " 43157,\n",
       " 8617,\n",
       " 40693,\n",
       " 10896,\n",
       " 10539,\n",
       " 34126,\n",
       " 19649,\n",
       " 16809,\n",
       " 6720,\n",
       " 16902,\n",
       " 23582,\n",
       " 25530,\n",
       " 38697,\n",
       " 24793,\n",
       " 8593,\n",
       " 18888,\n",
       " 6921,\n",
       " 40985,\n",
       " 2327,\n",
       " 31962,\n",
       " 23927,\n",
       " 33778,\n",
       " 35376,\n",
       " 30208,\n",
       " 13656,\n",
       " 14481,\n",
       " 13281,\n",
       " 3218,\n",
       " 13234,\n",
       " 31396,\n",
       " 35504,\n",
       " 24328,\n",
       " 6254,\n",
       " 9048,\n",
       " 24427,\n",
       " 10887,\n",
       " 18473,\n",
       " 43841,\n",
       " 19282,\n",
       " 15845,\n",
       " 17479,\n",
       " 5467,\n",
       " 4266,\n",
       " 43329,\n",
       " 8888,\n",
       " 13153,\n",
       " 117,\n",
       " 39191,\n",
       " 14914,\n",
       " 7982,\n",
       " 43221,\n",
       " 3485,\n",
       " 22662,\n",
       " 35597,\n",
       " 14289,\n",
       " 24087,\n",
       " 917,\n",
       " 2573,\n",
       " 20922,\n",
       " 11554,\n",
       " 523,\n",
       " 3863,\n",
       " 20232,\n",
       " 13077,\n",
       " 11026,\n",
       " 3343,\n",
       " 13384,\n",
       " 26544,\n",
       " 11177,\n",
       " 20042,\n",
       " 34845,\n",
       " 8025,\n",
       " 34765,\n",
       " 31854,\n",
       " 23941,\n",
       " 34447,\n",
       " 30921,\n",
       " 29932,\n",
       " 3610,\n",
       " 24335,\n",
       " 13625,\n",
       " 17148,\n",
       " 1078,\n",
       " 15050,\n",
       " 29549,\n",
       " 6339,\n",
       " 16130,\n",
       " 16634,\n",
       " 17282,\n",
       " 38801,\n",
       " 22013,\n",
       " 14149,\n",
       " 4031,\n",
       " 1399,\n",
       " 10921,\n",
       " 33667,\n",
       " 16995,\n",
       " 37317,\n",
       " 36747,\n",
       " 40941,\n",
       " 770,\n",
       " 16912,\n",
       " 26357,\n",
       " 36356,\n",
       " 239,\n",
       " 5088,\n",
       " 26320,\n",
       " 9071,\n",
       " 22730,\n",
       " 19740,\n",
       " 20748,\n",
       " 24063,\n",
       " 28347,\n",
       " 25593,\n",
       " 12256,\n",
       " 28937,\n",
       " 33517,\n",
       " 3368,\n",
       " 18303,\n",
       " 32821,\n",
       " 16575,\n",
       " 16658,\n",
       " 20299,\n",
       " 30775,\n",
       " 36795,\n",
       " 3363,\n",
       " 5797,\n",
       " 11511,\n",
       " 3767,\n",
       " 2086,\n",
       " 11557,\n",
       " 1894,\n",
       " 41311,\n",
       " 21729,\n",
       " 14613,\n",
       " 22731,\n",
       " 11344,\n",
       " 5190,\n",
       " 2682,\n",
       " 7634,\n",
       " 29263,\n",
       " 29562,\n",
       " 7739,\n",
       " 22895,\n",
       " 24176,\n",
       " 29774,\n",
       " 18308,\n",
       " 6851,\n",
       " 16407,\n",
       " 29963,\n",
       " 42665,\n",
       " 44008,\n",
       " 8122,\n",
       " 6247,\n",
       " 9785,\n",
       " 7947,\n",
       " 30583,\n",
       " 11308,\n",
       " 40264,\n",
       " 30193,\n",
       " 20357,\n",
       " 4303,\n",
       " 39266,\n",
       " 23836,\n",
       " 35158,\n",
       " 37787,\n",
       " 21498,\n",
       " 20694,\n",
       " 16986,\n",
       " 2135,\n",
       " 14881,\n",
       " 35717,\n",
       " 37919,\n",
       " 36886,\n",
       " 6111,\n",
       " 39004,\n",
       " 31952,\n",
       " 34152,\n",
       " 29416,\n",
       " 11181,\n",
       " 19583,\n",
       " 359,\n",
       " 10611,\n",
       " 43922,\n",
       " 23743,\n",
       " 8350,\n",
       " 31379,\n",
       " 2769,\n",
       " 30195,\n",
       " 26610,\n",
       " 3959,\n",
       " 43587,\n",
       " 2328,\n",
       " 11964,\n",
       " 4397,\n",
       " 34639,\n",
       " 35266,\n",
       " 27708,\n",
       " 8998,\n",
       " 28629,\n",
       " 22652,\n",
       " 42974,\n",
       " 19409,\n",
       " 35771,\n",
       " 31397,\n",
       " 17710,\n",
       " 4879,\n",
       " 31145,\n",
       " 35208,\n",
       " 14719,\n",
       " 12555,\n",
       " 34985,\n",
       " 17731,\n",
       " 6454,\n",
       " 6223,\n",
       " 19723,\n",
       " 16049,\n",
       " 12528,\n",
       " 33900,\n",
       " 32840,\n",
       " 11752,\n",
       " 34252,\n",
       " 34810,\n",
       " 39967,\n",
       " 25843,\n",
       " 11132,\n",
       " 11125,\n",
       " 30709,\n",
       " 19855,\n",
       " 22705,\n",
       " 23503,\n",
       " 8249,\n",
       " 14651,\n",
       " 43271,\n",
       " 25478,\n",
       " 21601,\n",
       " 28801,\n",
       " 265,\n",
       " 11012,\n",
       " 40829,\n",
       " 4461,\n",
       " 33876,\n",
       " 18290,\n",
       " 39248,\n",
       " 27791,\n",
       " 15357,\n",
       " 27138,\n",
       " 30763,\n",
       " 20489,\n",
       " 4220,\n",
       " 33052,\n",
       " 30713,\n",
       " 1090,\n",
       " 5938,\n",
       " 38339,\n",
       " 29810,\n",
       " 4092,\n",
       " 41245,\n",
       " 31468,\n",
       " 30157,\n",
       " 18163,\n",
       " 23259,\n",
       " 25774,\n",
       " 33464,\n",
       " 40525,\n",
       " 34335,\n",
       " 30981,\n",
       " 40454,\n",
       " 40222,\n",
       " 701,\n",
       " 6330,\n",
       " 21397,\n",
       " 14550,\n",
       " 41209,\n",
       " 21575,\n",
       " 38670,\n",
       " 34089,\n",
       " 6681,\n",
       " 25773,\n",
       " 2889,\n",
       " 6525,\n",
       " 10449,\n",
       " 15286,\n",
       " 41263,\n",
       " 14872,\n",
       " 2497,\n",
       " 19095,\n",
       " 11408,\n",
       " 9973,\n",
       " 15634,\n",
       " 20261,\n",
       " 18008,\n",
       " 11123,\n",
       " 20927,\n",
       " 36004,\n",
       " 36586,\n",
       " 29409,\n",
       " 6368,\n",
       " 10969,\n",
       " 12881,\n",
       " 19926,\n",
       " 24086,\n",
       " 24277,\n",
       " 37309,\n",
       " 42286,\n",
       " 30024,\n",
       " 42070,\n",
       " 4541,\n",
       " 31826,\n",
       " 21725,\n",
       " 17453,\n",
       " 38411,\n",
       " 43591,\n",
       " 14362,\n",
       " 41167,\n",
       " 9552,\n",
       " 32526,\n",
       " 7214,\n",
       " 24705,\n",
       " 43875,\n",
       " 17310,\n",
       " 43381,\n",
       " 38759,\n",
       " 29616,\n",
       " 24791,\n",
       " 14798,\n",
       " 20078,\n",
       " 34253,\n",
       " 32936,\n",
       " 12896,\n",
       " 9094,\n",
       " 6346,\n",
       " 24017,\n",
       " 4208,\n",
       " 27232,\n",
       " 14842,\n",
       " 26014,\n",
       " 10092,\n",
       " 40572,\n",
       " 20800,\n",
       " 5212,\n",
       " 20590,\n",
       " 31104,\n",
       " 27904,\n",
       " 18683,\n",
       " 12774,\n",
       " 17810,\n",
       " 17931,\n",
       " 5563,\n",
       " 42326,\n",
       " 33377,\n",
       " 2802,\n",
       " 25743,\n",
       " 2203,\n",
       " 9776,\n",
       " 25751,\n",
       " 7334,\n",
       " 9187,\n",
       " 19199,\n",
       " 32024,\n",
       " 5984,\n",
       " 19453,\n",
       " 26028,\n",
       " 19022,\n",
       " 30581,\n",
       " 12297,\n",
       " 25344,\n",
       " 28959,\n",
       " 20496,\n",
       " 37269,\n",
       " 28933,\n",
       " 35587,\n",
       " 41327,\n",
       " 38786,\n",
       " 42674,\n",
       " 17442,\n",
       " 26022,\n",
       " 143,\n",
       " 20963,\n",
       " 15159,\n",
       " 2495,\n",
       " 29877,\n",
       " 5631,\n",
       " 5591,\n",
       " 21263,\n",
       " 30757,\n",
       " 34599,\n",
       " 28080,\n",
       " 21423,\n",
       " 19231,\n",
       " 24773,\n",
       " 23358,\n",
       " 4082,\n",
       " 42203,\n",
       " 43088,\n",
       " 25134,\n",
       " 4005,\n",
       " 3255,\n",
       " 9013,\n",
       " 28079,\n",
       " 41186,\n",
       " 20516,\n",
       " 28957,\n",
       " 22806,\n",
       " 40608,\n",
       " 29918,\n",
       " 10123,\n",
       " 25387,\n",
       " 38321,\n",
       " 32674,\n",
       " 18745,\n",
       " 9850,\n",
       " 113,\n",
       " 24832,\n",
       " 11568,\n",
       " 5822,\n",
       " 1523,\n",
       " 22393,\n",
       " 26555,\n",
       " 20139,\n",
       " 20494,\n",
       " 30543,\n",
       " 7387,\n",
       " 16880,\n",
       " 35637,\n",
       " 40734,\n",
       " 13060,\n",
       " 29944,\n",
       " 20837,\n",
       " 19872,\n",
       " 41170,\n",
       " 223,\n",
       " 11247,\n",
       " 9311,\n",
       " 22698,\n",
       " 27665,\n",
       " 5859,\n",
       " 33217,\n",
       " 43478,\n",
       " 40391,\n",
       " 8660,\n",
       " 7465,\n",
       " 16488,\n",
       " 2551,\n",
       " 15723,\n",
       " 22931,\n",
       " 6838,\n",
       " 12209,\n",
       " 25277,\n",
       " 6752,\n",
       " 27370,\n",
       " 21348,\n",
       " 12517,\n",
       " 14112,\n",
       " 35718,\n",
       " 2989,\n",
       " 33196,\n",
       " 12700,\n",
       " 34997,\n",
       " 35507,\n",
       " 27936,\n",
       " 30423,\n",
       " 7218,\n",
       " 31704,\n",
       " 38485,\n",
       " 31155,\n",
       " 8732,\n",
       " 43806,\n",
       " 40823,\n",
       " 210,\n",
       " 2369,\n",
       " 41157,\n",
       " 33106,\n",
       " 9386,\n",
       " 1066,\n",
       " 31874,\n",
       " 12321,\n",
       " 43545,\n",
       " 35193,\n",
       " 43509,\n",
       " 30287,\n",
       " 40361,\n",
       " 19565,\n",
       " 29658,\n",
       " 13572,\n",
       " 15087,\n",
       " 11402,\n",
       " 26055,\n",
       " 20836,\n",
       " 17690,\n",
       " 41774,\n",
       " 27623,\n",
       " 43382,\n",
       " 439,\n",
       " 28766,\n",
       " 3004,\n",
       " 36306,\n",
       " 17493,\n",
       " 43087,\n",
       " 2913,\n",
       " 32357,\n",
       " 40359,\n",
       " 5390,\n",
       " 33161,\n",
       " 6591,\n",
       " 40486,\n",
       " 26278,\n",
       " 17027,\n",
       " 1454,\n",
       " 28286,\n",
       " 34687,\n",
       " 4339,\n",
       " 35012,\n",
       " 20603,\n",
       " 22880,\n",
       " 16105,\n",
       " 34032,\n",
       " 24155,\n",
       " 15308,\n",
       " 6175,\n",
       " 30112,\n",
       " 30917,\n",
       " 43851,\n",
       " 31605,\n",
       " 36521,\n",
       " 43352,\n",
       " 30508,\n",
       " 38993,\n",
       " 30507,\n",
       " 17772,\n",
       " 21156,\n",
       " 31597,\n",
       " 22733,\n",
       " 37506,\n",
       " 16822,\n",
       " 16521,\n",
       " 2050,\n",
       " 16873,\n",
       " 22048,\n",
       " 30235,\n",
       " 28762,\n",
       " 5051,\n",
       " 28355,\n",
       " 33398,\n",
       " 32883,\n",
       " 29172,\n",
       " 43855,\n",
       " 20643,\n",
       " 16737,\n",
       " 5704,\n",
       " 19259,\n",
       " 14763,\n",
       " 29177,\n",
       " 10846,\n",
       " 42716,\n",
       " 43119,\n",
       " 2487,\n",
       " 3740,\n",
       " 12604,\n",
       " 35685,\n",
       " 2755,\n",
       " 28939,\n",
       " 567,\n",
       " 20298,\n",
       " 27902,\n",
       " 1709,\n",
       " 9532,\n",
       " 9937,\n",
       " 17080,\n",
       " 3107,\n",
       " 13467,\n",
       " 4391,\n",
       " 31778,\n",
       " 16363,\n",
       " 21292,\n",
       " 13436,\n",
       " 6834,\n",
       " 35347,\n",
       " 44044,\n",
       " 631,\n",
       " 16213,\n",
       " 12922,\n",
       " 2017,\n",
       " 23780,\n",
       " 17638,\n",
       " 39916,\n",
       " 10967,\n",
       " 37997,\n",
       " 35122,\n",
       " 29699,\n",
       " 18538,\n",
       " 21120,\n",
       " 17579,\n",
       " 41348,\n",
       " 38101,\n",
       " 25739,\n",
       " 36247,\n",
       " 38736,\n",
       " 9663,\n",
       " 37014,\n",
       " 14045,\n",
       " 912,\n",
       " 26045,\n",
       " 211,\n",
       " 30109,\n",
       " 7305,\n",
       " 43305,\n",
       " 25915,\n",
       " 25479,\n",
       " 8336,\n",
       " 16316,\n",
       " 34333,\n",
       " 16619,\n",
       " 33070,\n",
       " 16147,\n",
       " 31793,\n",
       " 16843,\n",
       " 22768,\n",
       " 14247,\n",
       " 42265,\n",
       " 34235,\n",
       " 6331,\n",
       " 14928,\n",
       " 43417,\n",
       " 21503,\n",
       " 31278,\n",
       " 39733,\n",
       " 977,\n",
       " 8768,\n",
       " 22041,\n",
       " 23231,\n",
       " 841,\n",
       " 37724,\n",
       " 16901,\n",
       " 39049,\n",
       " 19993,\n",
       " 42030,\n",
       " 23681,\n",
       " 27243,\n",
       " 39747,\n",
       " 36619,\n",
       " 39651,\n",
       " 25509,\n",
       " 31838,\n",
       " 18639,\n",
       " 1345,\n",
       " 42169,\n",
       " 37546,\n",
       " 22562,\n",
       " 37272,\n",
       " 12025,\n",
       " 30618,\n",
       " 14701,\n",
       " 1342,\n",
       " 10525,\n",
       " 43377,\n",
       " 17997,\n",
       " 11160,\n",
       " 35340,\n",
       " 908,\n",
       " 4151,\n",
       " 27149,\n",
       " 33868,\n",
       " 36334,\n",
       " 38256,\n",
       " 36338,\n",
       " 17047,\n",
       " 1550,\n",
       " 18516,\n",
       " 20919,\n",
       " 32635,\n",
       " 39670,\n",
       " 25569,\n",
       " 28448,\n",
       " 37038,\n",
       " 40186,\n",
       " 2169,\n",
       " 23695,\n",
       " 42632,\n",
       " 9316,\n",
       " 26136,\n",
       " 38975,\n",
       " 7152,\n",
       " 11907,\n",
       " 16409,\n",
       " 19314,\n",
       " 20714,\n",
       " 30807,\n",
       " 38822,\n",
       " 22566,\n",
       " 16161,\n",
       " 31918,\n",
       " 26814,\n",
       " 667,\n",
       " 35262,\n",
       " 21381,\n",
       " 20658,\n",
       " 34837,\n",
       " 30091,\n",
       " 41819,\n",
       " 5054,\n",
       " 20627,\n",
       " 33978,\n",
       " 43083,\n",
       " 3545,\n",
       " 7118,\n",
       " 21588,\n",
       " 3983,\n",
       " 34792,\n",
       " 1409,\n",
       " 18001,\n",
       " 23355,\n",
       " 30847,\n",
       " 13010,\n",
       " 35990,\n",
       " 15376,\n",
       " 5946,\n",
       " 6972,\n",
       " 10638,\n",
       " 25180,\n",
       " 32372,\n",
       " 43584,\n",
       " 10731,\n",
       " 34696,\n",
       " 4449,\n",
       " 22461,\n",
       " 32315,\n",
       " 40195,\n",
       " 9455,\n",
       " 9554,\n",
       " 40761,\n",
       " 34405,\n",
       " 27691,\n",
       " 26661,\n",
       " 42021,\n",
       " 6759,\n",
       " 34054,\n",
       " 840,\n",
       " 26161,\n",
       " 12058,\n",
       " 20054,\n",
       " 27387,\n",
       " 11465,\n",
       " 899,\n",
       " 36218,\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.random.choice(len(new_df), size=(len(new_df)), replace=False)\n",
    "list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>productDisplayName</th>\n",
       "      <th>image</th>\n",
       "      <th>p_value_articleType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21252</th>\n",
       "      <td>58958</td>\n",
       "      <td>Men</td>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Fragrance</td>\n",
       "      <td>Deodorant</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Denim Original Men DeoMax Body Spray</td>\n",
       "      <td>58958.jpg</td>\n",
       "      <td>0.007873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14314</th>\n",
       "      <td>37185</td>\n",
       "      <td>Women</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Lime Green</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>United Colors of Benetton Women Lime Green Top</td>\n",
       "      <td>37185.jpg</td>\n",
       "      <td>0.039975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27981</th>\n",
       "      <td>28519</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Track Pants</td>\n",
       "      <td>Black</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nike Men Breakline Black Track Pants</td>\n",
       "      <td>28519.jpg</td>\n",
       "      <td>0.006897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19681</th>\n",
       "      <td>57972</td>\n",
       "      <td>Women</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Saree</td>\n",
       "      <td>Sarees</td>\n",
       "      <td>Multi</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Ethnic</td>\n",
       "      <td>Prafful Women Multi Coloured Sari</td>\n",
       "      <td>57972.jpg</td>\n",
       "      <td>0.009688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13284</th>\n",
       "      <td>25423</td>\n",
       "      <td>Men</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Sports Shoes</td>\n",
       "      <td>Black</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nike Men Air Profusion Black Sports Shoes</td>\n",
       "      <td>25423.jpg</td>\n",
       "      <td>0.045738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>4214</td>\n",
       "      <td>Girls</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Disney Kids Girl's Yellow Summer Fun Fair Kids...</td>\n",
       "      <td>4214.jpg</td>\n",
       "      <td>0.160310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33081</th>\n",
       "      <td>32203</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Kurtas</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Ethnic</td>\n",
       "      <td>Fabindia Men Printed Blue Kurta</td>\n",
       "      <td>32203.jpg</td>\n",
       "      <td>0.041836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32802</th>\n",
       "      <td>25639</td>\n",
       "      <td>Men</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>White</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Fastrack Men White Dial Watch N747PL01</td>\n",
       "      <td>25639.jpg</td>\n",
       "      <td>0.057672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>56303</td>\n",
       "      <td>Women</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Innerwear</td>\n",
       "      <td>Briefs</td>\n",
       "      <td>Rose</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Biara Women Rose Briefs</td>\n",
       "      <td>56303.jpg</td>\n",
       "      <td>0.019216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16199</th>\n",
       "      <td>48473</td>\n",
       "      <td>Women</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Dress</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>French Connection Women Blue Dress</td>\n",
       "      <td>48473.jpg</td>\n",
       "      <td>0.010527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44077 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id gender masterCategory subCategory   articleType  baseColour  \\\n",
       "21252  58958    Men  Personal Care   Fragrance     Deodorant        Blue   \n",
       "14314  37185  Women        Apparel     Topwear          Tops  Lime Green   \n",
       "27981  28519    Men        Apparel  Bottomwear   Track Pants       Black   \n",
       "19681  57972  Women        Apparel       Saree        Sarees       Multi   \n",
       "13284  25423    Men       Footwear       Shoes  Sports Shoes       Black   \n",
       "...      ...    ...            ...         ...           ...         ...   \n",
       "12334   4214  Girls        Apparel     Topwear       Tshirts      Yellow   \n",
       "33081  32203    Men        Apparel     Topwear        Kurtas        Blue   \n",
       "32802  25639    Men    Accessories     Watches       Watches       White   \n",
       "9101   56303  Women        Apparel   Innerwear        Briefs        Rose   \n",
       "16199  48473  Women        Apparel       Dress       Dresses        Blue   \n",
       "\n",
       "       season    year   usage  \\\n",
       "21252  Spring  2017.0  Casual   \n",
       "14314  Summer  2012.0  Casual   \n",
       "27981  Summer  2012.0  Sports   \n",
       "19681    Fall  2012.0  Ethnic   \n",
       "13284  Summer  2012.0  Sports   \n",
       "...       ...     ...     ...   \n",
       "12334  Summer  2011.0  Casual   \n",
       "33081  Summer  2012.0  Ethnic   \n",
       "32802  Winter  2016.0  Casual   \n",
       "9101   Winter  2015.0  Casual   \n",
       "16199  Winter  2011.0  Casual   \n",
       "\n",
       "                                      productDisplayName      image  \\\n",
       "21252               Denim Original Men DeoMax Body Spray  58958.jpg   \n",
       "14314     United Colors of Benetton Women Lime Green Top  37185.jpg   \n",
       "27981               Nike Men Breakline Black Track Pants  28519.jpg   \n",
       "19681                  Prafful Women Multi Coloured Sari  57972.jpg   \n",
       "13284          Nike Men Air Profusion Black Sports Shoes  25423.jpg   \n",
       "...                                                  ...        ...   \n",
       "12334  Disney Kids Girl's Yellow Summer Fun Fair Kids...   4214.jpg   \n",
       "33081                    Fabindia Men Printed Blue Kurta  32203.jpg   \n",
       "32802             Fastrack Men White Dial Watch N747PL01  25639.jpg   \n",
       "9101                             Biara Women Rose Briefs  56303.jpg   \n",
       "16199                 French Connection Women Blue Dress  48473.jpg   \n",
       "\n",
       "       p_value_articleType  \n",
       "21252             0.007873  \n",
       "14314             0.039975  \n",
       "27981             0.006897  \n",
       "19681             0.009688  \n",
       "13284             0.045738  \n",
       "...                    ...  \n",
       "12334             0.160310  \n",
       "33081             0.041836  \n",
       "32802             0.057672  \n",
       "9101              0.019216  \n",
       "16199             0.010527  \n",
       "\n",
       "[44077 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = new_df.loc[s]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15fb0c93d68>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,20))\n",
    "dataset.articleType.value_counts().sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15fb58d1828>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,7))\n",
    "sns.distplot(dataset.articleType.value_counts().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>productDisplayName</th>\n",
       "      <th>image</th>\n",
       "      <th>p_value_articleType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21252</td>\n",
       "      <td>58958</td>\n",
       "      <td>Men</td>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Fragrance</td>\n",
       "      <td>Deodorant</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Denim Original Men DeoMax Body Spray</td>\n",
       "      <td>58958.jpg</td>\n",
       "      <td>0.007873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14314</td>\n",
       "      <td>37185</td>\n",
       "      <td>Women</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Lime Green</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>United Colors of Benetton Women Lime Green Top</td>\n",
       "      <td>37185.jpg</td>\n",
       "      <td>0.039975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27981</td>\n",
       "      <td>28519</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Track Pants</td>\n",
       "      <td>Black</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nike Men Breakline Black Track Pants</td>\n",
       "      <td>28519.jpg</td>\n",
       "      <td>0.006897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19681</td>\n",
       "      <td>57972</td>\n",
       "      <td>Women</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Saree</td>\n",
       "      <td>Sarees</td>\n",
       "      <td>Multi</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Ethnic</td>\n",
       "      <td>Prafful Women Multi Coloured Sari</td>\n",
       "      <td>57972.jpg</td>\n",
       "      <td>0.009688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13284</td>\n",
       "      <td>25423</td>\n",
       "      <td>Men</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Sports Shoes</td>\n",
       "      <td>Black</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nike Men Air Profusion Black Sports Shoes</td>\n",
       "      <td>25423.jpg</td>\n",
       "      <td>0.045738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44072</th>\n",
       "      <td>12334</td>\n",
       "      <td>4214</td>\n",
       "      <td>Girls</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Disney Kids Girl's Yellow Summer Fun Fair Kids...</td>\n",
       "      <td>4214.jpg</td>\n",
       "      <td>0.160310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44073</th>\n",
       "      <td>33081</td>\n",
       "      <td>32203</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Kurtas</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Ethnic</td>\n",
       "      <td>Fabindia Men Printed Blue Kurta</td>\n",
       "      <td>32203.jpg</td>\n",
       "      <td>0.041836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44074</th>\n",
       "      <td>32802</td>\n",
       "      <td>25639</td>\n",
       "      <td>Men</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>White</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Fastrack Men White Dial Watch N747PL01</td>\n",
       "      <td>25639.jpg</td>\n",
       "      <td>0.057672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44075</th>\n",
       "      <td>9101</td>\n",
       "      <td>56303</td>\n",
       "      <td>Women</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Innerwear</td>\n",
       "      <td>Briefs</td>\n",
       "      <td>Rose</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Biara Women Rose Briefs</td>\n",
       "      <td>56303.jpg</td>\n",
       "      <td>0.019216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44076</th>\n",
       "      <td>16199</td>\n",
       "      <td>48473</td>\n",
       "      <td>Women</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Dress</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>French Connection Women Blue Dress</td>\n",
       "      <td>48473.jpg</td>\n",
       "      <td>0.010527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44077 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index     id gender masterCategory subCategory   articleType  \\\n",
       "0      21252  58958    Men  Personal Care   Fragrance     Deodorant   \n",
       "1      14314  37185  Women        Apparel     Topwear          Tops   \n",
       "2      27981  28519    Men        Apparel  Bottomwear   Track Pants   \n",
       "3      19681  57972  Women        Apparel       Saree        Sarees   \n",
       "4      13284  25423    Men       Footwear       Shoes  Sports Shoes   \n",
       "...      ...    ...    ...            ...         ...           ...   \n",
       "44072  12334   4214  Girls        Apparel     Topwear       Tshirts   \n",
       "44073  33081  32203    Men        Apparel     Topwear        Kurtas   \n",
       "44074  32802  25639    Men    Accessories     Watches       Watches   \n",
       "44075   9101  56303  Women        Apparel   Innerwear        Briefs   \n",
       "44076  16199  48473  Women        Apparel       Dress       Dresses   \n",
       "\n",
       "       baseColour  season    year   usage  \\\n",
       "0            Blue  Spring  2017.0  Casual   \n",
       "1      Lime Green  Summer  2012.0  Casual   \n",
       "2           Black  Summer  2012.0  Sports   \n",
       "3           Multi    Fall  2012.0  Ethnic   \n",
       "4           Black  Summer  2012.0  Sports   \n",
       "...           ...     ...     ...     ...   \n",
       "44072      Yellow  Summer  2011.0  Casual   \n",
       "44073        Blue  Summer  2012.0  Ethnic   \n",
       "44074       White  Winter  2016.0  Casual   \n",
       "44075        Rose  Winter  2015.0  Casual   \n",
       "44076        Blue  Winter  2011.0  Casual   \n",
       "\n",
       "                                      productDisplayName      image  \\\n",
       "0                   Denim Original Men DeoMax Body Spray  58958.jpg   \n",
       "1         United Colors of Benetton Women Lime Green Top  37185.jpg   \n",
       "2                   Nike Men Breakline Black Track Pants  28519.jpg   \n",
       "3                      Prafful Women Multi Coloured Sari  57972.jpg   \n",
       "4              Nike Men Air Profusion Black Sports Shoes  25423.jpg   \n",
       "...                                                  ...        ...   \n",
       "44072  Disney Kids Girl's Yellow Summer Fun Fair Kids...   4214.jpg   \n",
       "44073                    Fabindia Men Printed Blue Kurta  32203.jpg   \n",
       "44074             Fastrack Men White Dial Watch N747PL01  25639.jpg   \n",
       "44075                            Biara Women Rose Briefs  56303.jpg   \n",
       "44076                 French Connection Women Blue Dress  48473.jpg   \n",
       "\n",
       "       p_value_articleType  \n",
       "0                 0.007873  \n",
       "1                 0.039975  \n",
       "2                 0.006897  \n",
       "3                 0.009688  \n",
       "4                 0.045738  \n",
       "...                    ...  \n",
       "44072             0.160310  \n",
       "44073             0.041836  \n",
       "44074             0.057672  \n",
       "44075             0.019216  \n",
       "44076             0.010527  \n",
       "\n",
       "[44077 rows x 13 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_name = dataset.articleType.value_counts().index\n",
    "article_type_dict = {}\n",
    "for i in range(len(list_name)):\n",
    "    article_type_dict[list_name[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tshirts': 0,\n",
       " 'Shirts': 1,\n",
       " 'Casual Shoes': 2,\n",
       " 'Watches': 3,\n",
       " 'Sports Shoes': 4,\n",
       " 'Kurtas': 5,\n",
       " 'Tops': 6,\n",
       " 'Handbags': 7,\n",
       " 'Heels': 8,\n",
       " 'Sunglasses': 9,\n",
       " 'Wallets': 10,\n",
       " 'Flip Flops': 11,\n",
       " 'Sandals': 12,\n",
       " 'Briefs': 13,\n",
       " 'Belts': 14,\n",
       " 'Backpacks': 15,\n",
       " 'Socks': 16,\n",
       " 'Formal Shoes': 17,\n",
       " 'Perfume and Body Mist': 18,\n",
       " 'Jeans': 19,\n",
       " 'Shorts': 20,\n",
       " 'Trousers': 21,\n",
       " 'Flats': 22,\n",
       " 'Bra': 23,\n",
       " 'Dresses': 24,\n",
       " 'Sarees': 25,\n",
       " 'Earrings': 26,\n",
       " 'Deodorant': 27,\n",
       " 'Track Pants': 28,\n",
       " 'Clutches': 29,\n",
       " 'Sweatshirts': 30,\n",
       " 'Caps': 31,\n",
       " 'Nail Polish': 32,\n",
       " 'Sweaters': 33,\n",
       " 'Ties': 34,\n",
       " 'Lipstick': 35,\n",
       " 'Jackets': 36,\n",
       " 'Innerwear Vests': 37,\n",
       " 'Kurtis': 38,\n",
       " 'Tunics': 39,\n",
       " 'Nightdress': 40,\n",
       " 'Leggings': 41,\n",
       " 'Pendant': 42,\n",
       " 'Capris': 43,\n",
       " 'Necklace and Chains': 44,\n",
       " 'Night suits': 45,\n",
       " 'Trunk': 46,\n",
       " 'Skirts': 47,\n",
       " 'Scarves': 48,\n",
       " 'Ring': 49,\n",
       " 'Dupatta': 50,\n",
       " 'Lip Gloss': 51,\n",
       " 'Cufflinks': 52,\n",
       " 'Accessory Gift Set': 53,\n",
       " 'Kurta Sets': 54,\n",
       " 'Kajal and Eyeliner': 55,\n",
       " 'Free Gifts': 56,\n",
       " 'Stoles': 57,\n",
       " 'Duffel Bag': 58,\n",
       " 'Bangle': 59,\n",
       " 'Laptop Bag': 60,\n",
       " 'Foundation and Primer': 61,\n",
       " 'Sports Sandals': 62,\n",
       " 'Bracelet': 63,\n",
       " 'Lounge Pants': 64,\n",
       " 'Jewellery Set': 65,\n",
       " 'Fragrance Gift Set': 66,\n",
       " 'Boxers': 67,\n",
       " 'Mobile Pouch': 68,\n",
       " 'Face Moisturisers': 69,\n",
       " 'Lip Liner': 70,\n",
       " 'Messenger Bag': 71,\n",
       " 'Suspenders': 72,\n",
       " 'Compact': 73,\n",
       " 'Camisoles': 74,\n",
       " 'Mufflers': 75,\n",
       " 'Patiala': 76,\n",
       " 'Highlighter and Blush': 77,\n",
       " 'Jeggings': 78,\n",
       " 'Lounge Shorts': 79,\n",
       " 'Stockings': 80,\n",
       " 'Eyeshadow': 81,\n",
       " 'Salwar': 82,\n",
       " 'Churidar': 83,\n",
       " 'Tracksuits': 84,\n",
       " 'Gloves': 85,\n",
       " 'Hair Colour': 86,\n",
       " 'Swimwear': 87,\n",
       " 'Bath Robe': 88,\n",
       " 'Waist Pouch': 89,\n",
       " 'Face Wash and Cleanser': 90,\n",
       " 'Jumpsuit': 91,\n",
       " 'Travel Accessory': 92,\n",
       " 'Sunscreen': 93,\n",
       " 'Waistcoat': 94,\n",
       " 'Baby Dolls': 95,\n",
       " 'Basketballs': 96,\n",
       " 'Booties': 97,\n",
       " 'Rompers': 98,\n",
       " 'Mascara': 99,\n",
       " 'Water Bottle': 100,\n",
       " 'Rucksacks': 101,\n",
       " 'Concealer': 102,\n",
       " 'Mask and Peel': 103,\n",
       " 'Tights': 104,\n",
       " 'Shapewear': 105,\n",
       " 'Footballs': 106,\n",
       " 'Blazers': 107,\n",
       " 'Clothing Set': 108,\n",
       " 'Wristbands': 109,\n",
       " 'Salwar and Dupatta': 110,\n",
       " 'Headband': 111,\n",
       " 'Lip Care': 112,\n",
       " 'Rain Jacket': 113,\n",
       " 'Nail Essentials': 114,\n",
       " 'Shrug': 115,\n",
       " 'Umbrellas': 116,\n",
       " 'Nehru Jackets': 117,\n",
       " 'Lip Plumper': 118,\n",
       " 'Robe': 119,\n",
       " 'Lehenga Choli': 120,\n",
       " 'Eye Cream': 121,\n",
       " 'Face Scrub and Exfoliator': 122,\n",
       " 'Hat': 123,\n",
       " 'Body Lotion': 124,\n",
       " 'Lounge Tshirts': 125,\n",
       " 'Tablet Sleeve': 126,\n",
       " 'Beauty Accessory': 127,\n",
       " 'Trolley Bag': 128,\n",
       " 'Shoe Accessories': 129,\n",
       " 'Key chain': 130,\n",
       " 'Toner': 131,\n",
       " 'Face Serum and Gel': 132,\n",
       " 'Makeup Remover': 133,\n",
       " 'Ties and Cufflinks': 134,\n",
       " 'Ipad': 135,\n",
       " 'Body Wash and Scrub': 136,\n",
       " 'Suits': 137,\n",
       " 'Mens Grooming Kit': 138,\n",
       " 'Shoe Laces': 139,\n",
       " 'Hair Accessory': 140,\n",
       " 'Cushion Covers': 141}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_type_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_type_code = [article_type_dict[i] for i in dataset['articleType']]\n",
    "dataset['code'] = article_type_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cut dataset to intersting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleType</th>\n",
       "      <th>code</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21252</th>\n",
       "      <td>Deodorant</td>\n",
       "      <td>27</td>\n",
       "      <td>58958.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14314</th>\n",
       "      <td>Tops</td>\n",
       "      <td>6</td>\n",
       "      <td>37185.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27981</th>\n",
       "      <td>Track Pants</td>\n",
       "      <td>28</td>\n",
       "      <td>28519.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19681</th>\n",
       "      <td>Sarees</td>\n",
       "      <td>25</td>\n",
       "      <td>57972.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13284</th>\n",
       "      <td>Sports Shoes</td>\n",
       "      <td>4</td>\n",
       "      <td>25423.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>Tshirts</td>\n",
       "      <td>0</td>\n",
       "      <td>4214.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33081</th>\n",
       "      <td>Kurtas</td>\n",
       "      <td>5</td>\n",
       "      <td>32203.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32802</th>\n",
       "      <td>Watches</td>\n",
       "      <td>3</td>\n",
       "      <td>25639.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>Briefs</td>\n",
       "      <td>13</td>\n",
       "      <td>56303.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16199</th>\n",
       "      <td>Dresses</td>\n",
       "      <td>24</td>\n",
       "      <td>48473.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44077 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        articleType  code      image\n",
       "21252     Deodorant    27  58958.jpg\n",
       "14314          Tops     6  37185.jpg\n",
       "27981   Track Pants    28  28519.jpg\n",
       "19681        Sarees    25  57972.jpg\n",
       "13284  Sports Shoes     4  25423.jpg\n",
       "...             ...   ...        ...\n",
       "12334       Tshirts     0   4214.jpg\n",
       "33081        Kurtas     5  32203.jpg\n",
       "32802       Watches     3  25639.jpg\n",
       "9101         Briefs    13  56303.jpg\n",
       "16199       Dresses    24  48473.jpg\n",
       "\n",
       "[44077 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interesting = dataset[['articleType', 'code', 'image']]\n",
    "interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Put image in one file to upload it to pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found = []\n",
    "def copy_image_to(src, dst):\n",
    "    try:\n",
    "        shutil.copy(src,dst)\n",
    "    except IOError as e:\n",
    "        print(e)\n",
    "        error = src.split('/')[4]\n",
    "        not_found.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44077"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(np.array(interesting['image'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11225.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11286.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10796.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11282.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10738.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11017.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10371.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10723.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10314.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10606.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10574.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10579.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11113.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11132.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11036.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10677.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11283.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10995.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11084.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11030.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10457.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10767.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11228.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11051.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10377.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11221.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10393.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11130.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11304.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11016.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11100.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10362.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10331.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11424.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10410.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10348.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10993.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11115.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10308.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11047.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10661.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11107.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10498.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11099.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11234.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10595.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10754.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10752.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11207.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10303.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11196.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11008.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11373.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11312.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10794.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10347.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10379.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11199.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10599.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10490.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10665.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10454.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10882.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11380.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10325.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10351.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10998.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11308.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11285.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11363.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11059.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10664.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11374.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10398.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10471.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10342.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10335.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10367.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11202.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10594.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11305.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10812.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10300.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10662.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11357.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10327.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10809.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10316.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10345.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11353.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10330.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10313.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10397.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11416.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10320.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11123.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10383.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10376.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10381.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10990.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11222.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10373.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10724.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10396.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11365.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11101.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10365.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11431.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10301.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11217.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10485.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10372.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11438.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10470.jpg'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10366.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11361.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11237.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11053.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11428.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10605.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11303.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11086.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10307.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10728.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10319.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11289.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11056.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10821.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11042.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11205.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10361.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10346.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10814.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10357.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10869.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11379.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10341.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10350.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10332.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11000.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10364.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10469.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10743.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11019.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11013.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10464.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10472.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10305.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10733.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11284.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11138.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10322.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10584.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11216.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10304.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11433.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10380.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10874.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10820.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10736.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11117.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10355.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10333.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11118.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10326.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11316.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10426.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11443.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10571.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10385.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10321.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11306.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10588.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11135.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10370.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10593.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10378.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/39425.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10482.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11287.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10408.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10352.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10338.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10339.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10403.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10462.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11376.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11310.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10340.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11355.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10492.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10374.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11360.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10776.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10602.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10495.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11091.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11109.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10678.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11126.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11034.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11368.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10478.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10315.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11031.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10669.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10660.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10991.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11076.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10405.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11024.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10475.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10473.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11356.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10670.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11435.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11296.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10391.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10302.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11096.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10336.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11307.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10748.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10791.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10328.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10358.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10344.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11198.jpg'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11064.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11045.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10369.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11298.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/39401.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11440.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10334.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10741.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10992.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10759.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10884.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10597.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10672.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10876.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10323.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10359.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10799.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10460.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/39410.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10487.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10354.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11089.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11081.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11037.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10727.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11104.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10878.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10312.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11111.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10306.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11067.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10468.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10787.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11413.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10730.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10343.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11429.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10581.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11290.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10400.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10725.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10324.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11136.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10823.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11301.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10500.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10871.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10596.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10356.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11188.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10318.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11040.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11231.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11422.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10585.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11213.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11240.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10480.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11003.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10817.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/12347.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10789.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10756.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11354.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/39403.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11079.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11220.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11427.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11292.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10329.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11027.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10587.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10726.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10309.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11033.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10360.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11010.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10783.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10317.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10805.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10785.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10363.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10768.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10576.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10771.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10467.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10774.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11094.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11211.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11022.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10388.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11073.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10987.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10764.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11420.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10607.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11139.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10428.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10739.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/10802.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11071.jpg'\n",
      "[Errno 2] No such file or directory: 'D:/Data/Fashion/images/11005.jpg'\n"
     ]
    }
   ],
   "source": [
    "for i in interesting['image']:\n",
    "    copy_image_to(DATASET_PATH+'/images/' + i, DATASET_PATH+'/5000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleType</th>\n",
       "      <th>code</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21252</th>\n",
       "      <td>Deodorant</td>\n",
       "      <td>27</td>\n",
       "      <td>58958.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14314</th>\n",
       "      <td>Tops</td>\n",
       "      <td>6</td>\n",
       "      <td>37185.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27981</th>\n",
       "      <td>Track Pants</td>\n",
       "      <td>28</td>\n",
       "      <td>28519.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19681</th>\n",
       "      <td>Sarees</td>\n",
       "      <td>25</td>\n",
       "      <td>57972.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13284</th>\n",
       "      <td>Sports Shoes</td>\n",
       "      <td>4</td>\n",
       "      <td>25423.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>Tshirts</td>\n",
       "      <td>0</td>\n",
       "      <td>4214.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33081</th>\n",
       "      <td>Kurtas</td>\n",
       "      <td>5</td>\n",
       "      <td>32203.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32802</th>\n",
       "      <td>Watches</td>\n",
       "      <td>3</td>\n",
       "      <td>25639.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>Briefs</td>\n",
       "      <td>13</td>\n",
       "      <td>56303.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16199</th>\n",
       "      <td>Dresses</td>\n",
       "      <td>24</td>\n",
       "      <td>48473.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43747 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        articleType  code      image\n",
       "21252     Deodorant    27  58958.jpg\n",
       "14314          Tops     6  37185.jpg\n",
       "27981   Track Pants    28  28519.jpg\n",
       "19681        Sarees    25  57972.jpg\n",
       "13284  Sports Shoes     4  25423.jpg\n",
       "...             ...   ...        ...\n",
       "12334       Tshirts     0   4214.jpg\n",
       "33081        Kurtas     5  32203.jpg\n",
       "32802       Watches     3  25639.jpg\n",
       "9101         Briefs    13  56303.jpg\n",
       "16199       Dresses    24  48473.jpg\n",
       "\n",
       "[43747 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interesting = interesting[~interesting.image.isin(not_found)]\n",
    "interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_gray_and_resize(img_path):\n",
    "    # convert to gray\n",
    "    src = cv2.imread(img_path, 0)\n",
    "    \n",
    "    # dsize\n",
    "    dsize = (28, 28)\n",
    "\n",
    "    # resize image\n",
    "    output = cv2.resize(src, dsize)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = os.listdir(DATASET_PATH + '/5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in img_list:\n",
    "    path  = DATASET_PATH + '/5000/' + i\n",
    "    data  = convert_image_to_gray_and_resize(path)\n",
    "    label = int(interesting[interesting['image'] == i]['code'])\n",
    "    my_data.append([data, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = np.array(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('my_data.npy', my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting.to_csv('interesting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = my_data[:, 0]\n",
    "y = my_data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16012a3e5f8>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu') #'cudo:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor([X_train])\n",
    "y_train = torch.FloatTensor([y_train])\n",
    "X_test  = torch.FloatTensor([X_test])\n",
    "y_test  = torch.FloatTensor([y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.view(X_train.size(1) , 1, 28, 28)\n",
    "X_test  = X_test.view(X_test.size(1), 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.type(torch.LongTensor)\n",
    "y_test  = y_test.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.view(-1)\n",
    "y_test  = y_test.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.dtype, y_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2)\n",
    "        self.act1   = torch.nn.Tanh()\n",
    "        self.pool1 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, padding=0) # сделает 10,10\n",
    "        self.act2  = torch.nn.Tanh()\n",
    "        self.pool2 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1   = torch.nn.Linear(5 * 5 * 16, 120)\n",
    "        self.act3  = torch.nn.Tanh()\n",
    "        \n",
    "        self.fc2   = torch.nn.Linear(120, 84)\n",
    "        self.act4  = torch.nn.Tanh()\n",
    "        \n",
    "        self.fc3   = torch.nn.Linear(84, 142)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.act3(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.act4(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (act1): Tanh()\n",
      "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (act2): Tanh()\n",
      "  (pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (act3): Tanh()\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (act4): Tanh()\n",
      "  (fc3): Linear(in_features=84, out_features=142, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "accuracy: tensor(0.7448) loss tensor(0.0820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0899, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.1350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0966, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.1072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0906, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0535, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.1081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.1092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.1832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.1018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.1024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.1069, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0778, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.1138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.1802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.1403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.1385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.1410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.1044, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0753, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7400) loss tensor(0.0978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.1367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.1233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0979, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.1013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0888, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.1278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0982, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.1199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0862, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.1196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.1206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.1506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.1170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.1005, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0851, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0952, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0772, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0972, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0497, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.1405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.1074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.1158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0800, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0953, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0813, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7456) loss tensor(0.1093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0466, grad_fn=<NllLossBackward>)\n",
      "epoch: 1\n",
      "accuracy: tensor(0.7435) loss tensor(0.0627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.1052, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.1174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.1071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0848, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0610, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0592, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.1241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0871, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.1079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.1286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.1146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.1352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.1437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.1199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0859, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.1216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.1020, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.1172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.1183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.1229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0966, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.1345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.1137, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7456) loss tensor(0.0529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0745, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.1233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0793, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0762, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.1037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.1049, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.1039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0596, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.1118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.1262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.1059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0772, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0864, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.1031, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.1198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0996, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0660, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.1027, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0944, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.1003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0685, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.1352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0860, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.1155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.1645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0907, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.1296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.1594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.1130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.1216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.1081, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7392) loss tensor(0.0813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.1129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.1059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0971, grad_fn=<NllLossBackward>)\n",
      "epoch: 2\n",
      "accuracy: tensor(0.7400) loss tensor(0.0906, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.1144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.1548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.1349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0823, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.1060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.1255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.1136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0622, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0745, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.0670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.0690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.1769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0672, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.1271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.1208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.1045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.1076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0780, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.1059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.1128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0836, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0686, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7447) loss tensor(0.0747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0845, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0911, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0932, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.1088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.1423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.1381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.1164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.1195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.1010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.1440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.1030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.1028, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.1095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.1234, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.1267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.1420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.1510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.0740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.0672, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7349) loss tensor(0.0951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.1003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.1146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.1101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0888, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.1386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.2014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.1677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.2155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.1072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.1518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.2118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.1083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.1499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.1805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.1083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.2156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.1076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.1022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.2025, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0622, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.1194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0846, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.1441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.1097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.1387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.1211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.1001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.1037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.1101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.1004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.0550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.1024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.1210, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7456) loss tensor(0.0726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.1657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0947, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.1433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.1089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.1091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.1155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.1367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.1352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "epoch: 3\n",
      "accuracy: tensor(0.7402) loss tensor(0.0985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.1089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0999, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.1160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0743, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.1140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.1197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0610, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.1100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.1550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0899, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0972, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0823, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.1169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.0723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.1168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.1132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0977, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.1231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0934, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.1694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.1610, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0592, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0625, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7459) loss tensor(0.0657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.2131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.1335, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.2345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.1123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.1252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0895, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.1051, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0982, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.1302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.1025, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.1110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.1510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.1065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.1387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.1076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.1049, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.1248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0994, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.1291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.0949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.1139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0904, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.1281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.1152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0952, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.1084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0715, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.1100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.1005, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7365) loss tensor(0.0700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.1340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.1355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0693, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.0486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.0569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7357) loss tensor(0.1218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7336) loss tensor(0.0799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.1194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.1284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.1179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.1086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.1237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.1073, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7501) loss tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.1157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.1221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.1210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.1113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.1102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.1634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.1077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.2756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.2073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0840, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.1193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.1269, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0903, grad_fn=<NllLossBackward>)\n",
      "epoch: 4\n",
      "accuracy: tensor(0.7397) loss tensor(0.0625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.1158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.1075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.1512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0840, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.1119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0953, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0617, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.1175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0778, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0577, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0577, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0448, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0888, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.1086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7470) loss tensor(0.0806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0715, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.1293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0833, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.1024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0697, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0862, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0744, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0848, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0701, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0779, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.1096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.1045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0871, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0932, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0622, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.1165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.1058, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.1176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.1186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0872, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.1271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0888, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.1029, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0772, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.1094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0603, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7453) loss tensor(0.1071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.1002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.1140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.1078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.1111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.1266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.1530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.1394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0804, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.1225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.1339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0703, grad_fn=<NllLossBackward>)\n",
      "epoch: 5\n",
      "accuracy: tensor(0.7402) loss tensor(0.0324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7368) loss tensor(0.0698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0779, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0592, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0577, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.1141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0836, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0782, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0697, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.1107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0786, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7455) loss tensor(0.0936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.1056, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7517) loss tensor(0.0609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7520) loss tensor(0.0582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.1141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0952, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0845, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.1202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.1126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0959, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.1087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.1138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0508, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.1004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.1134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.1161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0859, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0660, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0685, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0979, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0577, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.1278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.1007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0577, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0697, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.1451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0577, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.1382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0823, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7427) loss tensor(0.1023, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0984, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.1676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.1028, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.1009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.1214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.1316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.1186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0979, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0932, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.1173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.1825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.1285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.1601, grad_fn=<NllLossBackward>)\n",
      "epoch: 6\n",
      "accuracy: tensor(0.7449) loss tensor(0.0747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.1292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.1223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0892, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.1023, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.1076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7365) loss tensor(0.0631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7365) loss tensor(0.1294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.1092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.2103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.0488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7365) loss tensor(0.0716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.1485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.2003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.1293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.1218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.1018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.1323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.1866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.1300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.1347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.1662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.1115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.1142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.1717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.1147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.1056, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.1416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.1173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.1485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0904, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7434) loss tensor(0.0685, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.1168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0917, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0712, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.1114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.1106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.1187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0937, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0846, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.1139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0872, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0687, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.1067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.1069, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.1114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.1111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.1204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0622, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.1001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.1055, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.1770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0766, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7368) loss tensor(0.1735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.0486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.0817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.1534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.1249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7354) loss tensor(0.2230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.1245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.1110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.1487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.1426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.0697, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7365) loss tensor(0.1002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.1493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.1146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.0530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.0975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7339) loss tensor(0.0711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.1502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.1178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0899, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0751, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.1096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.1006, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.0863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.1038, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.1198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0908, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0848, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.1082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.1934, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.2745, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.1137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0737, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7407) loss tensor(0.0912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.1394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.2346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.1983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.1085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.1597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.2161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.1699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.1450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.1289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7357) loss tensor(0.1561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.1276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.1459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.1426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.1782, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.1061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.2203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.1203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.1089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.1157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7346) loss tensor(0.1037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.0980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.1338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.1148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.1276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.1159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.1045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.0964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.1469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.1131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.1089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "epoch: 7\n",
      "accuracy: tensor(0.7393) loss tensor(0.0681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0872, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.1151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0745, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0779, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0535, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0672, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7344) loss tensor(0.0698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.1031, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.1037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7362) loss tensor(0.0622, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7346) loss tensor(0.0638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7351) loss tensor(0.0938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7362) loss tensor(0.0399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0617, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.0897, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0751, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0780, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0932, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.1141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.1013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.1124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0596, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0617, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.1029, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0542, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7471) loss tensor(0.0527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0743, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.1010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.1049, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.1053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.1004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.1085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.1012, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0895, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.1157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0989, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.1150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.1133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0907, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0579, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.1094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.1061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0610, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.1002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0405, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7466) loss tensor(0.0549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0782, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0712, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0701, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0672, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "epoch: 8\n",
      "accuracy: tensor(0.7440) loss tensor(0.0540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0428, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7470) loss tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0497, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0693, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.1020, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0448, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0693, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0448, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0914, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0826, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.1136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0455, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7389) loss tensor(0.0788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.1053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0831, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "epoch: 9\n",
      "accuracy: tensor(0.7408) loss tensor(0.0284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0497, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0535, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0924, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.1098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0577, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0851, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0826, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0804, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0861, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7448) loss tensor(0.0597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.1072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0425, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0743, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.1149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0685, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0804, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0516, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0660, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.1380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.0669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.1002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.1773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.1144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.1280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.1431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.0988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7357) loss tensor(0.0597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.0922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7366) loss tensor(0.1359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7351) loss tensor(0.1357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.0769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.1171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.0861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7342) loss tensor(0.0587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7336) loss tensor(0.0714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7366) loss tensor(0.1000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.1256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.1010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.1002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0766, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.1128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.1195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0889, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7395) loss tensor(0.0626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.1256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.0763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.1089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0953, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.1183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.1247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.1147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.1177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.1181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.1094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0917, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.1416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7366) loss tensor(0.0991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7351) loss tensor(0.1073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.1195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.1348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.1061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.0727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.1112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0617, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0744, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.1485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.1343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.1423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.1154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0831, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.1412, grad_fn=<NllLossBackward>)\n",
      "epoch: 10\n",
      "accuracy: tensor(0.7447) loss tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0840, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.1030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0780, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0836, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.1112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0715, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.1238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0833, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0278, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7438) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0804, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.1553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0851, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0891, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.1124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.1008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.1058, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7365) loss tensor(0.0946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.1339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.1483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.1394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.1232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.1281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.1714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.1566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.1115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.1531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.1432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.1007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.1837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.1693, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7342) loss tensor(0.2417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.1658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.1527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.1560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.1635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.1652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.1055, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.1081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7342) loss tensor(0.2355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.2011, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.3142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.0863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.1077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.3584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.1964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.1354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.2062, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7354) loss tensor(0.2597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.2480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.1965, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7347) loss tensor(0.1717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.2018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.1836, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.1822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.1108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7368) loss tensor(0.1736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.1539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.1420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7358) loss tensor(0.2177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.1414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7344) loss tensor(0.1491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7357) loss tensor(0.2756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.2464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.1330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1996, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.2601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.1870, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.1530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.1082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7354) loss tensor(0.2185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.1821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.1742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.1635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.1533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.1939, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7346) loss tensor(0.2266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.1839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7349) loss tensor(0.2317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.2683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1934, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7354) loss tensor(0.1454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.2050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.2450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.1444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.1456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.1166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.1654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.1052, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.1466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.2580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.1261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.1178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0846, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.2458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.1093, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7392) loss tensor(0.1178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.2160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.1481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.2181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.2064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.1236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.1135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.1368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7342) loss tensor(0.1917, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7346) loss tensor(0.1236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.1255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0982, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0966, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.1584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.1040, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.1223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.1293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.1738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.1053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.1620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.1465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.0599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.0756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.0744, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.1647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.2257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.0843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.2373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.1102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.1516, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.1790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.1232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.1166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7354) loss tensor(0.1337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.0925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7347) loss tensor(0.1110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0999, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.1157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.1209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.1539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.1174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.1253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0939, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.1300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.1378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.0806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.1378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.1061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.1011, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.1230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.1603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.1211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0912, grad_fn=<NllLossBackward>)\n",
      "epoch: 11\n",
      "accuracy: tensor(0.7382) loss tensor(0.0901, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0840, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.1279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.1340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.1048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.1229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.1405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.1005, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0715, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.1003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.1346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.1188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.1191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.1301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.1295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0823, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0860, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.1115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0996, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0470, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7483) loss tensor(0.0688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.1166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0592, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.1160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0968, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0448, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0672, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0672, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.1307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.1128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0872, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.1043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.1039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0579, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.1103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0869, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0801, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0508, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0400, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.1009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.1058, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0610, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0745, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0670, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7410) loss tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.1256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.1084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0994, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0497, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0516, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "epoch: 12\n",
      "accuracy: tensor(0.7402) loss tensor(0.0653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0703, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7447) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0516, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0660, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7486) loss tensor(0.0552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0535, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.1152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0836, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "epoch: 13\n",
      "accuracy: tensor(0.7456) loss tensor(0.0368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7498) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0448, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0577, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0461, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7471) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0712, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0508, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7515) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0577, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0935, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "epoch: 14\n",
      "accuracy: tensor(0.7494) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0610, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0616, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7473) loss tensor(0.0681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0762, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7515) loss tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7528) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7515) loss tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0924, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0267, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7479) loss tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0508, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0479, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0400, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.1037, grad_fn=<NllLossBackward>)\n",
      "epoch: 15\n",
      "accuracy: tensor(0.7486) loss tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.1093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0455, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7496) loss tensor(0.0213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0479, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0859, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0415, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7429) loss tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0743, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.1261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.1233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0425, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.0591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7368) loss tensor(0.0589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0800, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0610, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.1307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0672, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0232, grad_fn=<NllLossBackward>)\n",
      "epoch: 16\n",
      "accuracy: tensor(0.7424) loss tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.1039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.1243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0840, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7437) loss tensor(0.0379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.1382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0535, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0579, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0891, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0610, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.1018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0892, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.1053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0516, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0697, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.1112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0871, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0840, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0937, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.1247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0851, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.1209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0966, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.0944, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.1337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.1247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.1060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0906, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7357) loss tensor(0.1027, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.1095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.1366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7345) loss tensor(0.0678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.1019, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7339) loss tensor(0.0760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.0658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7354) loss tensor(0.0456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.0957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.0988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.1400, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7351) loss tensor(0.1123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.1003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7358) loss tensor(0.1243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7347) loss tensor(0.0878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.0934, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.1036, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.1008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.0679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.0720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0806, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7382) loss tensor(0.0549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.1082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0898, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0753, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0745, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.1077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.1081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.1369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.1088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.1720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.2172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.1467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.1111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.2434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.0744, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7351) loss tensor(0.2093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.0727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.1375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.2053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7354) loss tensor(0.2988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.0971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.0631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2055, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.1560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.1817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7344) loss tensor(0.1001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.0954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7333) loss tensor(0.1580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7354) loss tensor(0.1583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.0800, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7337) loss tensor(0.1592, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7333) loss tensor(0.1168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.0787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.0942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7358) loss tensor(0.1490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.0769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.1109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7339) loss tensor(0.1728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.1043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.1304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.1540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7351) loss tensor(0.2797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7339) loss tensor(0.0927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.1433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.0912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.1189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.1657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.3139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.1243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7339) loss tensor(0.0803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7354) loss tensor(0.1578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.1887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.2141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.1052, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.1784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.0910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.0982, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.1405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.0948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7331) loss tensor(0.1189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.0616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7347) loss tensor(0.1324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1464, grad_fn=<NllLossBackward>)\n",
      "epoch: 17\n",
      "accuracy: tensor(0.7313) loss tensor(0.1132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1058, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.0855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.0817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.1178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.1510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7345) loss tensor(0.1437, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7360) loss tensor(0.1483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.0754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7339) loss tensor(0.1805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.1986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.1277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7337) loss tensor(0.1106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.0926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.0642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.1200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.0981, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.1553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7351) loss tensor(0.1514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.1176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.1184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7347) loss tensor(0.0949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.1124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.1032, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.1011, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.1267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.1074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.1034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.1287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.1513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.1025, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.1390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.0783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.0861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.1078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.1126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.1325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.1366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0914, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.2059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.0863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.1125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.2065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.0674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.1909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.1151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0917, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.1185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7357) loss tensor(0.0956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.2266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.1423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.1439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.0543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7366) loss tensor(0.1087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.1164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.1562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7337) loss tensor(0.1418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7347) loss tensor(0.1122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.2497, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.0744, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.0526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.0926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.1521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.1375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.0907, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7337) loss tensor(0.0668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.1593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.0960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.1223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7346) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.1324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.0785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.1097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.1605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.1199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.0853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7344) loss tensor(0.0787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.0697, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7365) loss tensor(0.0648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.1309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7358) loss tensor(0.0628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.1131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.1431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.1113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.1014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7333) loss tensor(0.0760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.1310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.0931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0937, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0965, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.1022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.1048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.1192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.0780, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0775, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7399) loss tensor(0.1561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.0931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.0910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.0593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7347) loss tensor(0.0661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.0788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.1529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7365) loss tensor(0.0852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7362) loss tensor(0.0750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.1057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.1486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.1039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.1294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.1409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.1071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.1417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.0961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.1047, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.1069, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0751, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.1133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.1058, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7339) loss tensor(0.1539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7358) loss tensor(0.0613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.0489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.1080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.0980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.1428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.1059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.1263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.1503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.1574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.0766, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.1117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.1521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7358) loss tensor(0.2069, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7351) loss tensor(0.1198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.1064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.1461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7366) loss tensor(0.0599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7339) loss tensor(0.1095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7331) loss tensor(0.1486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.1457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7362) loss tensor(0.0740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.1749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0984, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0908, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.1187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.1355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.1043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.1171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.1227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.1262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7357) loss tensor(0.1246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.2078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.1047, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.1015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.0737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.0998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7366) loss tensor(0.0815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.1005, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.1344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.1180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7345) loss tensor(0.0806, grad_fn=<NllLossBackward>)\n",
      "epoch: 18\n",
      "accuracy: tensor(0.7361) loss tensor(0.1205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.0564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7333) loss tensor(0.0874, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7352) loss tensor(0.1007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.1030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.0704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.1044, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.1144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.0745, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.1007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.0681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.0877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.1638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.3299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7354) loss tensor(0.0665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.1706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.2328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.2975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7365) loss tensor(0.3876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.1661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.0682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.3285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7365) loss tensor(0.0997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.0874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.2314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.1211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.1280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.0729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7354) loss tensor(0.1855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.1676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7339) loss tensor(0.1599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0939, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.1332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.1374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.2166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.1419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.1071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7358) loss tensor(0.0814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7337) loss tensor(0.1129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.0666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7346) loss tensor(0.0789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.1212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7345) loss tensor(0.1040, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7336) loss tensor(0.1019, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7345) loss tensor(0.1526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.1412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.1229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.0769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.0563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7347) loss tensor(0.1045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.0850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.1200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.1164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.1053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.1129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.1188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.1877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.1324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.1383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.1523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.1239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.1197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.1087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.0815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.2034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.1113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.1236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.2195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.1413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7344) loss tensor(0.1213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7354) loss tensor(0.1364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.2210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.1210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.0970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.1188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.1554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.1825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.1039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.1394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.1504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.0647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.0725, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7358) loss tensor(0.1168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.0756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7345) loss tensor(0.0765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7344) loss tensor(0.0887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.0962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0871, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.1073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0917, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.1192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.0748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7362) loss tensor(0.0778, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.1321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.1033, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.1388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.0532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7368) loss tensor(0.0395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.1423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0751, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.1036, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0753, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.1561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0745, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.1121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0516, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0862, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0762, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.1067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "epoch: 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7435) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0779, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0497, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0836, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0535, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0234, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7461) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0660, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0870, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.1147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0497, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0425, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0357, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7493) loss tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "epoch: 20\n",
      "accuracy: tensor(0.7499) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7519) loss tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7518) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7525) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7517) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7517) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7514) loss tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7514) loss tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7525) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7517) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7517) loss tensor(0.0299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7520) loss tensor(0.0296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0269, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7514) loss tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0342, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7462) loss tensor(0.0284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0479, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0496, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7446) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "epoch: 21\n",
      "accuracy: tensor(0.7474) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0508, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7349) loss tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0400, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7458) loss tensor(0.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0234, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.0419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7459) loss tensor(0.0537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0425, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0447, grad_fn=<NllLossBackward>)\n",
      "epoch: 22\n",
      "accuracy: tensor(0.7482) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7515) loss tensor(0.0302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7518) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7528) loss tensor(0.0291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7518) loss tensor(0.0182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7515) loss tensor(0.0207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7520) loss tensor(0.0314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7528) loss tensor(0.0203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7526) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7528) loss tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7530) loss tensor(0.0238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7536) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7517) loss tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7517) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7514) loss tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7479) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7523) loss tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7520) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7517) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7515) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7522) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7519) loss tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0899, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0268, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7391) loss tensor(0.0494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "epoch: 23\n",
      "accuracy: tensor(0.7485) loss tensor(0.0837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.0374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7368) loss tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.2120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.0410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.3367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.1883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.1434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.0838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.1961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.1304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.1441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.0900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7351) loss tensor(0.2252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.2385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.0938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.3865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.2825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.1062, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.1392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.1985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.4642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.3595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3895, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.2815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.3041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.3043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1932, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2579, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.1839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2959, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.1787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.1722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.1082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.2506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.1934, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2017, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.4404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7358) loss tensor(0.1484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.1265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.1622, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7349) loss tensor(0.1464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1870, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1516, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.1132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.1920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7346) loss tensor(0.1924, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7298) loss tensor(0.1476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1947, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.2729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2012, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2888, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.2858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7339) loss tensor(0.2673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.2277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.1570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.2023, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.2107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.1854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.1404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.1557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.2097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.1903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.2252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.2317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.1318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.2264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.1676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.1220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7333) loss tensor(0.1260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.2200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1906, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1056, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.1258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7342) loss tensor(0.1229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.1490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.1248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.0817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.1320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.1044, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.1140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.1245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.1274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.1187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.1245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.1359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.1045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.1534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.1461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.0986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.1303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.1491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.1382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.1186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0982, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.1104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0851, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.0827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.1056, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.0990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.1503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.1332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.1226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.1593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7366) loss tensor(0.1286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.1779, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7342) loss tensor(0.0825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.1786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.0583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7357) loss tensor(0.2557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7346) loss tensor(0.1694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.1260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.1252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7368) loss tensor(0.0965, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.1454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.1093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7335) loss tensor(0.1154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.2202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.1283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.1101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.1044, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.1214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.1222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.2111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.1661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.1279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7366) loss tensor(0.1179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.1375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7347) loss tensor(0.0842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.2887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.1809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.1055, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.1326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.1323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.1502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.1193, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7418) loss tensor(0.0731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.1133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.1356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0831, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0800, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.1404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.1134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.1100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0907, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.0770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.1435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.1210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.1371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.1057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.1154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.1720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.1630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.1843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.1827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.2034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7354) loss tensor(0.1855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.2076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.1809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.1666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.1362, grad_fn=<NllLossBackward>)\n",
      "epoch: 24\n",
      "accuracy: tensor(0.7349) loss tensor(0.1432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7349) loss tensor(0.0581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7349) loss tensor(0.1366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.0644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.0739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.1942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7349) loss tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7342) loss tensor(0.1121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.0769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.0650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.0750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.0923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.1267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.1128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.1419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.0826, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.0784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7362) loss tensor(0.0770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7345) loss tensor(0.0925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.1121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7358) loss tensor(0.0776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0996, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0958, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.1065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0779, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0745, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.1137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0930, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.1222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.0737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.1379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.1565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.1148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.1084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.1000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.1158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.1602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.1955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.1268, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7440) loss tensor(0.0542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.1417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.1104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0901, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.1231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.1464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.1100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.1131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.1616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0743, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.1219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.1451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.1890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.1042, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.1947, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.1287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.1326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.1559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.1991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.0876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.0682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.1050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.0420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.1267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7345) loss tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.1276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.0920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.1068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.1007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0772, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.0512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.0960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.0980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7333) loss tensor(0.0835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.0712, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.0777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.0818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.0609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7357) loss tensor(0.1015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.0988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.0680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.1388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.1113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.1080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7373) loss tensor(0.0729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7365) loss tensor(0.0532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.0550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.0866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.0874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0687, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0617, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.1068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0357, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7454) loss tensor(0.0569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.1559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "epoch: 25\n",
      "accuracy: tensor(0.7434) loss tensor(0.0435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0516, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0400, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7419) loss tensor(0.0425, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0497, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7518) loss tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.1215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.1483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0693, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0577, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0672, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0294, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7481) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "epoch: 26\n",
      "accuracy: tensor(0.7438) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0516, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7493) loss tensor(0.0654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0234, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7520) loss tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7518) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0391, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7448) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "epoch: 27\n",
      "accuracy: tensor(0.7474) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0269, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7479) loss tensor(0.0265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7521) loss tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0592, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0901, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.1137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0579, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0479, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0707, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7445) loss tensor(0.0414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0672, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.1248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.1351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0622, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0906, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.1619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.1133, grad_fn=<NllLossBackward>)\n",
      "epoch: 28\n",
      "accuracy: tensor(0.7361) loss tensor(0.0706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0516, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.1021, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0782, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.1513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.0526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0672, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0780, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7437) loss tensor(0.0476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.1057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0425, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0479, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0762, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.0687, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.0573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0892, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.0460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0592, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0425, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.0450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0343, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7413) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0751, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.1035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.1051, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.0832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0743, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.1303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "epoch: 29\n",
      "accuracy: tensor(0.7455) loss tensor(0.0467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0335, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0449, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7434) loss tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0497, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.1073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0579, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.1188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0766, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0667, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7405) loss tensor(0.0595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.0457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7362) loss tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0965, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.1417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0952, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0831, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "epoch: 30\n",
      "accuracy: tensor(0.7455) loss tensor(0.0223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0234, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0301, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7471) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0234, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0337, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7443) loss tensor(0.0297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0269, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0400, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0392, grad_fn=<NllLossBackward>)\n",
      "epoch: 31\n",
      "accuracy: tensor(0.7433) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7474) loss tensor(0.0261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0269, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7464) loss tensor(0.0427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0269, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0772, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7357) loss tensor(0.0327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.1012, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.0624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0384, grad_fn=<NllLossBackward>)\n",
      "epoch: 32\n",
      "accuracy: tensor(0.7378) loss tensor(0.0430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7368) loss tensor(0.0735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.1054, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7368) loss tensor(0.0773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7349) loss tensor(0.0619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.1002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.1009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.0801, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0860, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.1041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.0496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.1380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.1020, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.1592, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.1224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.1050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7349) loss tensor(0.1070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.0863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.0896, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7301) loss tensor(0.1181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.0671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.0760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.1471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2745, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.0801, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.1280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.0453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7336) loss tensor(0.1775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.1747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.0864, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.0964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7342) loss tensor(0.0969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.1246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7358) loss tensor(0.1179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7337) loss tensor(0.1295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.1478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.1915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.1090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.1600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.0614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.0700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.0661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.1378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7331) loss tensor(0.1422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.0711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1968, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.0527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.1088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1005, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.0849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.1379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.0771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.0884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.1795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.1275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.2360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7354) loss tensor(0.1165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.1087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.2839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.1661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3535, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.2114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7347) loss tensor(0.1763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.4208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.2895, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.1097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.4722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.3902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.8864, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.7878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.2001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.3266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7080) loss tensor(0.1767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7014) loss tensor(0.3989, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6941) loss tensor(0.6111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7101) loss tensor(0.6796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7090) loss tensor(0.3698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.3198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6950) loss tensor(0.2128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6905) loss tensor(0.5824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6923) loss tensor(0.7436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6959) loss tensor(0.8210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7042) loss tensor(0.4412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7032) loss tensor(0.3829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7088) loss tensor(0.5471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7097) loss tensor(0.1471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7017) loss tensor(0.3238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.5159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7099) loss tensor(0.3810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7017) loss tensor(0.3574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6994) loss tensor(0.4602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7046) loss tensor(0.6029, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.5523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.2474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.4186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.4182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.1744, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7177) loss tensor(0.3752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.5030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.2083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7081) loss tensor(0.6126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7094) loss tensor(0.4898, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.5109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.5254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.4223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.6120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.1500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.4883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3052, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.1845, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.2436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.3699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.2481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.3001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.4717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.5372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.1958, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.3931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.3092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.2676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.3638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.1542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.4416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.4057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.3364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.4307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.4821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.3180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.1548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7112) loss tensor(0.4900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.2491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.4928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.4592, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.2678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.4640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.2749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.2061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2701, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.4594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.3752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.4082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.1452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2029, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.4014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7339) loss tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "epoch: 33\n",
      "accuracy: tensor(0.7314) loss tensor(0.0636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1006, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.1767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7342) loss tensor(0.1137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.1347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7351) loss tensor(0.1559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1744, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7347) loss tensor(0.1050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.1611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1947, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.1698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.1547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7362) loss tensor(0.1641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.2105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7335) loss tensor(0.1749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2005, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2032, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.1505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7331) loss tensor(0.3390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.1777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1684, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7309) loss tensor(0.1834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.1176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.2090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.1314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.1729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7335) loss tensor(0.1703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.0700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7346) loss tensor(0.1401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.2180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7351) loss tensor(0.1261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.1055, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.0664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.0677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.0792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.1049, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.0968, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.0969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7335) loss tensor(0.1033, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7335) loss tensor(0.1030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7337) loss tensor(0.0798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7335) loss tensor(0.1643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7342) loss tensor(0.0917, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7346) loss tensor(0.0661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.1617, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.1030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.1215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.1204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.2179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.0604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.1754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.1591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.1245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7347) loss tensor(0.1188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.0785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.1694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.1256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.1091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.2212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.1417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7358) loss tensor(0.0853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.0850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0937, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.1130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.1272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.1405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0693, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.0517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7349) loss tensor(0.0754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.0766, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7337) loss tensor(0.1073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7335) loss tensor(0.0628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7362) loss tensor(0.0775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.0941, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.0818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.1012, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.1003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.1066, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.1256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.1031, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0728, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7442) loss tensor(0.0578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0479, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "epoch: 34\n",
      "accuracy: tensor(0.7488) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0315, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7424) loss tensor(0.0255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7434) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0234, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0269, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "epoch: 35\n",
      "accuracy: tensor(0.7487) loss tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0335, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7471) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7457) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "epoch: 36\n",
      "accuracy: tensor(0.7496) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7454) loss tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7483) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "epoch: 37\n",
      "accuracy: tensor(0.7477) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7493) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0069, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7522) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7515) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7514) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0121, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7497) loss tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7520) loss tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7514) loss tensor(0.0203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7518) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7514) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7514) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0179, grad_fn=<NllLossBackward>)\n",
      "epoch: 38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7503) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0054, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7517) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0269, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7451) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0899, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0335, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7459) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0693, grad_fn=<NllLossBackward>)\n",
      "epoch: 39\n",
      "accuracy: tensor(0.7461) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.1984, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.0506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.0411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.0644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.0989, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.1010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.0360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.1536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7366) loss tensor(0.0530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.1558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.1521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.2651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.0844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.2451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.0748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.2336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.1630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.1923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7335) loss tensor(0.1314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.1493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7333) loss tensor(0.2172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.1009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.2001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7345) loss tensor(0.1853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.1737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.1037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7345) loss tensor(0.1030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7333) loss tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.0854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.2448, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.1555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.1832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7336) loss tensor(0.2127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.0834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.1186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.0629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.1107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.3484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.0936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.0643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7331) loss tensor(0.1078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7357) loss tensor(0.1797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.1442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.1039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7357) loss tensor(0.0613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.1876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7351) loss tensor(0.1783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.1397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.0625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.3049, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.2421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.2130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.1523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.1864, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.2720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.4900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.3037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1182, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7307) loss tensor(0.2995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.2512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.3582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.1924, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.3422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.2729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7368) loss tensor(0.2191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7368) loss tensor(0.2212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.2393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.1762, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.1468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7336) loss tensor(0.2089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7358) loss tensor(0.1312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7365) loss tensor(0.1506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.0919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.1785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.1970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1871, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.3338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.1465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7349) loss tensor(0.1195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7374) loss tensor(0.1879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7365) loss tensor(0.1510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7393) loss tensor(0.1577, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.1913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.1598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.3727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1782, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.0989, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.1389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.1846, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.1003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.2478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.1746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7331) loss tensor(0.2240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.2165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.0960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.0976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.0946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.1692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1044, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.2854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.0756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1055, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.3574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1058, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.0980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.2265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7335) loss tensor(0.1197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.1653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.1538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.1924, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.2378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.1821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.2957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7344) loss tensor(0.0942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.1775, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7328) loss tensor(0.2330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.1656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7337) loss tensor(0.1799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1712, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1042, grad_fn=<NllLossBackward>)\n",
      "epoch: 40\n",
      "accuracy: tensor(0.7285) loss tensor(0.1441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.1194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.0987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.0896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.0950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.0731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7333) loss tensor(0.1267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7358) loss tensor(0.0780, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7349) loss tensor(0.0767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7346) loss tensor(0.0429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7352) loss tensor(0.0675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.0544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7355) loss tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.1331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7363) loss tensor(0.1573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7368) loss tensor(0.1404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.1134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7383) loss tensor(0.1059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.1086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0831, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.0620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.1048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7367) loss tensor(0.0474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7345) loss tensor(0.0751, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7342) loss tensor(0.0403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7345) loss tensor(0.1525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7335) loss tensor(0.1026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7349) loss tensor(0.0843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.0604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.0683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7331) loss tensor(0.1111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7345) loss tensor(0.1384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7331) loss tensor(0.0794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.0806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.1452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.1464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7345) loss tensor(0.0729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7336) loss tensor(0.1111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.1175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.1774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7359) loss tensor(0.1039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7362) loss tensor(0.1515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7399) loss tensor(0.0682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.1201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.1408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.1452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.1035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7351) loss tensor(0.0732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.0558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7337) loss tensor(0.0379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.0773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.0398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.0286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7346) loss tensor(0.0566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7358) loss tensor(0.0849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.1092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7365) loss tensor(0.1226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.1436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.0811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0800, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.1126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.1200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7426) loss tensor(0.0538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7417) loss tensor(0.0879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0947, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7410) loss tensor(0.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7418) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0958, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7362) loss tensor(0.0553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.0311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7362) loss tensor(0.0512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.0690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7392) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7402) loss tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7391) loss tensor(0.0559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7375) loss tensor(0.0790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0712, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.0864, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.0677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7368) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7381) loss tensor(0.0669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7410) loss tensor(0.0395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7431) loss tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7403) loss tensor(0.0528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.0929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7379) loss tensor(0.0666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7421) loss tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7407) loss tensor(0.0420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0935, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.1325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7378) loss tensor(0.0493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7377) loss tensor(0.0465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.0731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7400) loss tensor(0.1125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7397) loss tensor(0.0714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7370) loss tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7371) loss tensor(0.0470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7382) loss tensor(0.0564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7361) loss tensor(0.0816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7398) loss tensor(0.0656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7387) loss tensor(0.0601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7389) loss tensor(0.0461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7415) loss tensor(0.0413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0697, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7405) loss tensor(0.0296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7395) loss tensor(0.0807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7401) loss tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0751, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7409) loss tensor(0.0843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7390) loss tensor(0.0420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7394) loss tensor(0.0741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7406) loss tensor(0.0728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7419) loss tensor(0.0873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0999, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0245, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7439) loss tensor(0.0336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7422) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7425) loss tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7437) loss tensor(0.0335, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0455, grad_fn=<NllLossBackward>)\n",
      "epoch: 41\n",
      "accuracy: tensor(0.7463) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0269, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7434) loss tensor(0.0338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7435) loss tensor(0.0439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0448, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7416) loss tensor(0.0490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7413) loss tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7414) loss tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7411) loss tensor(0.0377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7423) loss tensor(0.0288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0508, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7430) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7424) loss tensor(0.0444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7479) loss tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7518) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7522) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7514) loss tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "epoch: 42\n",
      "accuracy: tensor(0.7469) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7514) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0145, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7479) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7518) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0171, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7481) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "epoch: 43\n",
      "accuracy: tensor(0.7499) loss tensor(0.0077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7515) loss tensor(0.0223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7519) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7527) loss tensor(0.0073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7517) loss tensor(0.0081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7519) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7514) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7504) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0056, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0062, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7515) loss tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7514) loss tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7534) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7529) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7528) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7515) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0303, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7479) loss tensor(0.0225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7515) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7521) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7517) loss tensor(0.0084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7507) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7515) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "epoch: 44\n",
      "accuracy: tensor(0.7493) loss tensor(0.0080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0058, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7466) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7499) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7519) loss tensor(0.0079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7517) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7514) loss tensor(0.0076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7514) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0080, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7473) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7459) loss tensor(0.0078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7446) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0084, grad_fn=<NllLossBackward>)\n",
      "epoch: 45\n",
      "accuracy: tensor(0.7487) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7510) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7517) loss tensor(0.0078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7519) loss tensor(0.0123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7512) loss tensor(0.0086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7503) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7513) loss tensor(0.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7514) loss tensor(0.0071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7504) loss tensor(0.0086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7506) loss tensor(0.0074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7501) loss tensor(0.0095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7509) loss tensor(0.0083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7502) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0095, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7501) loss tensor(0.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7495) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7496) loss tensor(0.0113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7497) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7487) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0062, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7499) loss tensor(0.0084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7498) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7494) loss tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7490) loss tensor(0.0065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7491) loss tensor(0.0096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7480) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7475) loss tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7479) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7471) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7474) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7469) loss tensor(0.0160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7438) loss tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7458) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7463) loss tensor(0.0076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7461) loss tensor(0.0179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7433) loss tensor(0.0107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7432) loss tensor(0.0147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7464) loss tensor(0.0144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7477) loss tensor(0.0081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7511) loss tensor(0.0330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7489) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7478) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7481) loss tensor(0.0084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7473) loss tensor(0.0182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7485) loss tensor(0.0255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7482) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7465) loss tensor(0.0157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7467) loss tensor(0.0211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7472) loss tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0173, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7446) loss tensor(0.0142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7441) loss tensor(0.0192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7447) loss tensor(0.0146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7451) loss tensor(0.0250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7453) loss tensor(0.0397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7448) loss tensor(0.0195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7442) loss tensor(0.0094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7440) loss tensor(0.0208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7456) loss tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7449) loss tensor(0.0117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7454) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7450) loss tensor(0.0189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7455) loss tensor(0.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7483) loss tensor(0.0591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7486) loss tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7493) loss tensor(0.0150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7505) loss tensor(0.0270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7488) loss tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7470) loss tensor(0.0223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7462) loss tensor(0.0326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7457) loss tensor(0.0492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7439) loss tensor(0.0127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7427) loss tensor(0.0283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7443) loss tensor(0.0509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7429) loss tensor(0.0275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7376) loss tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7384) loss tensor(0.1034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7445) loss tensor(0.1494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7408) loss tensor(0.0331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7369) loss tensor(0.0627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7365) loss tensor(0.0559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7386) loss tensor(0.0918, grad_fn=<NllLossBackward>)\n",
      "epoch: 46\n",
      "accuracy: tensor(0.7409) loss tensor(0.0447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.1008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.2335, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7385) loss tensor(0.2134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.0575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.0995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1032, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.6399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.4013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.2913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4627) loss tensor(0.2993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(3.5318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6730) loss tensor(0.5963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6445) loss tensor(0.7406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6287) loss tensor(1.2845, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6169) loss tensor(1.5703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6189) loss tensor(1.8128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5923) loss tensor(1.2363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.2567) loss tensor(1.5974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.2719) loss tensor(8.0857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.2889) loss tensor(8.2940, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.2966) loss tensor(5.9408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.3422) loss tensor(6.6946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.3704) loss tensor(5.4818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4640) loss tensor(5.8084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4646) loss tensor(4.3571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.2602) loss tensor(3.4199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.2698) loss tensor(8.7671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4613) loss tensor(8.0794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4509) loss tensor(4.1527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4753) loss tensor(4.0187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4800) loss tensor(3.5905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4857) loss tensor(3.4528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4955) loss tensor(3.6568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4997) loss tensor(3.5131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5192) loss tensor(3.2091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5331) loss tensor(3.1252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.2405) loss tensor(2.8836, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.2391) loss tensor(7.1628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.2608) loss tensor(6.2763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.2767) loss tensor(6.6898, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.2951) loss tensor(6.3564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.3209) loss tensor(5.7830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.3323) loss tensor(5.0274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.3571) loss tensor(4.6846, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.3787) loss tensor(4.8971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.3914) loss tensor(4.7410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4043) loss tensor(4.1702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4163) loss tensor(3.1134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4264) loss tensor(4.2217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4342) loss tensor(4.0185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4466) loss tensor(3.9122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4530) loss tensor(3.4395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4423) loss tensor(2.9365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4498) loss tensor(3.3228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4595) loss tensor(3.3304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4697) loss tensor(3.0905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4739) loss tensor(3.0404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4744) loss tensor(3.7587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4801) loss tensor(3.4049, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.4832) loss tensor(3.2810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4861) loss tensor(2.7261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4807) loss tensor(3.2655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4843) loss tensor(3.8412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4947) loss tensor(3.3799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5054) loss tensor(2.6796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5114) loss tensor(2.8147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5064) loss tensor(3.2723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5099) loss tensor(3.3761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5075) loss tensor(2.7975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5058) loss tensor(3.2328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5098) loss tensor(3.0886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5168) loss tensor(1.8819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5262) loss tensor(2.4256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5336) loss tensor(2.5761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5472) loss tensor(2.9228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5507) loss tensor(2.9433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5585) loss tensor(2.2109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5608) loss tensor(2.5633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5577) loss tensor(2.1914, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5483) loss tensor(1.8944, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5430) loss tensor(1.9958, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5537) loss tensor(2.8893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5553) loss tensor(2.2575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5581) loss tensor(1.8867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5624) loss tensor(2.4628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5615) loss tensor(2.2895, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5647) loss tensor(2.0033, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5679) loss tensor(2.4356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5677) loss tensor(1.9718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5705) loss tensor(2.1525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5790) loss tensor(2.0011, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5866) loss tensor(2.0152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5897) loss tensor(2.1660, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5904) loss tensor(2.0028, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5904) loss tensor(1.5765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5881) loss tensor(2.2538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5939) loss tensor(2.1529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5966) loss tensor(1.8775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5943) loss tensor(2.3997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5920) loss tensor(1.7664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5902) loss tensor(2.1715, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5915) loss tensor(1.8848, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5921) loss tensor(1.7571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5914) loss tensor(1.7548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5919) loss tensor(2.3070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5939) loss tensor(2.0537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5903) loss tensor(1.7821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5890) loss tensor(1.5050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5918) loss tensor(1.9619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5857) loss tensor(1.3081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5853) loss tensor(1.3240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5838) loss tensor(1.6971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5830) loss tensor(1.9003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5841) loss tensor(1.8142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5831) loss tensor(1.8214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5853) loss tensor(2.2221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5862) loss tensor(1.8920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5871) loss tensor(2.0384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5869) loss tensor(1.5787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5894) loss tensor(1.9067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5896) loss tensor(1.7047, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5920) loss tensor(2.0098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5958) loss tensor(1.8668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5970) loss tensor(2.3521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5966) loss tensor(1.9259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5925) loss tensor(2.0109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5911) loss tensor(1.5539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5917) loss tensor(2.0349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5842) loss tensor(2.0460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5763) loss tensor(1.5647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5910) loss tensor(1.6304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6011) loss tensor(1.3485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6064) loss tensor(1.7009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6058) loss tensor(1.2860, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6083) loss tensor(1.8474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6121) loss tensor(1.6942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6165) loss tensor(1.3187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6190) loss tensor(1.6344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6221) loss tensor(1.9480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6246) loss tensor(1.3722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6253) loss tensor(1.6194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6261) loss tensor(1.4863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6251) loss tensor(1.4502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6190) loss tensor(1.8117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6129) loss tensor(1.5308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6145) loss tensor(1.5929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6169) loss tensor(1.4121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6161) loss tensor(1.7358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6232) loss tensor(2.0183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6239) loss tensor(1.5038, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6186) loss tensor(1.6294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6175) loss tensor(1.2988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6154) loss tensor(1.5787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6114) loss tensor(1.8164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6127) loss tensor(1.9929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6110) loss tensor(1.9800, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6141) loss tensor(1.2941, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6165) loss tensor(1.5539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6166) loss tensor(2.1374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6151) loss tensor(1.9852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6128) loss tensor(1.8553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6105) loss tensor(1.6739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6064) loss tensor(1.2149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6054) loss tensor(1.5151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6018) loss tensor(1.8276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5814) loss tensor(1.7524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.3645) loss tensor(1.6070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.3699) loss tensor(3.8614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4114) loss tensor(4.4447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5345) loss tensor(4.3649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6313) loss tensor(2.3195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6262) loss tensor(1.5233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6142) loss tensor(1.7649, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.5949) loss tensor(1.9283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5832) loss tensor(1.9255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5799) loss tensor(2.0054, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5848) loss tensor(2.6560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5802) loss tensor(2.3799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5790) loss tensor(1.6880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5791) loss tensor(1.9550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5843) loss tensor(2.0754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5846) loss tensor(1.5164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5793) loss tensor(2.2414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.4662) loss tensor(1.5085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5794) loss tensor(3.0474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5638) loss tensor(2.1669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5513) loss tensor(1.7252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5462) loss tensor(1.8158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5392) loss tensor(2.3063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5391) loss tensor(2.3784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5432) loss tensor(2.0432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5458) loss tensor(2.7431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5456) loss tensor(2.3046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5456) loss tensor(2.1764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5477) loss tensor(1.7805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5510) loss tensor(2.1920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5507) loss tensor(2.3495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5608) loss tensor(2.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5621) loss tensor(2.2101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5670) loss tensor(1.7141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5712) loss tensor(1.9575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5738) loss tensor(2.5068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5742) loss tensor(1.7864, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5702) loss tensor(1.7457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5656) loss tensor(2.0122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5587) loss tensor(2.0879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5419) loss tensor(2.0974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5691) loss tensor(2.1174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5706) loss tensor(2.2585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5603) loss tensor(1.8158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5595) loss tensor(2.1194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5513) loss tensor(2.3355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5505) loss tensor(1.9699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5499) loss tensor(2.5616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5473) loss tensor(1.8419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5487) loss tensor(2.0188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5531) loss tensor(2.1267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5490) loss tensor(2.2162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5517) loss tensor(2.1468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5562) loss tensor(1.9845, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5533) loss tensor(1.5836, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5530) loss tensor(1.8114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5563) loss tensor(1.9010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5619) loss tensor(1.9570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5654) loss tensor(1.9648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5705) loss tensor(1.3860, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5723) loss tensor(2.0538, grad_fn=<NllLossBackward>)\n",
      "epoch: 47\n",
      "accuracy: tensor(0.5701) loss tensor(1.4153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5622) loss tensor(2.1997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5565) loss tensor(2.2023, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5534) loss tensor(1.3939, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5544) loss tensor(1.9703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5561) loss tensor(1.2251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5592) loss tensor(1.7151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5603) loss tensor(1.3218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5651) loss tensor(2.5538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5681) loss tensor(2.4415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5635) loss tensor(2.2660, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5639) loss tensor(1.5115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5710) loss tensor(1.8290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5709) loss tensor(1.8524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5701) loss tensor(1.6699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5746) loss tensor(1.2868, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5733) loss tensor(1.9828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5736) loss tensor(1.8056, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5779) loss tensor(1.4690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5874) loss tensor(1.3955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5937) loss tensor(1.6270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5915) loss tensor(1.7331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5929) loss tensor(1.5147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5931) loss tensor(1.8234, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5930) loss tensor(1.7251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5925) loss tensor(1.3006, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5968) loss tensor(1.8093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6019) loss tensor(1.7772, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6041) loss tensor(1.8703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6051) loss tensor(1.4682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6051) loss tensor(1.7041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6050) loss tensor(1.3171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6008) loss tensor(1.4430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6014) loss tensor(1.4957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5985) loss tensor(1.5837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6026) loss tensor(1.9648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6009) loss tensor(1.4582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6037) loss tensor(1.7137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6005) loss tensor(1.0628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5925) loss tensor(1.7883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5903) loss tensor(1.4991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5891) loss tensor(1.6455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5888) loss tensor(1.6091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5873) loss tensor(1.3726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5848) loss tensor(1.8590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5854) loss tensor(1.8704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5843) loss tensor(1.9798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5893) loss tensor(2.0178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5896) loss tensor(1.6177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5910) loss tensor(1.9014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5928) loss tensor(1.6461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5935) loss tensor(1.8015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5941) loss tensor(1.5152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5966) loss tensor(1.8013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6022) loss tensor(1.4568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6069) loss tensor(1.5067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6018) loss tensor(1.4179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6006) loss tensor(1.3328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6030) loss tensor(1.2662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6021) loss tensor(1.5855, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6013) loss tensor(1.5166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5999) loss tensor(1.4933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6045) loss tensor(1.6779, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6067) loss tensor(1.2690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6058) loss tensor(1.7530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6002) loss tensor(1.5927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5933) loss tensor(1.7554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5917) loss tensor(1.8476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5883) loss tensor(1.4054, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5879) loss tensor(1.7467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5834) loss tensor(2.0074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5799) loss tensor(1.9560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5795) loss tensor(1.9032, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5720) loss tensor(1.8893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5739) loss tensor(1.8651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5790) loss tensor(1.7230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5864) loss tensor(1.1239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5986) loss tensor(1.8652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6006) loss tensor(2.0353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6042) loss tensor(1.7672, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6086) loss tensor(1.6460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6085) loss tensor(1.3636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6097) loss tensor(1.0481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6105) loss tensor(1.2783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6103) loss tensor(1.5394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6096) loss tensor(1.4263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6118) loss tensor(1.7350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6120) loss tensor(1.6266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6105) loss tensor(1.2830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6136) loss tensor(1.6215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6182) loss tensor(1.4767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6174) loss tensor(1.7230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6152) loss tensor(1.7407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6138) loss tensor(1.3045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6157) loss tensor(2.0740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6160) loss tensor(1.1792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6103) loss tensor(1.7294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6064) loss tensor(1.5993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6087) loss tensor(1.4995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6080) loss tensor(1.4645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6096) loss tensor(1.5247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6091) loss tensor(1.3380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6081) loss tensor(1.5498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6087) loss tensor(1.0253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6109) loss tensor(1.2488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6081) loss tensor(1.2611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6096) loss tensor(1.1711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6099) loss tensor(1.9072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6106) loss tensor(1.5120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6147) loss tensor(1.4465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6129) loss tensor(1.4396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6138) loss tensor(1.5976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6102) loss tensor(1.3175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6096) loss tensor(1.6113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6114) loss tensor(1.2080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6104) loss tensor(1.1894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6097) loss tensor(1.9426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6095) loss tensor(1.9877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6114) loss tensor(1.0571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6117) loss tensor(1.3331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6122) loss tensor(1.4960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6157) loss tensor(1.4856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6161) loss tensor(1.4755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6173) loss tensor(1.1511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6182) loss tensor(1.6524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6193) loss tensor(1.8725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6195) loss tensor(1.7413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6200) loss tensor(1.2645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6215) loss tensor(1.7691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6208) loss tensor(0.9914, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6248) loss tensor(1.4221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6248) loss tensor(1.9587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6257) loss tensor(1.4523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6264) loss tensor(1.5809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6265) loss tensor(1.4161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6261) loss tensor(1.3966, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6243) loss tensor(1.3271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6259) loss tensor(1.2008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6242) loss tensor(1.2404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6240) loss tensor(1.4996, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6231) loss tensor(1.4372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6255) loss tensor(1.2658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6270) loss tensor(1.5109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6251) loss tensor(1.3896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6261) loss tensor(1.4306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6263) loss tensor(1.3034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6223) loss tensor(1.3911, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6219) loss tensor(1.1785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6221) loss tensor(1.2637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6189) loss tensor(1.2608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6183) loss tensor(1.3538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6145) loss tensor(1.4600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6145) loss tensor(1.6971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6173) loss tensor(1.3596, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6233) loss tensor(1.4553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6277) loss tensor(1.5228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6255) loss tensor(1.4449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6289) loss tensor(1.5765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6272) loss tensor(1.3190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6286) loss tensor(1.3252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6286) loss tensor(1.6975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6285) loss tensor(1.6845, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6278) loss tensor(1.2725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6217) loss tensor(1.3564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6191) loss tensor(1.6816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6167) loss tensor(1.7329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6139) loss tensor(1.4504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6150) loss tensor(1.3963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6153) loss tensor(1.3634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6129) loss tensor(1.5557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6114) loss tensor(1.5211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6143) loss tensor(1.4503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6221) loss tensor(1.2509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6221) loss tensor(1.5051, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6255) loss tensor(1.0975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6306) loss tensor(1.3542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6269) loss tensor(1.4155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6295) loss tensor(1.3996, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6299) loss tensor(1.8340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6282) loss tensor(1.2628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6263) loss tensor(1.4316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6133) loss tensor(1.3595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.2632) loss tensor(1.2806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.2790) loss tensor(6.3332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6314) loss tensor(6.1077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6242) loss tensor(1.4398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6070) loss tensor(1.6415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5939) loss tensor(1.5489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5817) loss tensor(1.2375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5768) loss tensor(1.9243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5737) loss tensor(1.8390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5762) loss tensor(2.2436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5810) loss tensor(2.2149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5894) loss tensor(1.8824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5965) loss tensor(1.9321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6009) loss tensor(2.0999, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6013) loss tensor(1.7092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6001) loss tensor(1.4618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6050) loss tensor(1.6988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6050) loss tensor(1.2786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6096) loss tensor(1.4836, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6130) loss tensor(1.3757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6168) loss tensor(1.5075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6168) loss tensor(1.9781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6203) loss tensor(1.3964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6170) loss tensor(1.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6183) loss tensor(1.1731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6119) loss tensor(1.4068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6138) loss tensor(1.3769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6145) loss tensor(1.6631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6135) loss tensor(1.4900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6138) loss tensor(1.3616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6178) loss tensor(1.0840, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6239) loss tensor(1.3004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6258) loss tensor(1.4068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6287) loss tensor(1.3799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6312) loss tensor(1.6609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6245) loss tensor(1.5472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6221) loss tensor(1.8138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6163) loss tensor(1.0571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6121) loss tensor(1.3113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6121) loss tensor(1.3877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6113) loss tensor(1.3507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6101) loss tensor(1.4642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6113) loss tensor(1.2495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6161) loss tensor(1.4347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6208) loss tensor(1.4887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6222) loss tensor(1.0737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6266) loss tensor(1.5067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6291) loss tensor(0.9807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6331) loss tensor(1.3714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6389) loss tensor(1.4036, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6382) loss tensor(1.6374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6421) loss tensor(0.7615, grad_fn=<NllLossBackward>)\n",
      "epoch: 48\n",
      "accuracy: tensor(0.6435) loss tensor(1.2870, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6415) loss tensor(1.4500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6370) loss tensor(1.1528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6363) loss tensor(1.2290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6381) loss tensor(1.4051, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6399) loss tensor(1.2186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6426) loss tensor(1.2440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6418) loss tensor(1.3574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6424) loss tensor(1.1379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6400) loss tensor(1.4774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6387) loss tensor(1.2805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6374) loss tensor(1.2996, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6347) loss tensor(1.1282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6344) loss tensor(1.5628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6328) loss tensor(1.8045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6336) loss tensor(1.2518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6310) loss tensor(1.1294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6325) loss tensor(1.1931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6342) loss tensor(1.4429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6358) loss tensor(1.1675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6354) loss tensor(1.3677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6355) loss tensor(0.8532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6349) loss tensor(1.2503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6354) loss tensor(1.5013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6362) loss tensor(1.3267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6355) loss tensor(1.2354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6367) loss tensor(1.3172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6375) loss tensor(1.2859, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6389) loss tensor(1.2052, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6398) loss tensor(1.0978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6403) loss tensor(1.1203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6389) loss tensor(1.0518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6457) loss tensor(1.0541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6487) loss tensor(1.2790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6472) loss tensor(1.0313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6429) loss tensor(1.1950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6402) loss tensor(1.1707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6383) loss tensor(1.1730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6390) loss tensor(1.2525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6375) loss tensor(1.2488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6386) loss tensor(1.0319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6418) loss tensor(1.1416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6422) loss tensor(1.5388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6438) loss tensor(0.9184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6442) loss tensor(0.9324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6449) loss tensor(0.9404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6464) loss tensor(1.5296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6462) loss tensor(1.1988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6456) loss tensor(1.2856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6463) loss tensor(1.0920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6482) loss tensor(1.0732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6475) loss tensor(1.3151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6470) loss tensor(1.0909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6505) loss tensor(1.1773, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6526) loss tensor(0.8882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6495) loss tensor(1.0852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6479) loss tensor(1.0259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6438) loss tensor(1.7980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6421) loss tensor(1.2209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6441) loss tensor(1.4832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6435) loss tensor(1.1534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6425) loss tensor(1.1653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6441) loss tensor(1.0426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6431) loss tensor(1.2858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6440) loss tensor(0.8934, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6458) loss tensor(1.3675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6471) loss tensor(1.3844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6477) loss tensor(0.9990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6478) loss tensor(1.1447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6513) loss tensor(1.3430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6520) loss tensor(1.2134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6529) loss tensor(1.0723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6502) loss tensor(1.1014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6506) loss tensor(0.9566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6502) loss tensor(1.3217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6528) loss tensor(1.2488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6552) loss tensor(1.1252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6546) loss tensor(1.1638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6515) loss tensor(1.2070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6485) loss tensor(1.0786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6433) loss tensor(0.9806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6403) loss tensor(1.0818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6416) loss tensor(1.0692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6414) loss tensor(0.9694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6413) loss tensor(1.3942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6398) loss tensor(1.2942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6382) loss tensor(1.0456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6379) loss tensor(0.9631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6432) loss tensor(1.2044, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6456) loss tensor(1.2525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6481) loss tensor(0.9407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6477) loss tensor(1.1834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6502) loss tensor(1.1766, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6486) loss tensor(1.1320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6487) loss tensor(1.4313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6392) loss tensor(1.1248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6295) loss tensor(1.3761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6487) loss tensor(1.5472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6579) loss tensor(1.4437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6606) loss tensor(1.5523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6582) loss tensor(1.0942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6466) loss tensor(1.4850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6389) loss tensor(1.1470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6343) loss tensor(1.1325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6342) loss tensor(1.0245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6345) loss tensor(1.0976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6374) loss tensor(1.2844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6355) loss tensor(1.1182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6335) loss tensor(1.5432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6353) loss tensor(1.6172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6328) loss tensor(1.1327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6327) loss tensor(1.4461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6309) loss tensor(1.1249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6286) loss tensor(1.1623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6288) loss tensor(1.1312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6343) loss tensor(1.0380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6377) loss tensor(1.6538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6407) loss tensor(1.5390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6449) loss tensor(1.0724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6430) loss tensor(1.2496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6470) loss tensor(1.2310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6475) loss tensor(1.2675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6422) loss tensor(1.0880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6414) loss tensor(0.9639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6402) loss tensor(1.0271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6411) loss tensor(1.0181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6406) loss tensor(1.3142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6391) loss tensor(0.8688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6419) loss tensor(1.0373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6455) loss tensor(1.3931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6505) loss tensor(1.2684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6533) loss tensor(0.9893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6551) loss tensor(1.3411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6539) loss tensor(1.1132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6539) loss tensor(1.3134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6550) loss tensor(1.2820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6551) loss tensor(1.2707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6543) loss tensor(1.1643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6579) loss tensor(1.3446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6587) loss tensor(1.1593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6576) loss tensor(1.2471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6579) loss tensor(1.0719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6587) loss tensor(1.0987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6591) loss tensor(1.2048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6597) loss tensor(1.4499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6585) loss tensor(0.9757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6591) loss tensor(0.9714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6583) loss tensor(1.1835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6569) loss tensor(1.4285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6576) loss tensor(1.2293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6561) loss tensor(0.8558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6555) loss tensor(1.2617, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6518) loss tensor(1.0815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6520) loss tensor(1.3143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6490) loss tensor(1.1882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6463) loss tensor(0.7521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6393) loss tensor(0.9696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6358) loss tensor(1.0624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6357) loss tensor(0.8940, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6345) loss tensor(1.0942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6385) loss tensor(0.9513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6394) loss tensor(0.8729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6405) loss tensor(0.9131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6415) loss tensor(1.2455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6433) loss tensor(1.1761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6456) loss tensor(1.2080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6499) loss tensor(1.1531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6502) loss tensor(1.2926, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6486) loss tensor(1.3677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6498) loss tensor(1.2283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6510) loss tensor(1.0736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6534) loss tensor(1.3876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6557) loss tensor(0.9350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6539) loss tensor(0.9824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6545) loss tensor(1.2351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6563) loss tensor(1.0364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6563) loss tensor(0.9373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6569) loss tensor(1.4098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6570) loss tensor(1.0768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6571) loss tensor(1.3150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6563) loss tensor(1.2365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6568) loss tensor(1.3576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6581) loss tensor(0.9305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6570) loss tensor(1.2403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6577) loss tensor(1.3496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6577) loss tensor(0.8502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6578) loss tensor(1.0449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6545) loss tensor(1.3029, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6529) loss tensor(1.2759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6481) loss tensor(1.0451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6456) loss tensor(1.0727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6501) loss tensor(0.9445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6558) loss tensor(0.9907, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6584) loss tensor(1.0286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6582) loss tensor(1.2729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6613) loss tensor(1.0460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6595) loss tensor(0.9416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6553) loss tensor(1.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6534) loss tensor(1.6328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6530) loss tensor(1.4657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6498) loss tensor(0.9631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6485) loss tensor(1.0873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6442) loss tensor(1.3778, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6432) loss tensor(1.3380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6433) loss tensor(0.9513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6403) loss tensor(0.9986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6397) loss tensor(1.0035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6362) loss tensor(1.2251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6342) loss tensor(1.2162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6373) loss tensor(1.1748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6405) loss tensor(1.0795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6413) loss tensor(1.2372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6401) loss tensor(1.0540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6431) loss tensor(1.1874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6448) loss tensor(1.2088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6459) loss tensor(1.1822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6475) loss tensor(1.3268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6494) loss tensor(1.4151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6496) loss tensor(1.0713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6491) loss tensor(1.1710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6509) loss tensor(1.2486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6531) loss tensor(1.2905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6530) loss tensor(1.2886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6523) loss tensor(0.9916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6521) loss tensor(1.1087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6533) loss tensor(1.0906, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6573) loss tensor(1.2961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6577) loss tensor(1.0496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6590) loss tensor(0.8991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6598) loss tensor(0.9831, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6593) loss tensor(1.1839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6586) loss tensor(0.9766, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6573) loss tensor(1.3482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6550) loss tensor(0.8466, grad_fn=<NllLossBackward>)\n",
      "epoch: 49\n",
      "accuracy: tensor(0.6547) loss tensor(0.9866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6547) loss tensor(1.3072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6514) loss tensor(0.9683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6501) loss tensor(1.0604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6501) loss tensor(0.7892, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6496) loss tensor(1.3522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6507) loss tensor(0.7517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6544) loss tensor(0.9451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6575) loss tensor(0.9883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6622) loss tensor(1.1043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6618) loss tensor(1.1075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6585) loss tensor(0.8257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6611) loss tensor(0.9948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6638) loss tensor(0.8128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6634) loss tensor(1.1088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6635) loss tensor(1.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6641) loss tensor(0.8624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6648) loss tensor(0.9761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6643) loss tensor(1.0948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6631) loss tensor(0.9347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6626) loss tensor(0.7657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6632) loss tensor(1.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6618) loss tensor(0.6731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6630) loss tensor(1.0355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6667) loss tensor(1.1653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6661) loss tensor(0.8291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6633) loss tensor(0.9082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6650) loss tensor(1.1653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6640) loss tensor(1.0498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6629) loss tensor(1.2189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6624) loss tensor(1.0024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6626) loss tensor(1.0812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6603) loss tensor(1.0141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6586) loss tensor(1.1719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6607) loss tensor(0.9980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6618) loss tensor(0.7987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6646) loss tensor(1.1354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6647) loss tensor(1.0791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6635) loss tensor(0.8978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6635) loss tensor(1.5380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6640) loss tensor(1.1196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6632) loss tensor(1.0520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6624) loss tensor(1.0347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6629) loss tensor(0.9796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6650) loss tensor(1.2579, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6655) loss tensor(0.9401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6661) loss tensor(0.9495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6669) loss tensor(1.0032, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6675) loss tensor(0.5650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6681) loss tensor(1.1123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6675) loss tensor(0.9629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6654) loss tensor(0.6679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6635) loss tensor(0.9811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6633) loss tensor(0.8397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6600) loss tensor(0.8383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6569) loss tensor(1.0590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6576) loss tensor(1.0942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6626) loss tensor(0.7443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6673) loss tensor(0.9387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6691) loss tensor(1.0375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6666) loss tensor(1.0417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6638) loss tensor(1.0961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6629) loss tensor(1.0962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6621) loss tensor(0.7208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6601) loss tensor(1.2504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6611) loss tensor(0.9339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6609) loss tensor(1.0803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6627) loss tensor(0.8941, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6663) loss tensor(1.0402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6671) loss tensor(0.8828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6678) loss tensor(0.8890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6694) loss tensor(0.9159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6682) loss tensor(1.2795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6657) loss tensor(1.1343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6647) loss tensor(0.9466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6633) loss tensor(0.8950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6647) loss tensor(0.9751, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6688) loss tensor(0.9834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6710) loss tensor(1.0548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6705) loss tensor(0.8131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6680) loss tensor(0.9732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6657) loss tensor(0.8942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6639) loss tensor(1.0634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6616) loss tensor(0.7506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6625) loss tensor(0.7752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6640) loss tensor(1.2621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6627) loss tensor(0.9928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6641) loss tensor(0.9567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6675) loss tensor(1.0403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6672) loss tensor(0.7770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6703) loss tensor(1.0548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6734) loss tensor(1.2545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6735) loss tensor(0.9131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6721) loss tensor(0.9582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6711) loss tensor(0.8778, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6707) loss tensor(1.0108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6707) loss tensor(0.9855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6703) loss tensor(0.8560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6709) loss tensor(1.1960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6671) loss tensor(0.8648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6643) loss tensor(1.1809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6657) loss tensor(1.1180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6634) loss tensor(1.1062, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6649) loss tensor(1.1881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6647) loss tensor(1.3991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6666) loss tensor(0.8768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6688) loss tensor(1.4375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6688) loss tensor(0.7564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6695) loss tensor(1.1297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6723) loss tensor(1.1829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6725) loss tensor(1.2584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6715) loss tensor(0.9679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6717) loss tensor(0.9285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6723) loss tensor(1.1183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6713) loss tensor(0.8270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6718) loss tensor(0.9370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6752) loss tensor(1.1099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6747) loss tensor(1.3645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6762) loss tensor(0.8722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6743) loss tensor(0.8993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6735) loss tensor(1.0000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6744) loss tensor(1.0717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6728) loss tensor(1.3419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6736) loss tensor(0.7423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6719) loss tensor(0.8518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6707) loss tensor(0.9636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6685) loss tensor(1.0172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6698) loss tensor(1.0550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6678) loss tensor(0.8421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6675) loss tensor(0.9632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6677) loss tensor(1.1153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6669) loss tensor(1.3937, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6647) loss tensor(1.0742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6646) loss tensor(1.0522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6646) loss tensor(1.0260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6661) loss tensor(1.0119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6666) loss tensor(1.0124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6666) loss tensor(1.0943, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6683) loss tensor(0.8462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6695) loss tensor(1.1524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6697) loss tensor(1.0994, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6717) loss tensor(1.0976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6719) loss tensor(1.1278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6712) loss tensor(1.1689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6730) loss tensor(1.2009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6739) loss tensor(1.3156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6758) loss tensor(1.0339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6778) loss tensor(0.9015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6774) loss tensor(1.1034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6774) loss tensor(1.0756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6766) loss tensor(1.0196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6781) loss tensor(0.9786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6730) loss tensor(0.9876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6712) loss tensor(0.8727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6659) loss tensor(1.3066, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6663) loss tensor(0.8388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6662) loss tensor(0.9477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6661) loss tensor(1.0912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6662) loss tensor(1.0051, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6661) loss tensor(1.1867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6651) loss tensor(1.1532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6663) loss tensor(0.9234, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6671) loss tensor(1.0423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6682) loss tensor(1.0132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6680) loss tensor(1.1121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6679) loss tensor(0.9903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6675) loss tensor(1.0191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6706) loss tensor(1.1350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6718) loss tensor(1.2491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6728) loss tensor(0.8012, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6711) loss tensor(0.8833, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6729) loss tensor(1.1607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6704) loss tensor(0.8333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6694) loss tensor(0.7884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6691) loss tensor(1.2360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6672) loss tensor(0.9094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6675) loss tensor(0.8516, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6678) loss tensor(1.0447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6678) loss tensor(1.0024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6679) loss tensor(0.9932, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6670) loss tensor(1.0138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6675) loss tensor(1.0667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6687) loss tensor(1.0530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6671) loss tensor(1.0568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6659) loss tensor(0.8374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6634) loss tensor(1.1525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6630) loss tensor(0.8956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6635) loss tensor(1.2602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6622) loss tensor(1.1181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6637) loss tensor(0.6946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6655) loss tensor(0.7832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6659) loss tensor(1.0942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6654) loss tensor(0.9478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6694) loss tensor(1.1748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6694) loss tensor(1.0648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6712) loss tensor(1.0407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6713) loss tensor(0.8661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6717) loss tensor(1.2803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6719) loss tensor(0.8869, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6725) loss tensor(0.9116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6744) loss tensor(1.3675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6741) loss tensor(1.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6760) loss tensor(1.4125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6769) loss tensor(0.8824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6759) loss tensor(0.9947, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6753) loss tensor(0.9911, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6749) loss tensor(0.9151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6751) loss tensor(0.9612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6745) loss tensor(0.7669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6755) loss tensor(0.8821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6754) loss tensor(1.1830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6747) loss tensor(0.8403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6735) loss tensor(0.9046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6726) loss tensor(0.9329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6720) loss tensor(1.2593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6696) loss tensor(0.7258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6667) loss tensor(0.8410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6667) loss tensor(0.7355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6669) loss tensor(0.7105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6671) loss tensor(0.8964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6678) loss tensor(0.8167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6670) loss tensor(1.2356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6670) loss tensor(0.9545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6672) loss tensor(0.8509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6677) loss tensor(0.9783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6680) loss tensor(0.9834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6654) loss tensor(1.3860, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6658) loss tensor(0.8722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6650) loss tensor(1.0370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6653) loss tensor(1.0946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6665) loss tensor(0.9439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6649) loss tensor(0.9965, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6656) loss tensor(1.0524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6653) loss tensor(1.1738, grad_fn=<NllLossBackward>)\n",
      "epoch: 50\n",
      "accuracy: tensor(0.6639) loss tensor(0.8401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6639) loss tensor(0.8749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6651) loss tensor(0.8828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6646) loss tensor(0.7262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6625) loss tensor(0.6152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6616) loss tensor(0.9068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6635) loss tensor(1.2486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6640) loss tensor(1.0055, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6649) loss tensor(1.0514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6658) loss tensor(0.8831, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6665) loss tensor(0.5763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6666) loss tensor(0.8174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6690) loss tensor(0.9277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6705) loss tensor(1.0716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6728) loss tensor(1.1766, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6741) loss tensor(0.7989, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6751) loss tensor(0.8134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6762) loss tensor(0.9322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6751) loss tensor(1.0118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6745) loss tensor(0.8434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6751) loss tensor(0.9032, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6734) loss tensor(1.0156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6742) loss tensor(0.6403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6731) loss tensor(0.6950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6730) loss tensor(0.8423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6739) loss tensor(1.0073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6744) loss tensor(0.8895, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6762) loss tensor(0.8302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6782) loss tensor(0.8954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6803) loss tensor(0.8223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6805) loss tensor(0.6666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6813) loss tensor(0.8634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6818) loss tensor(0.7652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6759) loss tensor(0.8125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6747) loss tensor(0.8401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6745) loss tensor(1.0208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6759) loss tensor(1.2478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6790) loss tensor(1.0091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6826) loss tensor(1.0079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6773) loss tensor(1.0112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6766) loss tensor(0.7908, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6681) loss tensor(0.7335, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6631) loss tensor(0.9923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6591) loss tensor(0.8958, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6551) loss tensor(0.9266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6533) loss tensor(0.8675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6503) loss tensor(1.0162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6461) loss tensor(1.0578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6488) loss tensor(1.5307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6525) loss tensor(1.2398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6568) loss tensor(1.0827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6651) loss tensor(1.1110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6707) loss tensor(0.8981, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6697) loss tensor(0.7819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6726) loss tensor(0.7293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6729) loss tensor(0.9123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6679) loss tensor(1.0163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6631) loss tensor(1.2387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6130) loss tensor(0.7602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6741) loss tensor(1.1545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6504) loss tensor(0.9757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6337) loss tensor(0.9045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6186) loss tensor(1.3975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6023) loss tensor(1.1281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5904) loss tensor(1.5559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5791) loss tensor(1.2349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5710) loss tensor(1.6777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5645) loss tensor(1.6601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5607) loss tensor(1.8061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5582) loss tensor(1.3343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5584) loss tensor(1.5936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5571) loss tensor(1.7621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5553) loss tensor(1.6906, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5602) loss tensor(1.9142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5606) loss tensor(1.6638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5499) loss tensor(1.7585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5461) loss tensor(1.9602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5502) loss tensor(1.6109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5515) loss tensor(1.6299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5549) loss tensor(1.6562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5576) loss tensor(1.2626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5582) loss tensor(1.8520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5578) loss tensor(2.1606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5585) loss tensor(1.7833, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5615) loss tensor(2.0655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5635) loss tensor(2.1680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5667) loss tensor(1.8545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5649) loss tensor(1.9292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5656) loss tensor(1.4193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5627) loss tensor(1.8230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5618) loss tensor(1.3958, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5650) loss tensor(1.7120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5693) loss tensor(1.6763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5704) loss tensor(1.6921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5699) loss tensor(1.6048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5669) loss tensor(1.8474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5665) loss tensor(1.6518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5639) loss tensor(1.5825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5622) loss tensor(1.8719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5605) loss tensor(1.4276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5613) loss tensor(1.7016, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5611) loss tensor(1.1354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5618) loss tensor(1.5961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5633) loss tensor(1.5404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5635) loss tensor(1.9594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5695) loss tensor(1.6125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5706) loss tensor(1.1298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5710) loss tensor(1.6063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5734) loss tensor(1.9113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5753) loss tensor(1.4781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5754) loss tensor(1.5092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5757) loss tensor(1.6055, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5806) loss tensor(1.7591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5807) loss tensor(1.4220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5833) loss tensor(1.6487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5830) loss tensor(1.4844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5831) loss tensor(1.5848, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5837) loss tensor(1.7599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5854) loss tensor(1.4918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5879) loss tensor(1.4796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5893) loss tensor(1.4654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5901) loss tensor(1.5384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5902) loss tensor(1.3104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5889) loss tensor(1.4443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5885) loss tensor(1.4052, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5887) loss tensor(1.7314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5880) loss tensor(1.5787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5921) loss tensor(1.6671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5967) loss tensor(1.8428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5976) loss tensor(1.6508, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6003) loss tensor(1.5537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6019) loss tensor(1.2532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6041) loss tensor(1.3443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6061) loss tensor(1.5096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6081) loss tensor(1.4809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6111) loss tensor(1.5135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6095) loss tensor(1.3003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6113) loss tensor(1.2354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6149) loss tensor(1.6491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6133) loss tensor(1.6064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6089) loss tensor(1.7682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6082) loss tensor(1.7000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6056) loss tensor(1.5536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6075) loss tensor(1.2301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6051) loss tensor(1.4415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6024) loss tensor(1.5417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6016) loss tensor(1.3057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6039) loss tensor(1.5910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6010) loss tensor(1.7965, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.5999) loss tensor(1.4647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6007) loss tensor(1.5195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6032) loss tensor(1.4423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6062) loss tensor(1.6530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6119) loss tensor(1.2197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6125) loss tensor(1.5512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6141) loss tensor(1.2548, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6155) loss tensor(1.3704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6182) loss tensor(1.1469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6217) loss tensor(2.0070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6225) loss tensor(1.4272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6226) loss tensor(1.6435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6230) loss tensor(1.1948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6221) loss tensor(1.1557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6235) loss tensor(1.5422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6261) loss tensor(1.5293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6243) loss tensor(1.1802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6267) loss tensor(1.3419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6279) loss tensor(1.3481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6253) loss tensor(1.5813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6256) loss tensor(1.2547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6262) loss tensor(1.5814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6297) loss tensor(1.4483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6271) loss tensor(1.8308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6282) loss tensor(1.2676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6271) loss tensor(1.3564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6301) loss tensor(1.1701, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6318) loss tensor(1.3958, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6322) loss tensor(1.2040, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6323) loss tensor(1.3178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6341) loss tensor(1.2226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6352) loss tensor(1.1933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6369) loss tensor(1.4848, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6370) loss tensor(1.1902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6408) loss tensor(1.2067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6414) loss tensor(1.1124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6432) loss tensor(1.0494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6409) loss tensor(1.1110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6429) loss tensor(1.1525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6440) loss tensor(1.2161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6449) loss tensor(1.4721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6447) loss tensor(1.4761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6445) loss tensor(1.1831, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6465) loss tensor(1.1312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6472) loss tensor(1.2004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6479) loss tensor(1.0545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6497) loss tensor(1.0689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6489) loss tensor(1.0048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6498) loss tensor(1.0507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6504) loss tensor(1.3190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6525) loss tensor(1.1777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6557) loss tensor(0.8761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6567) loss tensor(0.9250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6560) loss tensor(1.0264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6559) loss tensor(1.3249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6558) loss tensor(1.2410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6549) loss tensor(1.1008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6558) loss tensor(0.9431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6577) loss tensor(1.0254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6583) loss tensor(1.0986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6574) loss tensor(0.9075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6567) loss tensor(1.0707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6578) loss tensor(1.0225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6571) loss tensor(1.1970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6571) loss tensor(1.0246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6592) loss tensor(0.9039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6590) loss tensor(0.9884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6595) loss tensor(0.8883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6616) loss tensor(1.0761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6645) loss tensor(0.8763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6658) loss tensor(1.0899, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6677) loss tensor(0.7237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6704) loss tensor(0.8909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6704) loss tensor(0.7010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6721) loss tensor(0.9648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6733) loss tensor(1.0177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6731) loss tensor(0.6853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6723) loss tensor(1.0004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6750) loss tensor(0.8499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6761) loss tensor(1.0232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6745) loss tensor(0.7493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6755) loss tensor(1.0549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6751) loss tensor(1.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6762) loss tensor(0.9934, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6773) loss tensor(0.6412, grad_fn=<NllLossBackward>)\n",
      "epoch: 51\n",
      "accuracy: tensor(0.6763) loss tensor(0.8635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6760) loss tensor(0.9984, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6768) loss tensor(0.6945, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6759) loss tensor(0.9806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6771) loss tensor(0.9451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6750) loss tensor(0.9890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6725) loss tensor(0.8167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6714) loss tensor(0.6733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6701) loss tensor(0.8374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6686) loss tensor(1.0146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6691) loss tensor(1.0265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6686) loss tensor(0.6995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6691) loss tensor(1.0693, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6714) loss tensor(0.7210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6739) loss tensor(1.0437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6749) loss tensor(1.0886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6762) loss tensor(1.1450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6768) loss tensor(0.7527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6777) loss tensor(0.9519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6774) loss tensor(0.9219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6771) loss tensor(0.8948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6776) loss tensor(0.8214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6790) loss tensor(0.8951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6805) loss tensor(0.7156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6792) loss tensor(0.5915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6782) loss tensor(0.9050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6790) loss tensor(0.8420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6763) loss tensor(1.1363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6765) loss tensor(0.9669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6770) loss tensor(1.0531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6762) loss tensor(0.9343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6754) loss tensor(0.8622, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6757) loss tensor(0.8086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6769) loss tensor(1.0556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6755) loss tensor(0.6136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6767) loss tensor(1.0519, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6775) loss tensor(0.9083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6789) loss tensor(0.7170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6789) loss tensor(0.8547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6776) loss tensor(0.8942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6761) loss tensor(0.9034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6768) loss tensor(1.0841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6741) loss tensor(0.9606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6750) loss tensor(0.9437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6765) loss tensor(0.8294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6776) loss tensor(0.8671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6778) loss tensor(0.9227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6795) loss tensor(0.9546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6811) loss tensor(0.7913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6813) loss tensor(0.8720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6801) loss tensor(0.7140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6800) loss tensor(1.0493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6786) loss tensor(0.6052, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6782) loss tensor(0.8022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6774) loss tensor(0.7548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6777) loss tensor(0.8818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6792) loss tensor(1.0389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6803) loss tensor(0.8265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6814) loss tensor(0.9655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6825) loss tensor(0.9155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6825) loss tensor(0.9107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6817) loss tensor(0.9238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6817) loss tensor(0.8885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6819) loss tensor(0.9236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6831) loss tensor(0.5216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6825) loss tensor(0.9296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6827) loss tensor(1.0722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6825) loss tensor(0.8795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6810) loss tensor(0.9872, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6811) loss tensor(1.1162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6816) loss tensor(0.7970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6833) loss tensor(0.8237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6809) loss tensor(0.8848, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6811) loss tensor(0.6920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6830) loss tensor(0.7041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6830) loss tensor(1.0898, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6827) loss tensor(0.8158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6846) loss tensor(0.8326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6850) loss tensor(1.0852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6846) loss tensor(1.0128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6835) loss tensor(0.5876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6845) loss tensor(1.0521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6865) loss tensor(0.7485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6865) loss tensor(0.8865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6854) loss tensor(0.8199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6845) loss tensor(0.9240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6858) loss tensor(0.5692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6858) loss tensor(0.8084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6857) loss tensor(0.8561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6866) loss tensor(0.9105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6894) loss tensor(0.8994, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6905) loss tensor(1.0741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6911) loss tensor(0.8298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6906) loss tensor(0.8022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6906) loss tensor(0.8932, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6898) loss tensor(0.9117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6904) loss tensor(0.9885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6890) loss tensor(0.8038, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6874) loss tensor(0.8438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6870) loss tensor(0.8163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6847) loss tensor(0.7670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6833) loss tensor(0.9255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6815) loss tensor(0.8663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6823) loss tensor(0.8536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6818) loss tensor(0.8982, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6823) loss tensor(0.8570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6814) loss tensor(0.7661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6799) loss tensor(0.6522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6803) loss tensor(0.7110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6824) loss tensor(0.9470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6823) loss tensor(0.9641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6837) loss tensor(0.9426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6841) loss tensor(0.8860, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6841) loss tensor(0.9877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6839) loss tensor(0.6854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6833) loss tensor(0.9116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6846) loss tensor(0.8373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6887) loss tensor(1.0294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6881) loss tensor(0.6947, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6882) loss tensor(1.0354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6871) loss tensor(0.8685, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6858) loss tensor(0.5232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6853) loss tensor(0.8524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6861) loss tensor(0.9073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6864) loss tensor(0.7716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6851) loss tensor(0.7973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6858) loss tensor(0.7131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6849) loss tensor(0.7933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6849) loss tensor(0.6565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6847) loss tensor(0.7089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6850) loss tensor(1.0684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6863) loss tensor(1.1360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6877) loss tensor(0.6514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6877) loss tensor(1.0980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6891) loss tensor(0.9007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6902) loss tensor(0.9179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6904) loss tensor(0.8082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6893) loss tensor(0.6571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6888) loss tensor(0.8253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6870) loss tensor(1.1188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6888) loss tensor(1.0049, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6873) loss tensor(0.7171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6859) loss tensor(0.7678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6853) loss tensor(0.6811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6841) loss tensor(0.7854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6835) loss tensor(0.9220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6823) loss tensor(0.6789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6829) loss tensor(0.8195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6856) loss tensor(0.8704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6854) loss tensor(0.8145, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6846) loss tensor(0.6823, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6845) loss tensor(0.7138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6853) loss tensor(0.7607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6849) loss tensor(0.8499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6835) loss tensor(0.9332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6833) loss tensor(1.1215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6846) loss tensor(0.8958, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6843) loss tensor(0.9369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6861) loss tensor(0.8357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6870) loss tensor(0.8985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6869) loss tensor(0.6331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6850) loss tensor(1.0954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6834) loss tensor(1.1570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6846) loss tensor(0.9495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6823) loss tensor(0.8484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6830) loss tensor(0.8590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6822) loss tensor(1.0100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6831) loss tensor(0.9518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6847) loss tensor(1.0420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6840) loss tensor(1.0923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6821) loss tensor(0.7891, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6842) loss tensor(0.9839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6848) loss tensor(1.1391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6841) loss tensor(1.0642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6849) loss tensor(0.7014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6857) loss tensor(0.7898, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6865) loss tensor(0.6501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6878) loss tensor(1.0434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6883) loss tensor(0.6037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6896) loss tensor(0.9979, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6882) loss tensor(0.7849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6881) loss tensor(0.8146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6873) loss tensor(0.9272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6855) loss tensor(0.8073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6841) loss tensor(0.9348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6833) loss tensor(1.0428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6830) loss tensor(0.8949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6835) loss tensor(0.5482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6815) loss tensor(0.7152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6798) loss tensor(0.8565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6805) loss tensor(0.7450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6798) loss tensor(0.8426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6816) loss tensor(0.8265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6778) loss tensor(0.7087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6782) loss tensor(0.9654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6799) loss tensor(0.9674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6808) loss tensor(0.8960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6821) loss tensor(0.6519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6830) loss tensor(0.9306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6829) loss tensor(0.9589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6831) loss tensor(0.7443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6849) loss tensor(1.0305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6851) loss tensor(0.7118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6877) loss tensor(0.8862, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6861) loss tensor(0.6952, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6870) loss tensor(0.6627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6872) loss tensor(0.7688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6870) loss tensor(1.0622, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6869) loss tensor(0.8844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6846) loss tensor(0.8507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6838) loss tensor(0.7589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6826) loss tensor(0.7466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6806) loss tensor(0.8667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6809) loss tensor(0.8846, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6795) loss tensor(0.8275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6813) loss tensor(0.8318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6825) loss tensor(0.8154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6825) loss tensor(0.6666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6850) loss tensor(0.8451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6838) loss tensor(0.8052, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6835) loss tensor(0.7858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6843) loss tensor(0.9551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6863) loss tensor(0.8620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6886) loss tensor(0.9079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6895) loss tensor(1.2415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6920) loss tensor(0.9383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6934) loss tensor(0.8325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6946) loss tensor(0.9578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6939) loss tensor(1.0751, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6917) loss tensor(0.8460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6903) loss tensor(0.8148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6894) loss tensor(1.0424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6885) loss tensor(0.8146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6882) loss tensor(0.9395, grad_fn=<NllLossBackward>)\n",
      "epoch: 52\n",
      "accuracy: tensor(0.6889) loss tensor(0.5301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6910) loss tensor(1.1203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6881) loss tensor(0.9372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6879) loss tensor(0.7219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6877) loss tensor(0.8192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6846) loss tensor(0.7806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6830) loss tensor(0.8818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6826) loss tensor(0.8156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6839) loss tensor(0.9172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6855) loss tensor(0.6962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6861) loss tensor(0.8204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6891) loss tensor(0.7912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6880) loss tensor(0.9049, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6883) loss tensor(0.6125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6871) loss tensor(0.7608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6879) loss tensor(0.4121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6887) loss tensor(0.8282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6872) loss tensor(0.8282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6874) loss tensor(0.7817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6883) loss tensor(0.7851, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6887) loss tensor(0.5690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6882) loss tensor(1.1039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6883) loss tensor(0.8574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6874) loss tensor(0.7428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6871) loss tensor(0.8459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6857) loss tensor(0.8665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6857) loss tensor(0.6925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6859) loss tensor(0.8768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6861) loss tensor(0.6894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6853) loss tensor(0.7035, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6846) loss tensor(0.6145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6871) loss tensor(0.9380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6871) loss tensor(0.7287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6882) loss tensor(0.6612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6879) loss tensor(0.6003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6883) loss tensor(0.8069, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6890) loss tensor(0.8356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6877) loss tensor(0.9492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6870) loss tensor(0.8871, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6855) loss tensor(0.8962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6856) loss tensor(0.6260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6869) loss tensor(0.8719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6875) loss tensor(0.8372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6878) loss tensor(0.5787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6897) loss tensor(0.6908, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6901) loss tensor(0.7188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6887) loss tensor(0.6498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6906) loss tensor(0.7729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6883) loss tensor(0.6550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6896) loss tensor(0.8342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6909) loss tensor(0.9414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6921) loss tensor(0.5733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6943) loss tensor(0.6261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6931) loss tensor(0.6031, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6934) loss tensor(0.8463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6936) loss tensor(0.8566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6946) loss tensor(0.6403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6947) loss tensor(0.5063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6933) loss tensor(0.7003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6933) loss tensor(0.6757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6907) loss tensor(0.8379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6930) loss tensor(0.7542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6930) loss tensor(0.8275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6943) loss tensor(1.0227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6951) loss tensor(0.8659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6936) loss tensor(0.8820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6938) loss tensor(0.7081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6934) loss tensor(0.9140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6938) loss tensor(0.7659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6943) loss tensor(0.6494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6943) loss tensor(0.7517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6933) loss tensor(0.9595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6910) loss tensor(0.6337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6894) loss tensor(1.1165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6894) loss tensor(0.8230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6890) loss tensor(0.9133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6917) loss tensor(0.5849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6950) loss tensor(0.9101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6965) loss tensor(1.0756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6975) loss tensor(0.8317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6973) loss tensor(0.7709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6962) loss tensor(0.8628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6961) loss tensor(0.6713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6954) loss tensor(0.8527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6966) loss tensor(0.7247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6982) loss tensor(0.6950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6946) loss tensor(0.8044, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6929) loss tensor(0.7825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6935) loss tensor(0.6442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6910) loss tensor(0.6467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6911) loss tensor(0.9843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6882) loss tensor(0.8068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6872) loss tensor(0.7643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6886) loss tensor(0.7728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6890) loss tensor(0.9246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6889) loss tensor(0.6026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6891) loss tensor(0.9441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6905) loss tensor(1.0396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6911) loss tensor(0.7290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6902) loss tensor(0.7955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6898) loss tensor(0.7793, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6891) loss tensor(0.8130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6909) loss tensor(0.7345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6904) loss tensor(0.7352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6888) loss tensor(0.6874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6888) loss tensor(0.6602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6856) loss tensor(0.9347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6842) loss tensor(0.8749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6853) loss tensor(0.9415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6864) loss tensor(0.6181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6871) loss tensor(0.9126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6881) loss tensor(0.8649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6896) loss tensor(0.8264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6901) loss tensor(0.8447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6910) loss tensor(0.9718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6937) loss tensor(0.6995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6944) loss tensor(0.8433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6960) loss tensor(0.7652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6954) loss tensor(0.9857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6923) loss tensor(0.7721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6930) loss tensor(0.6843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6922) loss tensor(0.9041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6895) loss tensor(0.7374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6887) loss tensor(0.6826, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6867) loss tensor(0.7305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6862) loss tensor(0.6815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6873) loss tensor(0.9568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6847) loss tensor(0.7679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6862) loss tensor(0.7930, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6851) loss tensor(0.7700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6835) loss tensor(0.9222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6833) loss tensor(0.7033, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6823) loss tensor(0.7182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6834) loss tensor(0.8303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6859) loss tensor(1.0210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6872) loss tensor(0.7763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6867) loss tensor(0.6703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6869) loss tensor(0.6226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6882) loss tensor(0.8667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6888) loss tensor(0.5911, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6869) loss tensor(0.8511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6858) loss tensor(0.7367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6847) loss tensor(0.8574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6849) loss tensor(0.6182, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6832) loss tensor(0.9493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6827) loss tensor(0.5777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6839) loss tensor(1.1282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6848) loss tensor(0.6360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6848) loss tensor(0.9100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6867) loss tensor(0.7140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6895) loss tensor(0.8476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6909) loss tensor(0.7004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6919) loss tensor(0.7170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6914) loss tensor(0.8082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6945) loss tensor(0.7093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6946) loss tensor(0.8231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6957) loss tensor(0.9348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6952) loss tensor(0.8441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6943) loss tensor(0.8776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6930) loss tensor(0.6841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6914) loss tensor(0.8272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6909) loss tensor(0.8033, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6917) loss tensor(0.7296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6904) loss tensor(0.9115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6901) loss tensor(0.9596, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6902) loss tensor(0.8286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6899) loss tensor(0.8955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6889) loss tensor(0.7901, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6897) loss tensor(0.6479, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6899) loss tensor(0.9097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6902) loss tensor(0.6504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6891) loss tensor(0.6677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6918) loss tensor(0.8386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6917) loss tensor(0.8879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6941) loss tensor(0.9677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6945) loss tensor(0.7860, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6949) loss tensor(0.6965, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6955) loss tensor(0.7858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6953) loss tensor(0.7463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6951) loss tensor(0.6187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6954) loss tensor(0.9084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6958) loss tensor(1.0510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6961) loss tensor(0.7785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6958) loss tensor(0.9616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6945) loss tensor(0.6713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6930) loss tensor(0.6840, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6910) loss tensor(0.6074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6911) loss tensor(0.8469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6901) loss tensor(0.9908, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6919) loss tensor(0.9121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6910) loss tensor(0.8884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6901) loss tensor(0.7058, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6899) loss tensor(0.7954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6895) loss tensor(0.8214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6864) loss tensor(0.7459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6858) loss tensor(0.8486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6850) loss tensor(0.7884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6827) loss tensor(0.7882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6830) loss tensor(0.7728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6834) loss tensor(0.7996, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6845) loss tensor(0.7822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6857) loss tensor(0.8047, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6854) loss tensor(0.6386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6872) loss tensor(1.0040, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6883) loss tensor(1.0921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6893) loss tensor(0.8372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6911) loss tensor(0.7895, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6933) loss tensor(0.9477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6938) loss tensor(0.9020, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6955) loss tensor(0.7334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6942) loss tensor(0.7755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6909) loss tensor(0.5999, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6886) loss tensor(0.5851, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6872) loss tensor(0.8991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6858) loss tensor(0.9973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6871) loss tensor(0.9671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6875) loss tensor(0.6269, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6881) loss tensor(0.8438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6866) loss tensor(0.9113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6864) loss tensor(0.7117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6869) loss tensor(0.7020, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6841) loss tensor(0.9039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6856) loss tensor(0.8210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6862) loss tensor(0.8332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6856) loss tensor(0.8273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6872) loss tensor(0.7402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6874) loss tensor(0.5426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6883) loss tensor(0.7895, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6904) loss tensor(0.6974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6917) loss tensor(0.5014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6938) loss tensor(0.7087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6939) loss tensor(0.5618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6945) loss tensor(0.6491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6960) loss tensor(0.6214, grad_fn=<NllLossBackward>)\n",
      "epoch: 53\n",
      "accuracy: tensor(0.6968) loss tensor(0.7890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6969) loss tensor(0.7512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6965) loss tensor(0.6241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6953) loss tensor(0.6553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6942) loss tensor(0.5167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6952) loss tensor(0.6853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6942) loss tensor(0.6676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6941) loss tensor(0.8932, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6938) loss tensor(0.9593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6942) loss tensor(0.8346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6927) loss tensor(0.6040, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6929) loss tensor(0.6405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6929) loss tensor(0.5815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6935) loss tensor(0.6677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6934) loss tensor(0.6432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6929) loss tensor(0.6854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6933) loss tensor(0.5946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6929) loss tensor(0.7184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6918) loss tensor(0.8870, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6938) loss tensor(0.6166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6917) loss tensor(0.7189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6915) loss tensor(0.6454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6920) loss tensor(0.7045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6935) loss tensor(0.6742, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6939) loss tensor(0.7231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6954) loss tensor(0.6406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6963) loss tensor(0.7682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6971) loss tensor(0.8700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6983) loss tensor(0.5656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6989) loss tensor(0.6866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7001) loss tensor(0.6502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7015) loss tensor(0.7640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6994) loss tensor(0.6903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7002) loss tensor(0.7237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6997) loss tensor(0.8466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6993) loss tensor(0.5526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6991) loss tensor(0.7795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6959) loss tensor(0.7161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6955) loss tensor(0.8069, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6951) loss tensor(0.7633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6942) loss tensor(0.7138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6923) loss tensor(0.7325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6915) loss tensor(0.5439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6915) loss tensor(0.7360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6913) loss tensor(0.7177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6927) loss tensor(0.9496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6917) loss tensor(0.7015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6915) loss tensor(0.8070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6895) loss tensor(0.7331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6895) loss tensor(0.7227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6909) loss tensor(0.8248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6939) loss tensor(0.8734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6946) loss tensor(0.7215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6950) loss tensor(0.9084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6949) loss tensor(0.6317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6939) loss tensor(0.6560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6952) loss tensor(0.5453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6960) loss tensor(0.5087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6960) loss tensor(0.6935, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6957) loss tensor(0.6098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6934) loss tensor(0.6461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6926) loss tensor(0.6206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6923) loss tensor(0.5877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6936) loss tensor(0.6601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6915) loss tensor(0.8803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6922) loss tensor(0.7149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6926) loss tensor(0.7329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6929) loss tensor(0.6910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6938) loss tensor(0.6248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6947) loss tensor(0.7658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6947) loss tensor(0.6802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6957) loss tensor(0.9009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6950) loss tensor(0.7416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6946) loss tensor(0.5979, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6945) loss tensor(0.7178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6957) loss tensor(0.8563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6963) loss tensor(0.8266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6960) loss tensor(0.5749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6983) loss tensor(0.5854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6971) loss tensor(0.7099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6967) loss tensor(0.9205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6958) loss tensor(0.8006, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6962) loss tensor(0.8385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6986) loss tensor(0.8506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6993) loss tensor(0.6385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6987) loss tensor(0.6745, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7000) loss tensor(0.5692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6997) loss tensor(0.6834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7001) loss tensor(0.4915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6985) loss tensor(0.6258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6945) loss tensor(0.8107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6942) loss tensor(0.9311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6950) loss tensor(0.9215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6968) loss tensor(0.8677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6981) loss tensor(0.9304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6961) loss tensor(0.6219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6976) loss tensor(0.7012, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6975) loss tensor(0.6258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6953) loss tensor(0.6248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6935) loss tensor(0.6411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6925) loss tensor(0.6492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6911) loss tensor(0.6329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6881) loss tensor(0.7828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6872) loss tensor(0.5987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6849) loss tensor(0.8523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6864) loss tensor(0.7848, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6866) loss tensor(0.8083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6864) loss tensor(0.6441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6893) loss tensor(1.0030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6925) loss tensor(0.7105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6926) loss tensor(0.6358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6930) loss tensor(0.7319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6935) loss tensor(0.8821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6942) loss tensor(0.6731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6934) loss tensor(0.6916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6926) loss tensor(0.8152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6933) loss tensor(0.7101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6936) loss tensor(0.5647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6949) loss tensor(0.7018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6950) loss tensor(0.6999, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6923) loss tensor(0.8084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6933) loss tensor(0.5650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6950) loss tensor(0.5018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6951) loss tensor(0.7123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6968) loss tensor(0.7174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6982) loss tensor(0.5980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6984) loss tensor(0.6346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6984) loss tensor(0.9172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6977) loss tensor(0.5188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6994) loss tensor(0.6409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7001) loss tensor(0.9602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7002) loss tensor(0.8313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7005) loss tensor(0.6165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6999) loss tensor(0.7633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6990) loss tensor(0.8128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6984) loss tensor(0.6896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6994) loss tensor(0.6307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7000) loss tensor(0.5449, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6992) loss tensor(0.9119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6975) loss tensor(0.7507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6990) loss tensor(0.7326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6999) loss tensor(0.5868, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6975) loss tensor(0.9477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6952) loss tensor(0.7440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6959) loss tensor(0.7799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6944) loss tensor(0.5537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6935) loss tensor(0.8671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6937) loss tensor(0.6623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6930) loss tensor(0.6180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6946) loss tensor(0.8629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6909) loss tensor(0.5764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6902) loss tensor(0.8990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6888) loss tensor(0.7890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6903) loss tensor(0.6007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6913) loss tensor(0.8403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6927) loss tensor(0.5881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6915) loss tensor(0.7149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6912) loss tensor(0.7137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6938) loss tensor(0.8556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6952) loss tensor(0.5908, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6955) loss tensor(0.7412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6952) loss tensor(0.7425, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6958) loss tensor(0.8508, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6969) loss tensor(0.5738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6944) loss tensor(0.6913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6923) loss tensor(0.7844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6928) loss tensor(0.6145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6919) loss tensor(0.9292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6934) loss tensor(0.8065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6951) loss tensor(0.7224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6957) loss tensor(0.7808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6965) loss tensor(0.6928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6975) loss tensor(0.7079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6984) loss tensor(0.6025, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6987) loss tensor(0.8794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6991) loss tensor(0.8548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6995) loss tensor(0.6909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6987) loss tensor(0.9029, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7011) loss tensor(0.8937, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7021) loss tensor(0.6854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6994) loss tensor(0.8707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6973) loss tensor(0.6060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6973) loss tensor(0.8837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6971) loss tensor(0.6445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6958) loss tensor(0.5975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6937) loss tensor(0.7257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6935) loss tensor(0.6537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6927) loss tensor(0.9132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6929) loss tensor(0.8597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6922) loss tensor(0.6608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6919) loss tensor(0.8262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6942) loss tensor(0.7393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6958) loss tensor(0.7151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6966) loss tensor(0.7618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6982) loss tensor(0.6899, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6991) loss tensor(0.5679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6991) loss tensor(0.6417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6976) loss tensor(0.7138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6958) loss tensor(0.5897, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6944) loss tensor(0.5692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6943) loss tensor(0.7723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6935) loss tensor(0.8401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6930) loss tensor(0.5450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6933) loss tensor(0.8098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6923) loss tensor(0.7966, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6939) loss tensor(0.7106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6947) loss tensor(0.7428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6975) loss tensor(0.6767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6979) loss tensor(0.7336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6975) loss tensor(0.7946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6992) loss tensor(0.7595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6981) loss tensor(0.7018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7000) loss tensor(0.7901, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6994) loss tensor(0.7600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6997) loss tensor(0.7638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7000) loss tensor(0.8483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6985) loss tensor(1.0236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6970) loss tensor(0.6079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6963) loss tensor(0.6977, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6947) loss tensor(0.5874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6949) loss tensor(0.7821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6943) loss tensor(0.6459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6943) loss tensor(0.7113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6929) loss tensor(0.7724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6923) loss tensor(0.7231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6946) loss tensor(0.8918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6958) loss tensor(0.7332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6976) loss tensor(1.1689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6978) loss tensor(0.8041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6979) loss tensor(0.6900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6970) loss tensor(0.8889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6962) loss tensor(0.5637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6959) loss tensor(0.8876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6974) loss tensor(0.4924, grad_fn=<NllLossBackward>)\n",
      "epoch: 54\n",
      "accuracy: tensor(0.6951) loss tensor(0.7201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6939) loss tensor(0.6673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6943) loss tensor(0.6312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6926) loss tensor(0.7185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6945) loss tensor(0.7611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6952) loss tensor(0.5251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6965) loss tensor(0.8166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6984) loss tensor(0.7431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7002) loss tensor(0.6960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6987) loss tensor(0.5555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6961) loss tensor(0.8078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6954) loss tensor(0.9121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6976) loss tensor(0.5675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6979) loss tensor(0.8159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6963) loss tensor(0.9932, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6951) loss tensor(0.5498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6951) loss tensor(0.6517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6935) loss tensor(0.6524, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6919) loss tensor(0.6728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6920) loss tensor(0.7488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6902) loss tensor(0.5450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6886) loss tensor(0.6691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6902) loss tensor(0.5490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6909) loss tensor(0.5752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6923) loss tensor(0.6164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6934) loss tensor(0.6164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6949) loss tensor(0.8886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6987) loss tensor(0.6071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7005) loss tensor(0.7287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7018) loss tensor(0.5280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7015) loss tensor(0.7908, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7009) loss tensor(0.5462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6998) loss tensor(0.7212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7016) loss tensor(0.6416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7005) loss tensor(0.4509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6975) loss tensor(0.6190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6965) loss tensor(0.7680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6961) loss tensor(0.6765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6974) loss tensor(0.5652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7007) loss tensor(0.8774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7019) loss tensor(0.5629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7025) loss tensor(0.4926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7017) loss tensor(0.6477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7006) loss tensor(0.7781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7011) loss tensor(0.8123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7003) loss tensor(0.6349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6999) loss tensor(0.7287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6993) loss tensor(0.4877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6983) loss tensor(0.5410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6987) loss tensor(0.6071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6997) loss tensor(0.5231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6986) loss tensor(0.5001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6970) loss tensor(0.7903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6983) loss tensor(0.5446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6990) loss tensor(0.6000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6969) loss tensor(0.5363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6974) loss tensor(0.4933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6983) loss tensor(0.5793, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6986) loss tensor(0.6355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6982) loss tensor(0.7533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6985) loss tensor(0.5480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6990) loss tensor(0.5694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7005) loss tensor(0.5980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7007) loss tensor(0.9306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7003) loss tensor(0.7221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6993) loss tensor(0.5428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6965) loss tensor(0.5853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6969) loss tensor(0.8464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6970) loss tensor(0.6165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6981) loss tensor(0.4847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7000) loss tensor(0.9383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7002) loss tensor(0.5133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7002) loss tensor(0.6066, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7014) loss tensor(0.5561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6997) loss tensor(0.7192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7010) loss tensor(0.9534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7017) loss tensor(0.7829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7003) loss tensor(0.6732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6976) loss tensor(0.4531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6984) loss tensor(0.7793, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6973) loss tensor(0.5803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6981) loss tensor(0.7678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6990) loss tensor(0.7264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6997) loss tensor(0.6207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7003) loss tensor(0.5041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7006) loss tensor(0.7075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7005) loss tensor(0.7853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7003) loss tensor(0.6812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7007) loss tensor(0.8009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7027) loss tensor(0.5919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7023) loss tensor(0.7186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7021) loss tensor(0.6500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.7682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7050) loss tensor(0.6367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7030) loss tensor(0.6491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7014) loss tensor(0.7125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6998) loss tensor(0.7083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6993) loss tensor(0.5565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6979) loss tensor(0.5515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6994) loss tensor(0.6377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7001) loss tensor(0.5413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7006) loss tensor(0.8061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6992) loss tensor(0.6452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6995) loss tensor(0.7282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7000) loss tensor(0.6259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6998) loss tensor(0.7132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6984) loss tensor(0.6844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6995) loss tensor(0.5812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6974) loss tensor(0.6294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6982) loss tensor(0.6486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6971) loss tensor(0.7271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6975) loss tensor(0.6645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6990) loss tensor(0.7978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7007) loss tensor(0.8578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7010) loss tensor(0.5556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7029) loss tensor(0.7037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7011) loss tensor(0.6969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6997) loss tensor(0.6201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6998) loss tensor(0.6746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6979) loss tensor(0.7503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6967) loss tensor(0.7169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6963) loss tensor(0.7471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6959) loss tensor(0.5998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6971) loss tensor(0.5664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6977) loss tensor(0.8291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6970) loss tensor(0.6455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6973) loss tensor(0.7246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6974) loss tensor(0.4961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6973) loss tensor(0.6790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6970) loss tensor(0.8900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6960) loss tensor(0.7881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6976) loss tensor(0.5578, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6984) loss tensor(0.8330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6986) loss tensor(0.6261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7005) loss tensor(0.5658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6993) loss tensor(0.4826, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7008) loss tensor(0.7212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7006) loss tensor(0.7190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6995) loss tensor(0.7013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6981) loss tensor(0.7317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6977) loss tensor(0.5841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6977) loss tensor(0.7922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6985) loss tensor(0.4890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7010) loss tensor(0.7320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6995) loss tensor(0.6294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6990) loss tensor(0.5087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6968) loss tensor(0.5399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6975) loss tensor(0.7556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6983) loss tensor(0.8795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6983) loss tensor(0.6910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6985) loss tensor(0.5410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6999) loss tensor(0.6553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6985) loss tensor(0.6645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6977) loss tensor(0.7940, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6971) loss tensor(0.6641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6966) loss tensor(0.7734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6966) loss tensor(0.5192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6962) loss tensor(0.8009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6977) loss tensor(0.6943, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6986) loss tensor(0.6638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7011) loss tensor(0.4831, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7011) loss tensor(0.6182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7011) loss tensor(0.7261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7021) loss tensor(0.8036, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7013) loss tensor(1.0230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7010) loss tensor(0.7846, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7022) loss tensor(0.8976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7007) loss tensor(0.7009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7005) loss tensor(0.3834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7022) loss tensor(0.5008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7026) loss tensor(0.7313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7018) loss tensor(0.7331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7021) loss tensor(0.5646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7015) loss tensor(0.8059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7016) loss tensor(0.8193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6999) loss tensor(0.6111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7017) loss tensor(0.6694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7010) loss tensor(0.7162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7002) loss tensor(0.6111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6993) loss tensor(0.6273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7000) loss tensor(0.6797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7001) loss tensor(0.5145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6975) loss tensor(0.5347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6970) loss tensor(0.5111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6977) loss tensor(0.7383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6960) loss tensor(0.8375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6938) loss tensor(0.6720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6954) loss tensor(0.7559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6984) loss tensor(0.8109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6992) loss tensor(0.7620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7008) loss tensor(0.6771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7006) loss tensor(0.7983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7026) loss tensor(0.7787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7024) loss tensor(0.7387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7024) loss tensor(0.6537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7002) loss tensor(0.7468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7018) loss tensor(0.3550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7013) loss tensor(0.8784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6984) loss tensor(0.6564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6969) loss tensor(0.5067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6947) loss tensor(0.6571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6913) loss tensor(0.8147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6917) loss tensor(0.4987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6933) loss tensor(0.9133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6935) loss tensor(0.7272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6943) loss tensor(0.5435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6945) loss tensor(0.7796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6941) loss tensor(0.7767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6952) loss tensor(0.7120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6944) loss tensor(0.8596, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6941) loss tensor(0.7987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6929) loss tensor(0.5977, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6930) loss tensor(0.8266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6929) loss tensor(0.7477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6930) loss tensor(0.7095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6949) loss tensor(0.7237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6960) loss tensor(0.7093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6990) loss tensor(0.7144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6986) loss tensor(0.6117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7005) loss tensor(0.8931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7015) loss tensor(0.8117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6975) loss tensor(0.8535, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6974) loss tensor(0.8257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6953) loss tensor(0.4814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6951) loss tensor(0.7042, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6959) loss tensor(0.8475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6966) loss tensor(0.8267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6963) loss tensor(0.6433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6971) loss tensor(0.4152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6978) loss tensor(0.8924, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6973) loss tensor(0.7711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6982) loss tensor(0.5690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6995) loss tensor(0.9465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7015) loss tensor(0.4590, grad_fn=<NllLossBackward>)\n",
      "epoch: 55\n",
      "accuracy: tensor(0.7015) loss tensor(0.7459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6993) loss tensor(0.6983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6976) loss tensor(0.6218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6970) loss tensor(0.6185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6976) loss tensor(0.7861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6978) loss tensor(0.6987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6984) loss tensor(0.4854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6970) loss tensor(0.6315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6970) loss tensor(0.5438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6983) loss tensor(0.4814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6981) loss tensor(0.6229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6995) loss tensor(0.6799, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7008) loss tensor(0.5324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7011) loss tensor(0.6635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7003) loss tensor(0.8196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7016) loss tensor(0.6081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7022) loss tensor(0.6632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7011) loss tensor(0.7445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7010) loss tensor(0.6715, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7010) loss tensor(0.7199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7023) loss tensor(0.5880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7033) loss tensor(0.7398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7040) loss tensor(0.5879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7038) loss tensor(0.5180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7050) loss tensor(0.7104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7022) loss tensor(0.7343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7026) loss tensor(0.5544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7032) loss tensor(0.6422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7030) loss tensor(0.6336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7025) loss tensor(0.5351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7018) loss tensor(0.5806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7006) loss tensor(0.6248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7026) loss tensor(0.6716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7013) loss tensor(0.6445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7000) loss tensor(0.7431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6991) loss tensor(0.7623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7001) loss tensor(0.5054, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7005) loss tensor(0.6265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7010) loss tensor(0.6412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7015) loss tensor(0.6180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7009) loss tensor(0.6890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7001) loss tensor(0.6170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6977) loss tensor(0.9290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6977) loss tensor(0.8421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6971) loss tensor(0.5933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6983) loss tensor(0.5792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6997) loss tensor(0.5949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7014) loss tensor(0.5972, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7016) loss tensor(0.6449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7021) loss tensor(0.7380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7009) loss tensor(0.5288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7003) loss tensor(0.6636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7015) loss tensor(0.6749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7024) loss tensor(0.5055, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7014) loss tensor(0.5695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7022) loss tensor(0.4183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7007) loss tensor(0.5694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7017) loss tensor(0.8593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7013) loss tensor(0.5370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7029) loss tensor(0.6048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7038) loss tensor(0.5443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.4474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.6004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7019) loss tensor(0.7963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7022) loss tensor(0.5920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7031) loss tensor(0.6008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7019) loss tensor(0.7809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6998) loss tensor(0.7169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6994) loss tensor(0.5546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7011) loss tensor(0.4543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7007) loss tensor(0.7471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7003) loss tensor(0.6816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6993) loss tensor(0.5872, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6986) loss tensor(0.6348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6979) loss tensor(0.6770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6968) loss tensor(0.5900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6968) loss tensor(0.5700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6963) loss tensor(0.5626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6958) loss tensor(0.5442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6965) loss tensor(0.5481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6982) loss tensor(0.7006, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6979) loss tensor(0.7783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6998) loss tensor(0.7450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7010) loss tensor(0.6674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6999) loss tensor(0.7422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7030) loss tensor(0.5716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7015) loss tensor(0.5473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7014) loss tensor(0.6920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7023) loss tensor(0.4655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7013) loss tensor(0.5212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7017) loss tensor(0.7363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7018) loss tensor(0.6342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6990) loss tensor(0.4928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6979) loss tensor(0.6939, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6986) loss tensor(0.5618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7002) loss tensor(0.8235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6994) loss tensor(0.4013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7003) loss tensor(0.6764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7029) loss tensor(0.7716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7016) loss tensor(0.6239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7021) loss tensor(0.7421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7040) loss tensor(0.7688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7051) loss tensor(0.7233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7030) loss tensor(0.6785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7022) loss tensor(0.6287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7007) loss tensor(0.6495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7010) loss tensor(0.4828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7030) loss tensor(0.8758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7022) loss tensor(0.5175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7022) loss tensor(0.6107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7005) loss tensor(0.7429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7011) loss tensor(0.6767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7000) loss tensor(0.7047, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7019) loss tensor(0.4800, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7006) loss tensor(0.4247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6991) loss tensor(0.6295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7000) loss tensor(0.6228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7007) loss tensor(0.6391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7000) loss tensor(0.7189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6990) loss tensor(0.5830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6982) loss tensor(0.5842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6977) loss tensor(0.6585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6985) loss tensor(0.6335, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6994) loss tensor(0.5442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7009) loss tensor(0.5886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7026) loss tensor(0.5835, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7057) loss tensor(0.7906, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7049) loss tensor(0.6480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7046) loss tensor(0.9787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7061) loss tensor(0.6695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7049) loss tensor(0.5984, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7043) loss tensor(0.6102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.9412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7049) loss tensor(0.4333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7039) loss tensor(0.5463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7041) loss tensor(0.6100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7037) loss tensor(0.7264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7031) loss tensor(0.7061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7045) loss tensor(0.6257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7040) loss tensor(0.6196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7027) loss tensor(0.8824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7027) loss tensor(0.4141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7038) loss tensor(0.6669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7038) loss tensor(0.7161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7041) loss tensor(0.6463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7038) loss tensor(0.6117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7033) loss tensor(0.7398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7040) loss tensor(0.5885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7056) loss tensor(0.6918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7049) loss tensor(0.5596, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7047) loss tensor(0.5788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7047) loss tensor(0.7040, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7049) loss tensor(0.6377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7039) loss tensor(0.6035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.5611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7054) loss tensor(0.6255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7062) loss tensor(0.6885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.7713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.5319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.6998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(1.0048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7064) loss tensor(0.4628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7063) loss tensor(0.4923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.6141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7069) loss tensor(0.4970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7066) loss tensor(0.5508, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.8079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.7857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7067) loss tensor(0.6781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.8277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7064) loss tensor(0.5343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7067) loss tensor(0.5747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7069) loss tensor(0.5762, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7056) loss tensor(0.3350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7046) loss tensor(0.6217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7042) loss tensor(0.6589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7042) loss tensor(0.6086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7041) loss tensor(0.6651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7059) loss tensor(0.5688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7056) loss tensor(0.6674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7065) loss tensor(0.7382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.8135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7074) loss tensor(0.5118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7073) loss tensor(0.7366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.5685, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7064) loss tensor(0.5875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.4952, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7063) loss tensor(0.6039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7049) loss tensor(0.4683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7065) loss tensor(0.5284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7074) loss tensor(0.6389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7061) loss tensor(0.6312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7061) loss tensor(0.6933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.5692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.6195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7050) loss tensor(0.7600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7050) loss tensor(0.5943, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.5926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7064) loss tensor(0.6150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7074) loss tensor(0.4519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.6104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7083) loss tensor(0.5219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.6388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.5338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7099) loss tensor(0.6352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7088) loss tensor(0.6482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.6362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7042) loss tensor(0.5509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7024) loss tensor(0.7568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7010) loss tensor(0.7127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7002) loss tensor(0.6572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7005) loss tensor(0.4849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7011) loss tensor(0.7767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7000) loss tensor(0.8160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7006) loss tensor(0.7518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7017) loss tensor(0.7636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6994) loss tensor(0.6064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7007) loss tensor(0.5886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7009) loss tensor(0.4986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7025) loss tensor(0.6883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7032) loss tensor(0.7548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7017) loss tensor(0.7004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7008) loss tensor(0.7015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6985) loss tensor(0.5762, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6979) loss tensor(0.7172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6981) loss tensor(0.7931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6977) loss tensor(0.5929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6973) loss tensor(0.7533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6973) loss tensor(0.7469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6960) loss tensor(0.6298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6976) loss tensor(0.8111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7003) loss tensor(0.7592, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7002) loss tensor(0.6244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6997) loss tensor(0.6508, grad_fn=<NllLossBackward>)\n",
      "epoch: 56\n",
      "accuracy: tensor(0.7005) loss tensor(0.6825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7022) loss tensor(0.6575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7017) loss tensor(0.4857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7008) loss tensor(0.5475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6998) loss tensor(0.7521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6990) loss tensor(0.6278, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6995) loss tensor(0.6905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7000) loss tensor(0.5541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7018) loss tensor(0.6208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7024) loss tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7038) loss tensor(0.5708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.6062, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7047) loss tensor(0.8627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7032) loss tensor(0.5371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7043) loss tensor(0.7202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7047) loss tensor(0.6017, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.5479, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7035) loss tensor(0.5756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7055) loss tensor(0.5405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7056) loss tensor(0.7541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7062) loss tensor(0.5908, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7081) loss tensor(0.7795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.7024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7072) loss tensor(0.5002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7066) loss tensor(0.5823, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.5334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.6471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7046) loss tensor(0.4954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7041) loss tensor(0.4467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7034) loss tensor(0.5886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7047) loss tensor(0.5751, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7023) loss tensor(0.5731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7039) loss tensor(0.5883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7029) loss tensor(0.5688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7031) loss tensor(0.4894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7007) loss tensor(0.7139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6987) loss tensor(0.4548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7001) loss tensor(0.6003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7022) loss tensor(0.5445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7014) loss tensor(0.5653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7011) loss tensor(0.6522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7007) loss tensor(0.6103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7014) loss tensor(0.4749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7021) loss tensor(0.7077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7007) loss tensor(0.4850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7037) loss tensor(0.5706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.4521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7047) loss tensor(0.5369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7056) loss tensor(0.7285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.4342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7047) loss tensor(0.4850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7026) loss tensor(0.4331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7037) loss tensor(0.5738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7019) loss tensor(0.5619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7025) loss tensor(0.6917, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7002) loss tensor(0.5575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6970) loss tensor(0.5845, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6979) loss tensor(0.5882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6975) loss tensor(0.6666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6977) loss tensor(0.8041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7013) loss tensor(0.5272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7024) loss tensor(0.5015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7037) loss tensor(0.5116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7038) loss tensor(0.6362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7056) loss tensor(0.5421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7042) loss tensor(0.5750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7041) loss tensor(0.8573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7043) loss tensor(0.7066, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7032) loss tensor(0.5911, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7031) loss tensor(0.8073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7041) loss tensor(0.6375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7043) loss tensor(0.7237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7023) loss tensor(0.4756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7037) loss tensor(0.6453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7018) loss tensor(0.7739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7016) loss tensor(0.6967, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7005) loss tensor(0.5580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7018) loss tensor(0.6626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7016) loss tensor(0.7062, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7006) loss tensor(0.7066, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7025) loss tensor(0.6834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.6224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.5293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.6870, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7080) loss tensor(0.4524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7063) loss tensor(0.5019, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7049) loss tensor(0.6427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7039) loss tensor(0.6200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7042) loss tensor(0.6513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7045) loss tensor(0.4330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7061) loss tensor(0.5196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7029) loss tensor(0.6822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7027) loss tensor(0.6951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7021) loss tensor(0.4472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7026) loss tensor(0.5861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7030) loss tensor(0.4396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7034) loss tensor(0.5543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7039) loss tensor(0.6070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7034) loss tensor(0.7066, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7042) loss tensor(0.6995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7039) loss tensor(0.4903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7033) loss tensor(0.5498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7046) loss tensor(0.5713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7026) loss tensor(0.5252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7035) loss tensor(0.7562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7026) loss tensor(0.5781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7027) loss tensor(0.6832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7029) loss tensor(0.4791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7032) loss tensor(0.6475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7026) loss tensor(0.5228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7033) loss tensor(0.5314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7046) loss tensor(0.7182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7046) loss tensor(0.7041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7051) loss tensor(0.5779, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.8342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.6299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7043) loss tensor(0.5259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7050) loss tensor(0.5607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7041) loss tensor(0.5931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7047) loss tensor(0.5709, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7056) loss tensor(0.7602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7032) loss tensor(0.5829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7033) loss tensor(0.6156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7031) loss tensor(0.4985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7039) loss tensor(0.5217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7042) loss tensor(0.7805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7033) loss tensor(0.7489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7054) loss tensor(0.5364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7040) loss tensor(0.7573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7047) loss tensor(0.6372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.5211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7067) loss tensor(0.5063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7066) loss tensor(0.6070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7054) loss tensor(0.6570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.6479, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.5079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.6475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7083) loss tensor(0.5668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7067) loss tensor(0.5685, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7087) loss tensor(0.5389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.7813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7065) loss tensor(0.5523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7045) loss tensor(0.6533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7034) loss tensor(0.7043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7023) loss tensor(0.4378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7035) loss tensor(0.8392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7027) loss tensor(0.5814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7024) loss tensor(0.5225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7007) loss tensor(0.5846, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6987) loss tensor(0.4662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6993) loss tensor(0.6185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7001) loss tensor(0.4733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6982) loss tensor(0.6784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6970) loss tensor(0.7604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6986) loss tensor(0.4269, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7009) loss tensor(0.6979, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7016) loss tensor(0.6059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7018) loss tensor(0.6362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7050) loss tensor(0.6208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7051) loss tensor(0.5430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7045) loss tensor(0.6050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7053) loss tensor(0.5600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7053) loss tensor(0.4588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.3701, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7045) loss tensor(0.4609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7039) loss tensor(0.4623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.5351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7051) loss tensor(0.5465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7039) loss tensor(0.5925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7045) loss tensor(0.8598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.6392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7087) loss tensor(0.4772, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7051) loss tensor(0.4847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.4210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7054) loss tensor(0.5235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.5581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7035) loss tensor(0.5990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7021) loss tensor(0.6373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7015) loss tensor(0.7240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7000) loss tensor(0.6873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6985) loss tensor(0.6206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6989) loss tensor(0.7801, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7006) loss tensor(0.6644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6998) loss tensor(0.4581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6998) loss tensor(0.7231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7008) loss tensor(0.4743, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7015) loss tensor(0.6304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6999) loss tensor(0.8774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7019) loss tensor(0.4904, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7025) loss tensor(0.7777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7018) loss tensor(0.6186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7026) loss tensor(0.5867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7043) loss tensor(0.5816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7045) loss tensor(0.7392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7043) loss tensor(0.5835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7022) loss tensor(0.6115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7025) loss tensor(0.7437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7024) loss tensor(0.5353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7011) loss tensor(0.7113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7032) loss tensor(0.5335, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7025) loss tensor(0.5256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7040) loss tensor(0.6152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7043) loss tensor(0.6649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7045) loss tensor(0.5230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7030) loss tensor(0.7109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7038) loss tensor(0.7390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.6343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7053) loss tensor(0.5164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7072) loss tensor(0.6844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.6721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7081) loss tensor(0.7013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7077) loss tensor(0.5329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7065) loss tensor(0.4654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7062) loss tensor(0.5250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7059) loss tensor(0.4690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.7326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7061) loss tensor(0.5980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7053) loss tensor(0.5109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7056) loss tensor(0.6117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7073) loss tensor(0.7519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7077) loss tensor(0.6084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.6110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7075) loss tensor(0.5939, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7067) loss tensor(0.5108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7061) loss tensor(0.6919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7046) loss tensor(0.4924, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7046) loss tensor(0.5531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7045) loss tensor(0.7466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7027) loss tensor(0.6208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7042) loss tensor(0.7777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7024) loss tensor(0.6047, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7029) loss tensor(0.4080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7054) loss tensor(0.5586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7047) loss tensor(0.7651, grad_fn=<NllLossBackward>)\n",
      "epoch: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7050) loss tensor(0.5686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.6287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7063) loss tensor(0.5329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7075) loss tensor(0.5229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.4757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7082) loss tensor(0.5551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7064) loss tensor(0.4975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7063) loss tensor(0.5836, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.4748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7065) loss tensor(0.6652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7061) loss tensor(0.5366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7053) loss tensor(0.4845, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7062) loss tensor(0.4292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.4746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.5170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7077) loss tensor(0.5304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.5706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7059) loss tensor(0.4439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7046) loss tensor(0.5725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7043) loss tensor(0.4536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7025) loss tensor(0.5610, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7034) loss tensor(0.7797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7062) loss tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7061) loss tensor(0.6149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7064) loss tensor(0.6023, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.7082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7067) loss tensor(0.5586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7067) loss tensor(0.6165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.4435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7090) loss tensor(0.4722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7099) loss tensor(0.5589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.5418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7072) loss tensor(0.5551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7065) loss tensor(0.5470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7051) loss tensor(0.5125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7069) loss tensor(0.5540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.6474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7073) loss tensor(0.5473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7085) loss tensor(0.3750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7064) loss tensor(0.6871, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7075) loss tensor(0.5039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.7843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7059) loss tensor(0.7595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.5214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7019) loss tensor(0.4912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7023) loss tensor(0.5735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7017) loss tensor(0.6270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6998) loss tensor(0.6007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7001) loss tensor(0.6261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7022) loss tensor(0.6199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7018) loss tensor(0.6274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7016) loss tensor(0.5676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7047) loss tensor(0.6351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7067) loss tensor(0.6705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.5038, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7082) loss tensor(0.4962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.5414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7072) loss tensor(0.6584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7061) loss tensor(0.4202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.7059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7056) loss tensor(0.4748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.4814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.4721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.5864, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7088) loss tensor(0.4080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7069) loss tensor(0.5191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7077) loss tensor(0.5524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7082) loss tensor(0.4980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.5537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7062) loss tensor(0.4482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7054) loss tensor(0.5974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7062) loss tensor(0.4515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7053) loss tensor(0.5226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.5473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7066) loss tensor(0.4306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.5830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.4264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7072) loss tensor(0.5830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.5615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7074) loss tensor(0.7009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7081) loss tensor(0.5754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.3931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.4837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.4606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7113) loss tensor(0.5293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7107) loss tensor(0.4129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.4429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7064) loss tensor(0.6058, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7064) loss tensor(0.5815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.6170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7075) loss tensor(0.6153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7069) loss tensor(0.7401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7058) loss tensor(0.5324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7054) loss tensor(0.4528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7051) loss tensor(0.6261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7046) loss tensor(0.5917, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.6453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7022) loss tensor(0.4585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7031) loss tensor(0.6020, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7040) loss tensor(0.6692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7032) loss tensor(0.6564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7033) loss tensor(0.6793, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7037) loss tensor(0.7060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.6384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7054) loss tensor(0.5558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.6408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7046) loss tensor(0.6211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7033) loss tensor(0.4951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7037) loss tensor(0.4288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7023) loss tensor(0.6691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7014) loss tensor(0.6507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7007) loss tensor(0.4957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7000) loss tensor(0.7022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6995) loss tensor(0.5596, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.6991) loss tensor(0.5226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7003) loss tensor(0.6471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.6989) loss tensor(0.4515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7019) loss tensor(0.4718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7006) loss tensor(0.6336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7014) loss tensor(0.5817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7027) loss tensor(0.5387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7032) loss tensor(0.5734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7033) loss tensor(0.6526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7047) loss tensor(0.8182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7043) loss tensor(0.5785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.6490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7042) loss tensor(0.4795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7039) loss tensor(0.4117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7030) loss tensor(0.5893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7031) loss tensor(0.5508, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7037) loss tensor(0.5795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7042) loss tensor(0.5307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7045) loss tensor(0.7785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7043) loss tensor(0.6361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7033) loss tensor(0.6431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7032) loss tensor(0.4597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7047) loss tensor(0.6122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7037) loss tensor(0.6876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7014) loss tensor(0.5192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7016) loss tensor(0.3905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7023) loss tensor(0.4801, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7031) loss tensor(0.5825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7027) loss tensor(0.6193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7037) loss tensor(0.6379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.7339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7080) loss tensor(0.4443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7055) loss tensor(0.6832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7067) loss tensor(0.7023, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7073) loss tensor(0.5874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7069) loss tensor(0.5393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7064) loss tensor(0.6727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7063) loss tensor(0.6302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7062) loss tensor(0.5892, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7051) loss tensor(0.5914, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7056) loss tensor(0.5148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.5542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7026) loss tensor(0.6949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7023) loss tensor(0.5079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7025) loss tensor(0.6285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7023) loss tensor(0.8963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7031) loss tensor(0.7569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7032) loss tensor(0.4939, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7027) loss tensor(0.5499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7014) loss tensor(0.4731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7022) loss tensor(0.7150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7024) loss tensor(0.4936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.5419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7039) loss tensor(0.5347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7061) loss tensor(0.6113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.5015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7059) loss tensor(0.7670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7083) loss tensor(0.7112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7077) loss tensor(0.5883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.7522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7063) loss tensor(0.4489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7050) loss tensor(0.5113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7054) loss tensor(0.6228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7034) loss tensor(0.7163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7051) loss tensor(0.5120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7051) loss tensor(0.5475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7054) loss tensor(0.5686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7061) loss tensor(0.5643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7039) loss tensor(0.6184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7040) loss tensor(0.5178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7035) loss tensor(0.6470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7023) loss tensor(0.6193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7026) loss tensor(0.6747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7041) loss tensor(0.6188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7050) loss tensor(0.5926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7031) loss tensor(0.4830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7037) loss tensor(0.6496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7041) loss tensor(0.4103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7030) loss tensor(0.6776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7014) loss tensor(0.8681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7038) loss tensor(0.6116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7065) loss tensor(0.5745, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7069) loss tensor(0.5738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.6873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7075) loss tensor(0.5587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7083) loss tensor(0.5007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7093) loss tensor(0.6837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7113) loss tensor(0.6335, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.6317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.7257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.4965, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7107) loss tensor(0.6468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7102) loss tensor(0.6192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.5101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.4409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7103) loss tensor(0.4542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7082) loss tensor(0.4156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7080) loss tensor(0.5001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7062) loss tensor(0.5037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7055) loss tensor(0.6621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7065) loss tensor(0.3764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7097) loss tensor(0.4876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.6212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.6319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7101) loss tensor(0.7151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7088) loss tensor(0.5410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7045) loss tensor(0.6097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7035) loss tensor(0.7081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7027) loss tensor(0.6295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7041) loss tensor(0.7916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7067) loss tensor(0.5766, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.6985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7095) loss tensor(0.7753, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7089) loss tensor(0.5970, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7070) loss tensor(0.7224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7073) loss tensor(0.6513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7095) loss tensor(0.5637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.5369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7053) loss tensor(0.4480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7065) loss tensor(0.4870, grad_fn=<NllLossBackward>)\n",
      "epoch: 58\n",
      "accuracy: tensor(0.7071) loss tensor(0.5177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7082) loss tensor(0.3987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.5274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7087) loss tensor(0.5026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7085) loss tensor(0.7082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7087) loss tensor(0.5415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7083) loss tensor(0.4357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.5929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7056) loss tensor(0.6643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7054) loss tensor(0.5601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7062) loss tensor(0.6636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.4264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.3567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.5006, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7083) loss tensor(0.5222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7082) loss tensor(0.3779, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7112) loss tensor(0.7481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.5673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7114) loss tensor(0.5524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7090) loss tensor(0.6797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7081) loss tensor(0.5567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7082) loss tensor(0.6191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.5775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7088) loss tensor(0.4005, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.5804, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7072) loss tensor(0.5354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7069) loss tensor(0.7280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7083) loss tensor(0.4953, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7069) loss tensor(0.6963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.3737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7075) loss tensor(0.3854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7081) loss tensor(0.3394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.4946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.5802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7081) loss tensor(0.5117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7083) loss tensor(0.6482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.5431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7081) loss tensor(0.3790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7077) loss tensor(0.5085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7096) loss tensor(0.3607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7097) loss tensor(0.4082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7077) loss tensor(0.5349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7059) loss tensor(0.5439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7066) loss tensor(0.4100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7069) loss tensor(0.6324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.5118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.7119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.4511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.6585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.4495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7072) loss tensor(0.6128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.6039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.5060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7055) loss tensor(0.5545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7035) loss tensor(0.6889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7063) loss tensor(0.4823, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7053) loss tensor(0.6922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.6042, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7103) loss tensor(0.3870, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7095) loss tensor(0.4662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.3755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7097) loss tensor(0.4696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.6575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.4186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7083) loss tensor(0.4376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7099) loss tensor(0.5640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.4924, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.4791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7067) loss tensor(0.6148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7101) loss tensor(0.5878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.6826, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.6333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.6335, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.5362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.7004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.5396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.4770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7113) loss tensor(0.6895, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7089) loss tensor(0.4728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7074) loss tensor(0.6387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.4602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7059) loss tensor(0.5573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.6264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.7006, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7066) loss tensor(0.5894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.6222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.4594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.5852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.5602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.4658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.7459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.5036, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.6887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.5091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.6818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.6525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.7164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.5915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.6611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.5358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.4151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.4675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.4201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.4297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.4725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.7051, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.5721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7090) loss tensor(0.4612, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7043) loss tensor(0.7202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7039) loss tensor(0.5338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7026) loss tensor(0.6009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7045) loss tensor(0.6889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7042) loss tensor(0.7660, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7049) loss tensor(0.6911, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.5750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7080) loss tensor(0.5946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7085) loss tensor(0.5538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7101) loss tensor(0.5299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7095) loss tensor(0.5028, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.5531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.6132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7083) loss tensor(0.5411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7102) loss tensor(0.4793, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7102) loss tensor(0.3503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.4461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7081) loss tensor(0.3422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7080) loss tensor(0.7798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.4809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.4983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.4722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.4994, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.4564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.3720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.5501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.5498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7107) loss tensor(0.4059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.6311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.5232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.6060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.5319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.8080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.5257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.4369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.7337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.5226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7093) loss tensor(0.5612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.4942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.6949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7083) loss tensor(0.5292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.5128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.6244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7094) loss tensor(0.5095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7090) loss tensor(0.5225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7103) loss tensor(0.5477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.7007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.3955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.4249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.5494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.5872, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.6171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7114) loss tensor(0.5352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7113) loss tensor(0.4673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.5326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.5586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.4958, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.4557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.6202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7085) loss tensor(0.4529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7081) loss tensor(0.6714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7077) loss tensor(0.5486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.6545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7094) loss tensor(0.6154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7085) loss tensor(0.3879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7093) loss tensor(0.6398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.6446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7065) loss tensor(0.6097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.6245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.7075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7077) loss tensor(0.5635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7073) loss tensor(0.4647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7069) loss tensor(0.7008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.6011, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7107) loss tensor(0.5416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7102) loss tensor(0.5331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.5139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7095) loss tensor(0.5810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7090) loss tensor(0.4709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.6590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7102) loss tensor(0.6614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.5575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.4096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.6887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.5114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7090) loss tensor(0.7475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.4353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7099) loss tensor(0.6363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.6888, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.5213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.6956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7113) loss tensor(0.5573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.5182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.5208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7093) loss tensor(0.5204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7094) loss tensor(0.5664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7093) loss tensor(0.5716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.7375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7095) loss tensor(0.6445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.6360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7072) loss tensor(0.3620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.5209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7087) loss tensor(0.4865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.5991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7089) loss tensor(0.6694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.4982, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.6684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.4997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.6010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.5026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.6794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.6484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.5686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7113) loss tensor(0.5070, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7110) loss tensor(0.4815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7107) loss tensor(0.4636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7099) loss tensor(0.6245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7094) loss tensor(0.5880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7103) loss tensor(0.5572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.7283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.5206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.4054, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.4989, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.5669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.6706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.5424, grad_fn=<NllLossBackward>)\n",
      "epoch: 59\n",
      "accuracy: tensor(0.7139) loss tensor(0.3453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.4948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.5096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.3531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.4396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.5037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.3855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.3737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.3212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.5080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.4113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.3465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.6439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.7238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.4428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.3816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7099) loss tensor(0.4131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7093) loss tensor(0.4593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7093) loss tensor(0.4376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7096) loss tensor(0.4233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7103) loss tensor(0.5105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.3676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.4024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.5040, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.4567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7077) loss tensor(0.4345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7082) loss tensor(0.4258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.4817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7073) loss tensor(0.4920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7056) loss tensor(0.5949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7074) loss tensor(0.5528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7087) loss tensor(0.4973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7081) loss tensor(0.4900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.5413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7112) loss tensor(0.5372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.3724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.4410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7099) loss tensor(0.4154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.6778, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.4770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.3841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.6366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.7100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.5756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.5239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7096) loss tensor(0.5539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7088) loss tensor(0.5318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7072) loss tensor(0.5909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.6507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7055) loss tensor(0.4742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.4825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7031) loss tensor(0.4226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7043) loss tensor(0.4674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7054) loss tensor(0.6043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7062) loss tensor(0.6293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7074) loss tensor(0.4138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7085) loss tensor(0.4248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7083) loss tensor(0.4659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.5205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.4030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7088) loss tensor(0.4544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7080) loss tensor(0.5236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.6240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.5160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.3715, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7093) loss tensor(0.3767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.4602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.4100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7095) loss tensor(0.5748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.5432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7107) loss tensor(0.6749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.3545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.4129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.4789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.6492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7093) loss tensor(0.3725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.5009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.6195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7072) loss tensor(0.4275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7063) loss tensor(0.3531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7067) loss tensor(0.3988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7083) loss tensor(0.4947, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7067) loss tensor(0.4384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7048) loss tensor(0.4552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7062) loss tensor(0.5547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7073) loss tensor(0.5904, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7087) loss tensor(0.4780, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7075) loss tensor(0.4516, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7077) loss tensor(0.6998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7074) loss tensor(0.6558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.6162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.4942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.5040, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7088) loss tensor(0.4019, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7083) loss tensor(0.4606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.5496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.6619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7103) loss tensor(0.6246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7107) loss tensor(0.4007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7114) loss tensor(0.5712, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7114) loss tensor(0.4401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7102) loss tensor(0.4564, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7102) loss tensor(0.5537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7097) loss tensor(0.5653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.5278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.5199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.5884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.4444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7113) loss tensor(0.4343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.5669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.4678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.5129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.4401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.8159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.4389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7095) loss tensor(0.5402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.6280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7102) loss tensor(0.4250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.6133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.5428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.4983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.6131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.5671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.3231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.6645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.3504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.5651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.4726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.4136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.4794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7114) loss tensor(0.5093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.4083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.6398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.7360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.5172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.4230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.6084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.4971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.6768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.3272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.4402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.5223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.4083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.5942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.4659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.5529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.4575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.4116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.7289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.6405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.6311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.6582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.6890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.7309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.4286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.3670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.6990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.5562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.5721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.4926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.6257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.6207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.3909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.4078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.5791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.5326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.5849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.8178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.5360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.5829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.6095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.4768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.6725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.5447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.5541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.7341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.5333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.5951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.5646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.5606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.6424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7102) loss tensor(0.5456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7095) loss tensor(0.5693, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.4749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.4256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.6002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.5529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.4866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.4763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.4903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.8053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.5759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.5619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7099) loss tensor(0.5018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7094) loss tensor(0.3735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.4310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7094) loss tensor(0.5285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7099) loss tensor(0.6009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.3431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.5923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.6726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.4511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.6013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.6579, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.5856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.6398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.5345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.4810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.5656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.5789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.5106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.4589, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7173) loss tensor(0.5499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.6347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.5280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.3899, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.5802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.4545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.5878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.6301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7112) loss tensor(0.5731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.3791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7102) loss tensor(0.6145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7114) loss tensor(0.5657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.6740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.5695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.5527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.3881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.5800, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.5554, grad_fn=<NllLossBackward>)\n",
      "epoch: 60\n",
      "accuracy: tensor(0.7110) loss tensor(0.5851, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.5739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7103) loss tensor(0.4140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.4599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.5574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7070) loss tensor(0.4482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7066) loss tensor(0.3969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.4405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7095) loss tensor(0.6594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7089) loss tensor(0.3725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7065) loss tensor(0.5041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7090) loss tensor(0.5943, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7102) loss tensor(0.4091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.5480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.4728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.3347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.4300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.4135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.5505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.3942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.4200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.4695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.7700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.4299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.4820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.4409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.4117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.5756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.4753, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.3580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.5207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.3509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.4818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.4081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.6397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.5750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7103) loss tensor(0.5137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7113) loss tensor(0.4425, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.5187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.4902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7103) loss tensor(0.3873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.3960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.7567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7099) loss tensor(0.5595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.4992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7113) loss tensor(0.4299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.5114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7107) loss tensor(0.3695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.5431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7113) loss tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7097) loss tensor(0.2869, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.4025, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.4147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.5237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.5195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.4973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7103) loss tensor(0.4424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.3977, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.4629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7096) loss tensor(0.5725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.5468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7073) loss tensor(0.3624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7087) loss tensor(0.5028, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7090) loss tensor(0.3779, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7093) loss tensor(0.3813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7093) loss tensor(0.4188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7089) loss tensor(0.6799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7069) loss tensor(0.5699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7065) loss tensor(0.6026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7081) loss tensor(0.5847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7089) loss tensor(0.4490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7093) loss tensor(0.4324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.5998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.5445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.5457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.5217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.5873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7102) loss tensor(0.5095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.7482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7085) loss tensor(0.4144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7081) loss tensor(0.3365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.3805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7080) loss tensor(0.4604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7080) loss tensor(0.4543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7081) loss tensor(0.5440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7085) loss tensor(0.4985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7101) loss tensor(0.5685, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.4686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7095) loss tensor(0.4198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.5850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.3537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.4658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.5576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.5535, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7142) loss tensor(0.5142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.3951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.4069, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.5746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7095) loss tensor(0.4442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.4882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.4795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.4413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.5056, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.5350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.4617, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.4172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.5932, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7089) loss tensor(0.3725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.5569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7089) loss tensor(0.5004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7077) loss tensor(0.5077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.5775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7064) loss tensor(0.7289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7072) loss tensor(0.5763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.5071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.5952, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.3336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.5410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.4706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.4568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.4855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.4502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.6287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.5635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.6219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.4903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.4226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.6709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7094) loss tensor(0.6783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.4883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.5278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.5537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.5928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.6365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.4680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.4417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.5202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.5191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.4778, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.4705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.4095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.6684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.5517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.4580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.4385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7103) loss tensor(0.4166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7101) loss tensor(0.5493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7102) loss tensor(0.3123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7107) loss tensor(0.4876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.4815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.7297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.5523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.5869, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.7843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.4938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.4837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.4490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.5014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.4724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.4428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.5560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7093) loss tensor(0.4811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7089) loss tensor(0.5330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7085) loss tensor(0.6730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7073) loss tensor(0.5802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7090) loss tensor(0.3591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.6638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.4246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.4435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.5202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.6425, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.6054, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.4621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7103) loss tensor(0.5079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.6603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7113) loss tensor(0.5037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.6022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.5622, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.4598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.4038, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.4538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.4269, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.5689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.5230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.5098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.5201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.5019, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.4746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.4761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.4935, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.5198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.6384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.5465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.4418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.4893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.5922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.6920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.5026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.4907, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.5281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.4401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.5453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.6995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.5139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.7424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.6151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7112) loss tensor(0.4222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.5946, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7088) loss tensor(0.5028, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7082) loss tensor(0.6172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7093) loss tensor(0.3861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7097) loss tensor(0.6449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.5614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.5445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7096) loss tensor(0.5543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.6008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.4209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.5934, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7096) loss tensor(0.3717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7057) loss tensor(0.4543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7066) loss tensor(0.6371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7107) loss tensor(0.6715, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.8245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.4760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.4828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.4028, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.5666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7086) loss tensor(0.4264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7099) loss tensor(0.7040, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7080) loss tensor(0.5391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7081) loss tensor(0.6091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.4190, grad_fn=<NllLossBackward>)\n",
      "epoch: 61\n",
      "accuracy: tensor(0.7098) loss tensor(0.3634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.5924, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.4859, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.3756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.4308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7114) loss tensor(0.3572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.3897, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.4175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.6316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.4097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7114) loss tensor(0.3794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.4023, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.3764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.4110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7101) loss tensor(0.5299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.5026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.5080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.4445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.4485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.2902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.5167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.5152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.7099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.4666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.4426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.5913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.5436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.4936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.3507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.5007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.4088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.5304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.4207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.5010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.4414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.4672, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.6450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.4385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.5439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7096) loss tensor(0.6315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.5754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7088) loss tensor(0.5034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7065) loss tensor(0.4884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7072) loss tensor(0.5796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.4394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7112) loss tensor(0.4275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.5003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.4536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.3606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.4430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.5011, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.5815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.5440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.4404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.4894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.5298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.3792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.5211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.5280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.4894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.4678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.5259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.4554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7099) loss tensor(0.4182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.5133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7097) loss tensor(0.4405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7085) loss tensor(0.5809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7079) loss tensor(0.3826, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.4798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.4442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.6204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.5530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.6702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.4453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.5368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.4031, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.4924, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.4979, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.5426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.4940, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.5170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7077) loss tensor(0.4996, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7067) loss tensor(0.3764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7062) loss tensor(0.5669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7069) loss tensor(0.4914, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7064) loss tensor(0.5950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7080) loss tensor(0.5098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7096) loss tensor(0.5334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.4746, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7142) loss tensor(0.5724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.4406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.5408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.5162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.3533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.4722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4766, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.4920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.5790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.5305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.3706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.5730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.3857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.4529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.4816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.8233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.3859, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.4105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.5469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.6133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.4750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.5167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.5686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.4006, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.4692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4762, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.4108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.6070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.4732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.4754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.5232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.6036, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.3348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.5035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.5504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.6566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.4449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.4359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.5704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.3941, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.5087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.6502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.5066, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.4272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.4819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.4499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.4777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.3945, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.6450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7102) loss tensor(0.5706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.5213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.6915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.6648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.6037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.6103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.5646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.4119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3577, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.4160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.4570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.4770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.4728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.5420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.3756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.4029, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.5492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.4165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.5482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.5080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.5417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.4775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.6300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.5544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.5590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7107) loss tensor(0.4202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.6035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.4764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.5012, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.4392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.3468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.4909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.5728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.6682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.4092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.6067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.3954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.5891, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.4880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.5583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.6194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.3261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.5647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.4922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.5842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.4163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.4957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.4326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.4493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.4466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7095) loss tensor(0.5688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.6167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7069) loss tensor(0.4927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7045) loss tensor(0.5309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7059) loss tensor(0.4700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7082) loss tensor(0.6652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7101) loss tensor(0.5100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.4761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.6059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.4932, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7095) loss tensor(0.5142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.4660, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7091) loss tensor(0.5057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.5056, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.5279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.3632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.5467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.4276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7113) loss tensor(0.4033, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7103) loss tensor(0.4369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.3502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.3776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.3882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.4040, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.5543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.4570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.4696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.5865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.4655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.6382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7101) loss tensor(0.3640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7099) loss tensor(0.4503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7099) loss tensor(0.4170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.4703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.6434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.5155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.4498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.6918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.5281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.6686, grad_fn=<NllLossBackward>)\n",
      "epoch: 62\n",
      "accuracy: tensor(0.7154) loss tensor(0.3851, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.3624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.4806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.3477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.4816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.5263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.4102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.5280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.4612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.3474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.4701, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.5423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.4227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.3792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.4103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.6964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.4943, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7114) loss tensor(0.3910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.3260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.4004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.4798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.4277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7114) loss tensor(0.4255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.3584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.3978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.5822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.5067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.3776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.4757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7112) loss tensor(0.3680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.3087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.4080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.4625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.4745, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.4664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.3989, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.5080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.4893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.4019, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.5781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7103) loss tensor(0.4789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7105) loss tensor(0.5213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.4829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.4716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.5125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.3678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.4449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.4662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.5658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.3889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.5648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.6093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.4383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.5526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7114) loss tensor(0.4499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.5260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.5061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.3026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.3602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.5633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.4526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.4548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.4101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.5262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.5031, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.3674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.5184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.3497, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.4449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7112) loss tensor(0.4834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.3267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7113) loss tensor(0.4810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.3970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7110) loss tensor(0.4429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.4325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.4792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.4109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.3281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.5852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.5435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.3112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.4429, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7159) loss tensor(0.5101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.4649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.4911, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.6404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.4317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.4429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.5214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.5057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.4015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.4400, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.4854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.5478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.5161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.4309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.5057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.4628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.4544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.4385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.4792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.4821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.4773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.4540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.4130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.6348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.4701, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.5197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.4578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.4069, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.4542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7102) loss tensor(0.5406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7103) loss tensor(0.3320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.4684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.4588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.5313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4979, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.5121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.4768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.4927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.3099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.6886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.4291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.4708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.4073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.5158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.4866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.5306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.4541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.4398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4753, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.3957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.3987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.4758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.6323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.4298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.5006, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4914, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.4969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.4602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.6519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.5311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.5761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.5085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.6582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.4357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.4416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.4880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4953, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.4738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.5361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.5485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.5030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.4436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.7494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.5428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.4779, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.3193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.3506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.7374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.5995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.5366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.4866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.4219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.6377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.4757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.4168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.4440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.3871, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.5210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.4941, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.4088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.5254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.3088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7113) loss tensor(0.4829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.3868, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7110) loss tensor(0.4734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.5608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.4889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.4343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.6424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.6041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.4684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.6498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.3606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.4817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.6602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.6159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.3798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.5154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.5639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.4650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.4024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.5131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.5195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.3142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.4172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.4380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.5233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.5010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.4696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.5270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.6177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.4750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.7283, grad_fn=<NllLossBackward>)\n",
      "epoch: 63\n",
      "accuracy: tensor(0.7173) loss tensor(0.3546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.4614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.4987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.5272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.5195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.3699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.4459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.4179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.5108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.5723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.4190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.5250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.3245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.4212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.5041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.5590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.5629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.3469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.4386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.4850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.6277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.5641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.4261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.4488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.3670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.4655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.4840, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.3309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.4485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.4792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.4582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.4422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.5376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.3262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.4261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7071) loss tensor(0.2803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7069) loss tensor(0.5135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.4565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7089) loss tensor(0.4754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7098) loss tensor(0.7058, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.4710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.4938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.3913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.5620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.5053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.4417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.4020, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.7368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.3260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.3947, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.3336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.3800, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.4100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.4534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.2954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.3194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.5450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.2820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.5274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.4299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.5241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.5278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.5199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.5568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.5949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.4323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.5197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.4807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.4060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.3821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.4463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.3879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4874, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7187) loss tensor(0.4394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.4934, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.3945, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.5056, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.4207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.4471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.4169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.5063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.5697, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.6413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.4589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.3884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7114) loss tensor(0.4645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7089) loss tensor(0.6632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7078) loss tensor(0.4861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7107) loss tensor(0.4826, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.4358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.4445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.5840, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.4189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.4152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.6321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.4575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.4890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.3091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.4143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.4885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.3499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.5060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.4239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.4939, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.4271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.4699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.5512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.4530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.5224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.2783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.4542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.6363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.5038, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.3366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.4472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.5602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.4484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.3310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.4392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.3606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.4153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.4319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.5837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.3645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.4477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.5365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.3278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.3685, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.3590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.3350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.4242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.4631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.4998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.2603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.4564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.4427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.3769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.4553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.4587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.3640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.3797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7115) loss tensor(0.2855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7113) loss tensor(0.4488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7107) loss tensor(0.5188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.3900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.4291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.4960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.5467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.5022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.5327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.3623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.4022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.4465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.5119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.4956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.3803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.4446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.5104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.4301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.5120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.4100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.4214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.6481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.4820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.4938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.4824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.4747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.4931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.5175, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7133) loss tensor(0.4357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.4773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.5350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.5958, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.3310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.3409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.3121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.5543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.4787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.3726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.5080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.6420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.5083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.4059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.4839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.8029, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.5580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.4159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.4810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.5838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.4077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.5691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.3705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.4369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.4625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.5190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.4803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.4747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.4134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.6966, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.5116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.6593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.3695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.3950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.5107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.5401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.7374, grad_fn=<NllLossBackward>)\n",
      "epoch: 64\n",
      "accuracy: tensor(0.7175) loss tensor(0.3838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.5157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.3820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.4553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.3551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.4697, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.2844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.3853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.4250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.5016, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.4318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.5620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.4352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.2636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.4881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.4024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.3861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.3855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.4691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.3694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.4448, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.4518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.4437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.4209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.4186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3906, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.2586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.4517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.5643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.7718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.4743, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.5186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3622, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.3942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.3031, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.4426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.5163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.4865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.4527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.3582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.4424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.3961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.5060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.5893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.3790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.5576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.4527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.4209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.3921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.5417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.5395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.5575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7117) loss tensor(0.3749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.4053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.3263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7114) loss tensor(0.5841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.5439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.4910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.5719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.4598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.5072, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7135) loss tensor(0.4583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.4733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.4815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.4129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.3829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.2912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.4650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.5424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4715, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.4195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.3313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.3759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.5757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.5093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.3640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.5658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7104) loss tensor(0.5478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7090) loss tensor(0.5001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7089) loss tensor(0.4214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7106) loss tensor(0.3434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7089) loss tensor(0.3529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.3324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.4388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7123) loss tensor(0.3959, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7114) loss tensor(0.4536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.4788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7120) loss tensor(0.3571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7127) loss tensor(0.4245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.3909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.6361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.4752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.4161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.3940, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.4420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.3668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.5228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.4034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.5008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.4659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.5099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.4947, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.5050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.5298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.4875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.4331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.3767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.5567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.4897, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.6510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.5365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.5252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.4026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.5083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.4168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2904, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.4014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.4528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.5290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.4686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.5285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.4554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.4819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.4376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.4837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.5050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.4656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.4005, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.4647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.4315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.4280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.4848, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.5366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.4508, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3027, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.2940, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.5058, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.4026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.4434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.4383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.5037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.3756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.4744, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.3537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.3565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.5457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.6342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.4661, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7143) loss tensor(0.5324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.5265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.4650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.3776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.5748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.4185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.4834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3744, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.4203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.5727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.4518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.5506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.5430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.5415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.5073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.4277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.5941, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.4614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.6174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.4750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.5440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.5390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.4992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.4684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.4472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.4293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.5156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7114) loss tensor(0.4723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.6002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.3651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.4875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7102) loss tensor(0.6320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7111) loss tensor(0.4543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7112) loss tensor(0.4062, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.6186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.4331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.6421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.4346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.6323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7082) loss tensor(0.4865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7073) loss tensor(0.5828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7045) loss tensor(0.5966, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7049) loss tensor(0.4811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7054) loss tensor(0.5246, grad_fn=<NllLossBackward>)\n",
      "epoch: 65\n",
      "accuracy: tensor(0.7041) loss tensor(0.5111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7072) loss tensor(0.4948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7072) loss tensor(0.4296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7062) loss tensor(0.3690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7043) loss tensor(0.4438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7041) loss tensor(0.4447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7055) loss tensor(0.4651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7050) loss tensor(0.4760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7088) loss tensor(0.4291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.5294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.4681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.4431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.3900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.4886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.4171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.5735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.4390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.4686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.3785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.3822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.5307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.5154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.3889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.4080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.4829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.2580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.4472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.3512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.4896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.4504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.5705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.3576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.5212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.3670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.2915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.3257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.3748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.4526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.4343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.4379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.4536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.4162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.5178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4062, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.5179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.4655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.5788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.3607, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7128) loss tensor(0.4602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.3913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.4821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.3001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.2883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.3131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.3527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.3864, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3833, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.4452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3968, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.4188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.4719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.4852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.4050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.5618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3744, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.4562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.4268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.4534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.4860, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.4537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.4009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.4926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.5356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.4022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.4713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.4175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.3864, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.4015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.3643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.3531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.2633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.3732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.5135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.3046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.4915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.4576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.4338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.5064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.4974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.2695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.4086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.5054, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.4863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.4345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.5892, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.4509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.4165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.4353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.4586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.3980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.4198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.4252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.2959, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.4073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.5905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.3557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.4669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.4199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.4598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.4869, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.4929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.3186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.4325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.4187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.4053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.5719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.4281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.3216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.5275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.4192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.5239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.4514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.4602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.3813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.4163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.5866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.3504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.5613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.4657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.4569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.4462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.3410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.3505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.3865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.4403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.4384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4959, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7169) loss tensor(0.4385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.3747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.3191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.4786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.4116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.6312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.3506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.4076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.3018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.3392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.3338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.4155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.4856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.4844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.3576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.4697, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.4314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.4027, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.4516, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.5034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.3160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.4117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.4069, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.5105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.3618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.4824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.3936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.4889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.5506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.4741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.4224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.6096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.4167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.4025, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.4263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.5044, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.4257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.5153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.5747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.3421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.2267, grad_fn=<NllLossBackward>)\n",
      "epoch: 66\n",
      "accuracy: tensor(0.7169) loss tensor(0.3024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.3354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.2957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.4261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.3813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.3728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.5103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.4327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.2882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.4637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.2360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.2769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.2888, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.2957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.4681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.4758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.4638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.4191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.3767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.3997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.4350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.3215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.4441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.3117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.3720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3989, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.4906, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.4420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.4679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.4121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.3705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.4276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.4135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.4631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.4716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3015, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7178) loss tensor(0.2781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.4820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.2941, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.2821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.5661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.3495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.3994, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.3876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.5378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.2364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.4455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3056, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2965, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.3138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3697, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2021, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.5209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.4191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3934, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3897, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3027, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.4118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.4200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.5015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.4524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.4271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.3317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.3882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.4976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.2741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.6240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.4664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.4857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.5038, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.4495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.4397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.6461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.5142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.3463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.4071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.3977, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.5600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.5569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.5093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3932, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4596, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.3367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.4593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.3620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.5455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.4261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4996, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.2774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3924, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.4713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.4670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.3523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.3283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.4021, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.5000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.5451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.4352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3065, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7181) loss tensor(0.4423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.4388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.3365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.2844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.3989, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.3866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.3454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.4639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.4920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.3828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.4262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.5280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.3360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.3614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.4351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.2904, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.2773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.6029, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.4455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.4619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.3333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.4258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.2969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.5459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.4180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.4661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.4588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.4432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.4298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.4192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.4659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.5007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.3199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.4291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.4439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.4738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.4150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.6631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.4419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.2936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.5890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.4689, grad_fn=<NllLossBackward>)\n",
      "epoch: 67\n",
      "accuracy: tensor(0.7200) loss tensor(0.4277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.2644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.2478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3687, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.4426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.3328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.4261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.3267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.3825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.4651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2800, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.5217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.4864, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.4431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.4526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.4477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3930, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.4046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.4213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.4014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.4007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.5369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.3096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.3832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.4274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.4182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.3678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.4352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.4530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.4120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3561, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7213) loss tensor(0.4165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.4541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.3519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.4401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.4191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.6583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.4462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3042, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.4458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3766, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.5200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3801, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.4469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.3883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7101) loss tensor(0.4299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7080) loss tensor(0.4794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7082) loss tensor(0.3522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7112) loss tensor(0.3027, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7107) loss tensor(0.4564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.4106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.3705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3778, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.4124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3497, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.4193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.4062, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3516, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.5273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.4281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.4277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.4998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.3137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.3476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.4253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.4666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.4320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.4039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.4391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.3882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.4382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.4676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.4177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.4035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2901, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.4756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.3645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.4410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.5218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.4405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.3498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3715, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.4412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.2882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.4910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.4842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.4857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.5253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.4135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2904, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.5239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.4046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.2488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.4386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.4226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.5488, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7190) loss tensor(0.3968, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.2757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.2923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.2681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.4677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.5190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3743, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.2558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.4508, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.4199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.5632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.4132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.3654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.5475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.4618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4006, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.3323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4047, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.4078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.4596, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.2886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.3790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.4670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.4344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.4927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.3902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.5111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.4181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.6743, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.4717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.5314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.4762, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.4660, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.4068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.4252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.5317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.4861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.2637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4042, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.4667, grad_fn=<NllLossBackward>)\n",
      "epoch: 68\n",
      "accuracy: tensor(0.7195) loss tensor(0.4896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.3029, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.3554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.2822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.4319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.3583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.2538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.3181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.4256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.3629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.4731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.4861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.2837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.3192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.3917, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.2505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3801, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.4342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.2867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.4858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.2898, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.4247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3966, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.2928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.4530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3540, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7198) loss tensor(0.3624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.2717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.4422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.5528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3687, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3744, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.5110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.2946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.5792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.4650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.4162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.4222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.4600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.5447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.4482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.4386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.4973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.3199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.4387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.3573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.2991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.4275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.3770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.5166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.3727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.4224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.4496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.4259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.3048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.4046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.6331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.5202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.5071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.4757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.4100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.2922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.5077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.2906, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.5094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.4150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.4488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.5775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.4813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.3216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.3173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.2997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.4679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.5493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.2673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.2896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.3833, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.4806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.4170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.3015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.3322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.4083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.3870, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.4206, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7222) loss tensor(0.4891, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.4466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.4832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.2837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.5775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.4147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.4003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3911, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.5073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.2996, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.5265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.5295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3801, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.5698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.4264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.4729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.3466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.3987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.3703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.3844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.4556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.4795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.5356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.4383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.3006, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.3735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7095) loss tensor(0.2833, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7077) loss tensor(0.5115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7112) loss tensor(0.4404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.3381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.4450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.4139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.4459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.4933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3870, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.4335, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.4175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.4408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.4089, grad_fn=<NllLossBackward>)\n",
      "epoch: 69\n",
      "accuracy: tensor(0.7194) loss tensor(0.2681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.3595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.4079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.5257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.4366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.4350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.2349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.4297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.2598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3846, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.5788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.4250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.4371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.4836, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.2283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.4278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.2948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.4366, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7185) loss tensor(0.4042, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.4412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.2938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.3185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.3167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.4555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.4037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.5299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.4128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.2709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.2651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.4413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.4927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.4193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.2650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.5385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.3880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.4046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.4630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.3817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2778, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.5154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.4582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.3722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.4520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.3920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.4785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.4713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.4984, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.5405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.3668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.4237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.3520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.3487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.4150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.4888, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.2456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.4674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.4201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.4248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.3684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.3671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.4003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.4250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.3375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.3417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3019, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.5034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.3090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.4224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.2827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.3931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.3927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.3972, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.4472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.4282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.2726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.3459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.3366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.3065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.4259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.3881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.4108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.4081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3888, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3772, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.4403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.3825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.2699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.3648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.4086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.3942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.3218, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7186) loss tensor(0.4350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.3707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.3181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.4099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.4274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.2408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.3772, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.3492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.3687, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.4789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.2752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.3412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.3863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.4230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.4784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.4877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.3627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.3019, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.3052, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.4104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3040, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.4252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.5511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.4879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.2764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.3425, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.4477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.4157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.3886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.4156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3939, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.2519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.5256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.4171, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.5182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.5814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.4053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.4391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.2671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.3802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.2976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.5132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3826, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.4356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.2621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3403, grad_fn=<NllLossBackward>)\n",
      "epoch: 70\n",
      "accuracy: tensor(0.7211) loss tensor(0.3471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.4052, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.2910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.2709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.3278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.4223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.3521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.3831, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.4230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3029, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.4003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.3585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.4315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.2937, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7184) loss tensor(0.5055, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.3200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.3506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.4064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3932, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.5441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.5127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3753, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.4846, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.4925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3901, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2753, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.4157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.4048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.3247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.4225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.4193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.3427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.5091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.4758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.4584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.4077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.4341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.4474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.3977, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.3794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7136) loss tensor(0.3358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7125) loss tensor(0.3451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7146) loss tensor(0.3110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.2946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.2863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.4771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.4260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.4567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.3575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.3907, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.4964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.3965, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4479, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.3469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.3489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.2949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.2990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.4076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.5538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.4052, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.5321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.4806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.4635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.2959, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.2808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.4398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.3983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.4024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.4375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3596, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.3548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.4224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.3085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.3586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.5605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.4893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.4054, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3782, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2897, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7217) loss tensor(0.4524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3054, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.2795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.2591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.3381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.5306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.2479, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4006, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.4613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.4649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.4473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.3938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.4460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3836, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.4379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.4348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2897, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.2409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.3991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3897, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.3554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7129) loss tensor(0.3538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7128) loss tensor(0.3910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.4816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.5205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.3173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.3751, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.4813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.5350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.4765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.2981, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.3028, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.2793, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.4111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.2858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.4887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.4181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.2520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7143) loss tensor(0.3604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7122) loss tensor(0.3864, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.3674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.3965, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.2962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.4489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.3044, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2981, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.4145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.6013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.4205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.6032, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.4233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.4147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.3423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.3315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.4630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.3303, grad_fn=<NllLossBackward>)\n",
      "epoch: 71\n",
      "accuracy: tensor(0.7237) loss tensor(0.3954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.3436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.4106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.2839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.4131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.2858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.3798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.2718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.4004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.4312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.3409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.2474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.1921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.4389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.4413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.3853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.4376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2401, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7226) loss tensor(0.2662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.2966, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.2708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.2585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.3712, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.2817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.3468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.2840, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4701, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.2661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.2827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.2816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3062, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.3162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.3562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2924, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.3680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.4584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.2939, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2762, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.3017, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.4740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.2552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.4169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.2468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3019, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.3373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.4163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.3185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.3534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2937, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.4272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.3621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.2975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.2562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.4474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.3575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.2549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.2315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.3343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3513, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.3011, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.4401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.3271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.4727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.4297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.3427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.3758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.2787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.4447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.3705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.2920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.4646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.3369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.3972, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.2519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.4318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3622, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7194) loss tensor(0.3146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.4129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.4111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.5262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3904, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3400, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.4572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.4114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.4540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.4976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.4517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.3717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.3591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7138) loss tensor(0.4509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.4713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7145) loss tensor(0.3764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.4203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.2579, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.3553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.4394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3859, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.2018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.3320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.3218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.3627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.4470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.2423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7131) loss tensor(0.4949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7109) loss tensor(0.3652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.4609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7130) loss tensor(0.3528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7150) loss tensor(0.3620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.4142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3862, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.4582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.3646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.5048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.4889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.4278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.4238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.4393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.4049, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.2923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.2819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.4468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.4223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.4592, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.4546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.2983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.4274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.3762, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.4443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3005, grad_fn=<NllLossBackward>)\n",
      "epoch: 72\n",
      "accuracy: tensor(0.7225) loss tensor(0.2857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.4529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2999, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.4018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.4642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.4245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2967, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.3098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3966, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7206) loss tensor(0.3136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.2789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.4214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.4562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3947, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.4015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.2509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.3517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.4683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.5013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.4236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.4122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.4389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2941, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.3447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.4245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2966, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.2813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.4397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.3882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2968, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.3114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.4263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.4913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3051, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3448, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2977, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3049, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2011, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.4404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2914, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.2729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.2413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.4132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.4329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.4967, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.4089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.3596, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.4166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.2422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.3528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3848, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.4188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3897, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.3806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.4608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3012, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.2609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.4654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.4149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4016, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.4715, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.2281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.2830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.4278, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7150) loss tensor(0.2868, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.3211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.4558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.3482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.2462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.3648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.2656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.2796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.3357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7155) loss tensor(0.2911, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.3889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.3910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.4222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3042, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.4225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.4023, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.4729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.2912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.1982, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3871, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.2824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.4178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.4101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.4251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.2879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.3738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.3993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.2819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.4471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.4260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3826, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.5038, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.3857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.2546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.4889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.3704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.4113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3959, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.4375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.4109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.5550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.3535, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.4022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.4179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.4849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.4847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.3693, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.2652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.2663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.5633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.4348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3052, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.1789, grad_fn=<NllLossBackward>)\n",
      "epoch: 73\n",
      "accuracy: tensor(0.7211) loss tensor(0.3498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.4493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.2411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3054, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.4704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.3452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.3480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.4772, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.2569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3486, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7232) loss tensor(0.3343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.3522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.3676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.2012, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.2650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.5007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.2326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.2766, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.3138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.2535, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3622, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.1650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2888, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.4366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.4750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.4079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.1915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3845, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.2709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.2696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.2450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2935, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.4334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.3799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.3544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.3313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.4313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.2616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.4130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2859, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.4249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.3832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.4582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.3832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.3842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.5306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3497, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2780, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3020, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.3777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3851, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.2316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3970, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7227) loss tensor(0.3540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.3105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.3011, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.4483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.3724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.3751, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.4007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7149) loss tensor(0.3172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7139) loss tensor(0.4571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.4035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.3636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3006, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.3814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.2247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.4228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.4347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.4028, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.4326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.2984, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.2993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.2536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2982, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.4336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.5050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.3373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.4263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.3735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.4084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.3847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.2595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.2705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.2412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.1988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.5148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4016, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3027, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.4677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.3440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.4080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.5115, grad_fn=<NllLossBackward>)\n",
      "epoch: 74\n",
      "accuracy: tensor(0.7250) loss tensor(0.2332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3898, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.2947, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.2293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7126) loss tensor(0.3203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.3356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.3047, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.2281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.2848, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7215) loss tensor(0.2464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.3211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.3678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3937, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2972, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.2579, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.4343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3005, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.2639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.3564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.3085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.3502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.2677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.3155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.4068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.3151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.3063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.3122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.3368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.3542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.3764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.3190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.3549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2968, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.3557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.3480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2766, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.4142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.4645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.3771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.4651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.4004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.1873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.4440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2934, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.4308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.4042, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.3134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.3507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.4318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3259, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7235) loss tensor(0.2643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.4708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7161) loss tensor(0.2692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.2558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3952, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.2719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.4053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.2825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.4093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3940, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.1735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3596, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.4651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.4054, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2914, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.4803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2994, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.2302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.4303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.4594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.2732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.2582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.4303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.2948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.2639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.2877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.2472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.2601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2800, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2979, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2953, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.4612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.5003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.4186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.3862, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.2899, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7154) loss tensor(0.2480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.2810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.4621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.5601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.4233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7142) loss tensor(0.3273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7121) loss tensor(0.3964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7119) loss tensor(0.4214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.4174, grad_fn=<NllLossBackward>)\n",
      "epoch: 75\n",
      "accuracy: tensor(0.7171) loss tensor(0.5557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.2841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3115, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7227) loss tensor(0.4020, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3508, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.2968, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.3311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3025, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.3266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.3107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.2760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.1847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.2547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.2618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3307, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3023, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.4542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.2458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.4991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.3153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3959, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.2695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.2797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.3091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.2495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.2977, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.3166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.3134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.4272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.2830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.4041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2872, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.3987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.4313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3033, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.2236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.4408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.2967, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.3322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.4114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.4289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.4482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.4354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.4139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.4346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.2372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.3378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.3190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.4310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.3609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.3112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.4117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.4314, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7262) loss tensor(0.2130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2846, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.4389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.4429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.3494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.4037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.3931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.3163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.4038, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.3483, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.3740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.2838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.4167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.2905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3033, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2833, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.2412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.2847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.5082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.4009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.4510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.3021, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.3113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.3469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.4243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.3064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.4573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.4116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.4191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.4750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.4254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.3168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3031, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3972, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.3286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2870, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.4325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.3566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2979, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.3577, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.3168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.3741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.3425, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.2757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.2885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.2757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.2763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.3948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.4983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2801, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2571, grad_fn=<NllLossBackward>)\n",
      "epoch: 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7254) loss tensor(0.2923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2324, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2943, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.2244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.2782, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2958, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3028, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.2548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.2413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.1739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2508, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2836, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.3122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.4000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.4061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.2005, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.4735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.2303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.4132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.4450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.1911, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.1868, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.3500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.4547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.4300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.3023, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2899, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.3773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.3302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.3542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.3683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.3061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2870, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.3855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2980, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7249) loss tensor(0.3188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.3757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.3475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2773, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.4007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.3017, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2687, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.4032, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2972, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.2818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.2902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.2280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.4253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.3573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.2690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.2759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.2647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.3544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.4170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.3856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3779, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.2946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.3006, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3017, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.4355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.3867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.3608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.3567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2862, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.3782, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.4018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.3278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.2925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.4182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.2781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.2728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.2961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.4712, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.2954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2930, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2868, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.3127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.3825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.4434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2928, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7231) loss tensor(0.2550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2753, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.2696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2952, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.5806, grad_fn=<NllLossBackward>)\n",
      "epoch: 77\n",
      "accuracy: tensor(0.7232) loss tensor(0.3094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2895, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2772, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2959, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.3066, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3051, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.2916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.3117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3804, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3579, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3895, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.1955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.3671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.3511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.1990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.2904, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.2442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.3113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.2654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.2997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7163) loss tensor(0.2778, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.2992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.2002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.2478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.2747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.2441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2862, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.3115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2935, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.4162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3011, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.4413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.1956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.2662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.4415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3531, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7262) loss tensor(0.3339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2779, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2826, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3062, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.4045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.3528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.3071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.3339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.4404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.3071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.4101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.4328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.3096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3051, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.3166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.4737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2888, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.4523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.4236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.2494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7147) loss tensor(0.2858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.4767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.4510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.5968, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.2500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.3358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7135) loss tensor(0.2372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7133) loss tensor(0.2787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.5269, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.2980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.2965, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.3963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.3129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2937, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.4285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.3297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2779, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3801, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2895, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.3289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.3936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.3490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.3384, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7281) loss tensor(0.2734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2982, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.3322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.3357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2454, grad_fn=<NllLossBackward>)\n",
      "epoch: 78\n",
      "accuracy: tensor(0.7238) loss tensor(0.3037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.2816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.2400, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.3159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.2564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.2900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.2802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.3318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.2911, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.2674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.2979, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.2939, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.1848, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2425, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3687, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3217, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.2747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.2438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2953, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.2669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.2510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.4078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2906, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.3728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2687, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.2487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.3111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7335) loss tensor(0.2091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.2274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.3710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7336) loss tensor(0.2798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.2187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.2111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.3264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.3258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.3084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3947, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3023, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3917, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.3512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.3107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2479, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2335, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.4691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3032, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3246, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7230) loss tensor(0.2110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.3485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.3591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.4629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.4044, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2864, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2823, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.3274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.3655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.3787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.3342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.3437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.3074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2804, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.4043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.3669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.3020, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.3795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.3447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2982, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2793, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.3814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.4021, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.4096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.3235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.4640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7162) loss tensor(0.2903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.3313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2901, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.3147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3016, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.3623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.3270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.3016, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.4156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.2866, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7201) loss tensor(0.5085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.2629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.4397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.4199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.2395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.2924, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.3209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.2796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3740, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.1497, grad_fn=<NllLossBackward>)\n",
      "epoch: 79\n",
      "accuracy: tensor(0.7251) loss tensor(0.2240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2685, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.3209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.3116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.3330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2032, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.3151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.3175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.3332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.3463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2038, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.2216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.3124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7339) loss tensor(0.2792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.2519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.3108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.3652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2897, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3031, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.3478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.3512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.3479, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.1554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3069, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2535, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2904, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.3865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.3293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.3053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2867, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7254) loss tensor(0.3002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.3662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.3769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.2726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3935, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.2833, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.2687, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.3328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2715, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.3095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.4122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.4223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.4271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.3938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.3263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.3072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.3535, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.3247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.3109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2541, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.1774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.3328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2860, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2891, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.3318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2187, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7293) loss tensor(0.3045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2883, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.3554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.2600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7179) loss tensor(0.2495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.3187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.4143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.3329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.3746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.5017, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.2123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.3533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7151) loss tensor(0.3815, grad_fn=<NllLossBackward>)\n",
      "epoch: 80\n",
      "accuracy: tensor(0.7157) loss tensor(0.3666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.2504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3520, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.2823, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.1957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.2524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.2828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2066, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.3580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.4127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.2469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.3226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.3230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3751, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.1828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.1865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.1892, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.3147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3025, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2967, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.3273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2891, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.3460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2994, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2904, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2932, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.3723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.3173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.4119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2517, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2003, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7232) loss tensor(0.3015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.3550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.3090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.3415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2780, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.1815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2860, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.3354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.4825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3907, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.4069, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.3409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.3704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.3363, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.4074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.2737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.3589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.3042, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.4209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.3492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.3092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.1940, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.3055, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1965, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2869, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1806, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.5707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2592, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2944, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.3672, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2219, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7295) loss tensor(0.3113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.3458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.3433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.3152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2577, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2712, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.3143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2908, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.2437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.2647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.1994, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.2765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.4278, grad_fn=<NllLossBackward>)\n",
      "epoch: 81\n",
      "accuracy: tensor(0.7246) loss tensor(0.1905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1859, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2020, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.3788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1898, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2851, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2685, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.3200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2374, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.2543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.2149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.2664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7186) loss tensor(0.2007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.2244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.2263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.2856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.2717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2745, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2941, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.1614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.2544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.3317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.4077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.2927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.1856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.2455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.2743, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7165) loss tensor(0.2747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.2362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1959, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.2463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.2731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.2847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2224, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7287) loss tensor(0.2676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.3099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2546, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1860, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2051, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.3233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.2101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.3255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.3256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.3157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.2062, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3032, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3058, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.3224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.3024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.3684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.4216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.3453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.3227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2715, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.2418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.3174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2959, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3945, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.3153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.4285, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2047, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.4411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2960, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7278) loss tensor(0.2627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2871, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.1953, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.4124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1941, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2696, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2042, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.3300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3190, grad_fn=<NllLossBackward>)\n",
      "epoch: 82\n",
      "accuracy: tensor(0.7270) loss tensor(0.2299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.2395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7176) loss tensor(0.2017, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.3254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.2425, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.1765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.1784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2967, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.2508, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.1754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2899, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1840, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1872, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2965, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.3563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1939, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.1992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2758, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7248) loss tensor(0.2404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.3057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2868, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2851, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.4041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.2774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.3136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.4146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.2422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.2938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.3017, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.3000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.3312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2400, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3056, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2027, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.4080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.3170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.3707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.3624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.3436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.3462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.3717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.3067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.2674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.2377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.2166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.2683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.3325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2005, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.3057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1904, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1528, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.3174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.3432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.3614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.3015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.2617, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.3554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2353, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7265) loss tensor(0.2937, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1673, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.3153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.3161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.3332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.2801, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.3266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.3321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7182) loss tensor(0.2829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7173) loss tensor(0.3648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.2830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.2255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.2890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.2791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.3181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.1820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.3593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.3214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2077, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.1197, grad_fn=<NllLossBackward>)\n",
      "epoch: 83\n",
      "accuracy: tensor(0.7215) loss tensor(0.2015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.3462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.1701, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.3105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2859, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2766, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.1958, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.1985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.3042, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.3917, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1743, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2400, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.4539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2687, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2062, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2940, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.1726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.1758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2660, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7256) loss tensor(0.3178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1800, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1965, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.3314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2037, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2479, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.3006, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3012, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.1698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.1861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.1872, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.3481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2801, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2944, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.1737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.3388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.3249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2968, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.3595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1953, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.3640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1898, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.4569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.3892, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.3150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.3862, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.2356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.2648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.1950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7195) loss tensor(0.2812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.2378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.2874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.3726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7159) loss tensor(0.3511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7137) loss tensor(0.5648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.4227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.4485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7141) loss tensor(0.3185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.4092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.4390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.4557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.4693, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.4968, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7198) loss tensor(0.2459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.2717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.2266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.3488, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7171) loss tensor(0.2783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7169) loss tensor(0.3459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.2971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.3227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.4115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3032, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2845, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.3017, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.3578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.3960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.3872, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.4084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2977, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.3084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2335, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.4204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2836, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.3767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.2567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.3470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.2033, grad_fn=<NllLossBackward>)\n",
      "epoch: 84\n",
      "accuracy: tensor(0.7215) loss tensor(0.2377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.2409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.4047, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7170) loss tensor(0.2405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7168) loss tensor(0.2512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.2737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.3220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7175) loss tensor(0.4221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.2493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.2532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7178) loss tensor(0.2995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7157) loss tensor(0.3065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.3231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.2897, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.3322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.3014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3069, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.1900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.2794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.3244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.3334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3011, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.2578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.3185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.1861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3019, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.3043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.2178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2891, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.2649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.2798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.2987, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7178) loss tensor(0.2820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7184) loss tensor(0.2661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.2832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.1816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3058, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7185) loss tensor(0.2354, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.3361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.2306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.3808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2391, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.1607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.3204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.2367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.3025, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.3855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2836, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.1600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.1747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2339, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2497, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2027, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.1395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.2426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2530, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3044, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.3139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.1934, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.3064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.2524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1531, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7254) loss tensor(0.2467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.3312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2753, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2400, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.4021, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2772, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2269, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.3114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2056, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2864, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.3402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.1855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.4599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2981, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.1364, grad_fn=<NllLossBackward>)\n",
      "epoch: 85\n",
      "accuracy: tensor(0.7235) loss tensor(0.2066, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2020, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.1863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.3550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.2778, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1871, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2056, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.1832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.2355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.2598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.3238, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2691, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7217) loss tensor(0.2488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7160) loss tensor(0.2506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.2656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7152) loss tensor(0.3198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.3911, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7190) loss tensor(0.2535, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7194) loss tensor(0.3814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3032, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7187) loss tensor(0.3039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.4337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2939, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.3415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.3519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.2838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.3316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3745, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.1950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3019, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2066, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.2351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.3007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.3158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7199) loss tensor(0.4145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.2583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.2869, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.3267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7189) loss tensor(0.2727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7205) loss tensor(0.1922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.3101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2897, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.3474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2697, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.2581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.3361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.1804, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2166, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1930, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2782, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3038, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3385, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7274) loss tensor(0.2822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3016, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2054, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.3357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.3046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2697, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1914, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.3286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2800, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.3543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.3001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.1728, grad_fn=<NllLossBackward>)\n",
      "epoch: 86\n",
      "accuracy: tensor(0.7253) loss tensor(0.1539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2134, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2800, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1994, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.3013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.2076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2017, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3152, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1801, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2744, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2047, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2033, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1988, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7248) loss tensor(0.1875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.1995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.1628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2023, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2569, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.2822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.3255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2685, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1837, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1981, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2671, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.3210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1917, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2804, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.4437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2634, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2930, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.3179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2871, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.3018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1870, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2895, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2730, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2645, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2198, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.3220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2301, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7282) loss tensor(0.2002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1896, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.3771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.1933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.2734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.3193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.3092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1892, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.3104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.3131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2176, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2456, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.3142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2450, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.2139, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.3014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.3116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2089, grad_fn=<NllLossBackward>)\n",
      "epoch: 87\n",
      "accuracy: tensor(0.7210) loss tensor(0.2978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1989, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1952, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.1941, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1533, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1826, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2190, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1785, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.3072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.2422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.1416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.2281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.2292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2488, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.3081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.1627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.2949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.1955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.1519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.3501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1805, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7267) loss tensor(0.2117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1618, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.3771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1937, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2234, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.3064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.3162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1862, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.2072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.2769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2891, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2414, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3020, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2660, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2687, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1999, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.2734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2040, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.3099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1941, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.3457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.2525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2392, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7314) loss tensor(0.1875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.2433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.2399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.2481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.1332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.2369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.2452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.2712, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.2287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.2835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.2158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2218, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2944, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1868, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.3786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.2388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.2346, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.2449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.2121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.2169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.3013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.3118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2301, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.2232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.3250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.2383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2372, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.3330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1937, grad_fn=<NllLossBackward>)\n",
      "epoch: 88\n",
      "accuracy: tensor(0.7281) loss tensor(0.2186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2845, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.2017, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.1308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.2151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1930, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.3094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2031, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1780, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1822, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1989, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2118, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7263) loss tensor(0.3076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.0945, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2772, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.1856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.2964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.3063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.3614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2471, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.1967, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.1879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.1962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1891, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.1738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.3181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2040, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.3531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1979, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2721, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2269, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1693, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2741, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.1590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.1713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2871, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2982, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.1462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.1642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3032, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1923, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.3091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3200, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1947, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2693, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.3642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.1977, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2348, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.3303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.2170, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.1948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.2793, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2637, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7298) loss tensor(0.3179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.3168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1981, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.2028, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.2710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2049, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.1800, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2055, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1840, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.4131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2290, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.3212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.1990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.3155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.3012, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.3482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.3873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.1860, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2463, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2629, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.3799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.3261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.2019, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.2258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.1582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.2819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.2633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.3287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.2788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.2142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.2243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.2669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.2440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.2385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2907, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1888, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.1727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2486, grad_fn=<NllLossBackward>)\n",
      "epoch: 89\n",
      "accuracy: tensor(0.7302) loss tensor(0.1677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.2399, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.2102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2070, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.2222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2579, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2860, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1788, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7278) loss tensor(0.2659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2021, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2424, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.3000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2389, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1968, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.1812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2243, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2029, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1801, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.2225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.2620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1943, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2712, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.3036, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2111, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2945, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.1950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1582, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2154, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.2188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.2222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.2522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1892, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2056, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.3632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2066, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1523, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2334, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1981, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1672, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.2718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.2744, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2033, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.2539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2945, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.2366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.2145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2838, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7275) loss tensor(0.2360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1945, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2227, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1762, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1680, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.2029, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7331) loss tensor(0.1795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.1667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.1526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1953, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2930, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.2315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1782, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.2230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2011, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2050, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.3178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.3002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.3192, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.3001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.2791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.2033, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.3384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.2958, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.2115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.3185, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.2303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.2160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1867, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1987, grad_fn=<NllLossBackward>)\n",
      "epoch: 90\n",
      "accuracy: tensor(0.7280) loss tensor(0.2411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2011, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1848, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.1633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.1497, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.2072, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7298) loss tensor(0.2141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1984, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.3048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2892, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1868, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2793, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2362, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2028, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.2063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.1512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.2314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.2464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.1914, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.2316, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2163, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.2704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.1685, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.1766, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.1992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.1737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2330, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.2742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.1902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.2338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7333) loss tensor(0.1999, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.2486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.1446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.1457, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.2360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1977, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.2475, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.1652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.1909, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2606, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2100, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.2038, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2053, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.2830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.1622, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.2623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2207, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.2076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.1549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.2409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1907, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2108, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.3280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.1617, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2304, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2462, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7304) loss tensor(0.3010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.3578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.3220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.2609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.2662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.2575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7171) loss tensor(0.2161, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7181) loss tensor(0.2064, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7192) loss tensor(0.2641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.3120, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2492, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2542, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2554, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2994, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2872, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.3109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.3666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.3720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.3260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7166) loss tensor(0.2705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7158) loss tensor(0.2477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7144) loss tensor(0.2698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7183) loss tensor(0.2824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.3160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3012, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.4048, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.3282, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2852, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2737, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.2057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1979, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2899, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.3197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.3225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.3010, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.3039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.1412, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2743, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1813, grad_fn=<NllLossBackward>)\n",
      "epoch: 91\n",
      "accuracy: tensor(0.7307) loss tensor(0.2130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2662, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7331) loss tensor(0.1949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1144, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.3278, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.3437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1568, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2119, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2082, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2225, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7295) loss tensor(0.2157, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2052, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.1684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.1975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.2097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2592, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.3102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1817, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1966, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1845, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.3946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2593, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.1527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1876, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2804, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1846, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.2675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.2068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.1288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.1317, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.2376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2058, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1999, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2782, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2336, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.1915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1935, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.1950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.2201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.3127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2540, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2930, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1928, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2058, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2162, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1560, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7247) loss tensor(0.2816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.3062, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2182, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.3760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.2715, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.3870, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.2495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.3633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.4021, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.3340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1850, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2666, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2220, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.3148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.4013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.2543, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2899, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2175, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1984, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.1716, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3714, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.3458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2033, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.3266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.1926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.1731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.1864, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.3115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.1859, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3050, grad_fn=<NllLossBackward>)\n",
      "epoch: 92\n",
      "accuracy: tensor(0.7242) loss tensor(0.2682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.2106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7206) loss tensor(0.1881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.1105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.1691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.1936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.1988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.1878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.2141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1999, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1897, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1830, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.1848, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7210) loss tensor(0.2024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.1527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.1638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2156, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1678, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1851, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2553, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2208, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1620, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1744, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1660, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1141, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2015, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1914, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1395, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1801, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2223, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.2692, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.1614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2028, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1780, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2465, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2349, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.3043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2051, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1840, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.2276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1608, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.1775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2975, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1771, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2868, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.1807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1610, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1531, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2300, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.2055, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1648, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2981, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.2168, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1968, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1429, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7282) loss tensor(0.2500, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.2752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.1360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.2094, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.2312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.3205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2172, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1905, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1924, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.2458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.1731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1617, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.2370, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2772, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1086, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2084, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2365, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1753, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1780, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1277, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.3057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1797, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.3049, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1934, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1765, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1768, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2063, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.1924, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.1963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2626, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2013, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2354, grad_fn=<NllLossBackward>)\n",
      "epoch: 93\n",
      "accuracy: tensor(0.7274) loss tensor(0.1452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2432, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2622, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1150, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2047, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1541, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7275) loss tensor(0.1912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2209, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.2199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.3151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2419, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2105, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2184, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.2522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2406, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2092, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1989, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1706, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1438, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1423, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.2901, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1727, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1619, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1770, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1561, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2056, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2038, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2140, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1937, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1415, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1749, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2418, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.3121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1661, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1494, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2574, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2493, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1780, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1924, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2036, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1687, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2174, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7299) loss tensor(0.3292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.1966, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1823, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1810, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1941, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1042, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7342) loss tensor(0.2044, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1930, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2151, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2222, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1675, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.1563, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2180, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.3935, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.2751, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.2486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2051, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.1565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1794, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.3296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2124, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1939, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1827, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2411, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1995, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1525, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2219, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.3113, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.2216, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2901, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1943, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2128, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.1846, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7344) loss tensor(0.2351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.2557, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7336) loss tensor(0.1712, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1954, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2248, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2690, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.3177, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2667, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1617, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.3041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2122, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2375, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.1605, grad_fn=<NllLossBackward>)\n",
      "epoch: 94\n",
      "accuracy: tensor(0.7217) loss tensor(0.1435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7214) loss tensor(0.2246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.1969, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.1558, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7201) loss tensor(0.1587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.1637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.2566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.1974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3123, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2109, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1211, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1859, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.3073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1978, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1774, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1869, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1976, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.0942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.2417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2338, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1856, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1758, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1815, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.1567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.1889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.0926, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1291, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1323, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1195, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2502, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1869, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1953, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.1490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.1584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.1838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2576, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1509, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2033, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.1587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.2551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7339) loss tensor(0.1495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.1565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.1970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.1984, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1859, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2403, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1782, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1982, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1658, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1674, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1466, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2076, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2028, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7305) loss tensor(0.1630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2017, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1536, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1892, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1857, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2342, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2449, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1769, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2231, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2572, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.1708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7216) loss tensor(0.1816, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7197) loss tensor(0.2117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7191) loss tensor(0.2496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.1958, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.2310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.1642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2234, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1811, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1622, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2125, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1234, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.1805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1911, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.2205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2697, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2408, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1510, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.1813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2138, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.3462, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.2654, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7203) loss tensor(0.3132, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.3367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.1798, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7177) loss tensor(0.1823, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7118) loss tensor(0.3236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7134) loss tensor(0.3744, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7153) loss tensor(0.3478, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7174) loss tensor(0.3957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.2435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.3136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.2559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.1961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.1970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7201) loss tensor(0.2253, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.3652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.3159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.2319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2202, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2383, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2127, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2158, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.2400, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.2034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2298, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.1894, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.3212, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2668, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2483, grad_fn=<NllLossBackward>)\n",
      "epoch: 95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7250) loss tensor(0.1983, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.2321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1685, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2135, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1823, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2174, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1991, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.1196, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.1950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1979, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1804, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1980, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2136, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1516, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1808, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2046, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1895, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2247, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1972, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.1964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.1490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7351) loss tensor(0.2230, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7344) loss tensor(0.1994, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7336) loss tensor(0.2314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7350) loss tensor(0.2160, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.1153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.1776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1757, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2237, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2224, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2369, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7222) loss tensor(0.2079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2371, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2071, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7219) loss tensor(0.2116, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.3809, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1433, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2868, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2017, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1729, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1326, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1754, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1429, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.1960, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.1720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.1242, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.1914, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1942, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1427, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.2257, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2426, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1691, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2328, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7282) loss tensor(0.1767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1953, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1482, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1947, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2089, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2167, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1594, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1467, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1341, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.3360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1866, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2397, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2777, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2101, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1911, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1892, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1707, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1143, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1689, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1441, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1470, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2359, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.1331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.1059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2251, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1892, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1725, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1420, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2272, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1955, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1584, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1916, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2455, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1839, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1434, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.2004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1547, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1912, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1379, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1862, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2421, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1724, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.1880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.1485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1793, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2321, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2259, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2225, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2239, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.3229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.2579, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1804, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1786, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.2515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.2079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7342) loss tensor(0.1069, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.2820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.1747, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.1984, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.0881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.1862, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.2416, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.2203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.2514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.3001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.2351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.1503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.1933, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.1614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.2214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7358) loss tensor(0.1734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7353) loss tensor(0.1931, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7333) loss tensor(0.2213, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.1550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7347) loss tensor(0.2681, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7360) loss tensor(0.2055, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.2470, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7329) loss tensor(0.2104, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.2343, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.1921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.1987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.2181, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7330) loss tensor(0.1409, grad_fn=<NllLossBackward>)\n",
      "epoch: 96\n",
      "accuracy: tensor(0.7322) loss tensor(0.2098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7331) loss tensor(0.2289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7351) loss tensor(0.1637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.2025, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1480, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1922, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1744, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2577, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2245, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1686, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2234, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1957, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1393, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1519, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.2297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1968, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1939, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1702, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1501, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.2260, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.1165, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.1722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.1819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.1641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.1636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.2292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7336) loss tensor(0.1511, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.2075, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7333) loss tensor(0.1601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.2532, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.1887, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.1752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1756, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1571, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1385, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2590, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1308, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.1748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2115, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1442, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1270, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1575, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1613, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1319, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1959, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1953, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1803, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1353, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.1821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.1888, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2704, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2221, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.2410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.1886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1669, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1024, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1258, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1759, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7290) loss tensor(0.2581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1337, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1103, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1524, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.2296, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7339) loss tensor(0.1405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2173, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7331) loss tensor(0.1792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.1683, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7339) loss tensor(0.2072, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.1748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.1987, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.1720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2009, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1761, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1864, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.1262, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1587, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.1695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.1824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.2264, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7336) loss tensor(0.1865, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7344) loss tensor(0.2095, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7341) loss tensor(0.2189, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2315, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.1254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.2961, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2146, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1711, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1984, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1999, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2286, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2142, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.1663, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7337) loss tensor(0.1361, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7344) loss tensor(0.2118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7346) loss tensor(0.2422, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7342) loss tensor(0.2194, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7342) loss tensor(0.2834, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7335) loss tensor(0.2169, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.2068, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.1840, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1567, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1649, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1604, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.1183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.1885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1287, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1703, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.1498, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7336) loss tensor(0.1107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7334) loss tensor(0.1643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7343) loss tensor(0.2405, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.2233, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7329) loss tensor(0.1652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2440, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2491, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1698, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2695, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.1709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.1723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.1914, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7211) loss tensor(0.3147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.2276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1927, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1310, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.1903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1946, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1647, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2847, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.2646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2294, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2392, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2267, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2649, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7273) loss tensor(0.1900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2340, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2114, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2472, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2366, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2617, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1793, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.1780, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.1776, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2458, grad_fn=<NllLossBackward>)\n",
      "epoch: 97\n",
      "accuracy: tensor(0.7243) loss tensor(0.1783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2149, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7200) loss tensor(0.2133, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.1925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.1984, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2655, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7167) loss tensor(0.2159, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2036, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1775, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1932, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.2014, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2011, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.2005, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.1875, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.1350, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2007, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1155, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1784, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.3073, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1628, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2065, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.1874, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.1943, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.1589, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1789, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2767, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1489, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1401, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1538, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1795, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1813, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1919, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1722, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.3042, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1893, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.2031, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.1750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.2126, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.1550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1535, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1460, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.1327, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7243) loss tensor(0.1322, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1821, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1384, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1709, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.1428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.1889, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1642, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1505, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.3045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1534, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.1378, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2244, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2216, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7259) loss tensor(0.1962, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1832, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1760, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1284, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2067, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2153, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1417, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7213) loss tensor(0.2746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.2280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.1877, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7207) loss tensor(0.1793, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2044, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7209) loss tensor(0.1664, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.1778, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2848, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.1831, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.1477, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7227) loss tensor(0.2057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.2074, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2147, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1854, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2020, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2694, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1373, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1232, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1249, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1565, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1413, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1941, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.1585, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1705, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.1512, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1632, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.1586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2507, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1297, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1835, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.3312, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1526, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1495, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1599, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1932, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.2522, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1600, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1862, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1352, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2191, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1818, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2989, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1598, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1941, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1948, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2313, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2078, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.2280, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.2479, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.2289, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1783, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1779, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1799, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1539, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.3060, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1630, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2032, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2281, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1544, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1639, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1241, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2486, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.2578, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2117, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.2240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2275, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2080, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2972, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.1570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.1461, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.2458, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7210) loss tensor(0.1439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2452, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.2437, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1958, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1325, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1755, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2254, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2518, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1993, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1496, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7238) loss tensor(0.1873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2459, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1314, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1203, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.2000, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.2752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2351, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1988, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.2121, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1930, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2355, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2332, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2235, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2311, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1796, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2866, grad_fn=<NllLossBackward>)\n",
      "epoch: 98\n",
      "accuracy: tensor(0.7223) loss tensor(0.2043, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.1611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.1790, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.1950, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1684, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1602, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2271, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1712, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.1586, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.0952, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.1878, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.1997, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1603, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.1682, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2001, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1328, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1615, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1994, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1807, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1884, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1882, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1802, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2780, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1743, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1545, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.0901, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1871, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.1597, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7233) loss tensor(0.1869, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7232) loss tensor(0.1621, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.2088, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1409, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1299, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1246, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1268, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1588, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1699, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1907, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1454, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1118, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1410, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1819, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1360, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2210, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.3110, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7246) loss tensor(0.1853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1034, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2199, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1564, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2394, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.2853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1833, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1805, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.2107, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.1601, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7301) loss tensor(0.2344, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1679, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1985, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1595, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2364, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2087, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1736, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1481, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1640, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.2041, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1710, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1986, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.2201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1659, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7298) loss tensor(0.1738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2079, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1913, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7321) loss tensor(0.1358, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7322) loss tensor(0.1735, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1970, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2081, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1738, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1720, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2179, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.2436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1644, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1967, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7328) loss tensor(0.1936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.1659, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2085, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2256, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1306, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1723, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1624, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1734, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1357, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7296) loss tensor(0.1990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2635, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1283, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1677, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1733, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1861, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1435, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2549, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7327) loss tensor(0.1570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1891, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1828, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7304) loss tensor(0.1439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1388, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1573, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2059, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1881, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7305) loss tensor(0.1614, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7310) loss tensor(0.1804, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1560, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2404, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.1443, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2112, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.2506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7217) loss tensor(0.0885, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.2764, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.1812, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7193) loss tensor(0.2551, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.1825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1787, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1956, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2004, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1851, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1989, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.0964, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1759, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2131, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.2130, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.3028, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1752, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.1402, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7314) loss tensor(0.2305, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1748, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1083, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1934, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1751, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7250) loss tensor(0.2008, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1226, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1855, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1869, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2097, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2763, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2732, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1646, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.3021, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1728, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1782, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1610, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.1637, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1506, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.1605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1205, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1979, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.2186, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.2018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7326) loss tensor(0.3214, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1831, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.1992, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7319) loss tensor(0.1657, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7325) loss tensor(0.1656, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7338) loss tensor(0.1991, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7350) loss tensor(0.2490, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.2183, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.2293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.3028, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2772, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1428, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7229) loss tensor(0.2829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.3240, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.2106, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.2731, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.1895, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2700, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.2380, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.3607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.3099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1998, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1853, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.2206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2178, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.3556, grad_fn=<NllLossBackward>)\n",
      "epoch: 99\n",
      "accuracy: tensor(0.7274) loss tensor(0.1638, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.2228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1382, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.2229, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.1252, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1873, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.1476, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.0953, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1676, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1451, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2396, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1742, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7272) loss tensor(0.1843, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1860, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.2555, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7315) loss tensor(0.1096, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.2033, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1697, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.2515, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1333, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1688, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1890, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.2236, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1713, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1469, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2061, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.2099, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1908, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1653, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.1496, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1715, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1377, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7264) loss tensor(0.2329, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2788, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1838, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.1973, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7215) loss tensor(0.2318, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7202) loss tensor(0.2562, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.1858, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7224) loss tensor(0.1650, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.1633, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.1583, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.1607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7235) loss tensor(0.1880, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.2035, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1368, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1651, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.2002, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1918, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1840, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.2020, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7323) loss tensor(0.0902, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1038, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1197, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.2098, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1188, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1487, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1623, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1596, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1963, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1559, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1193, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1398, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2445, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2302, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1137, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7249) loss tensor(0.1090, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1616, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1726, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7238) loss tensor(0.2514, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.1849, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1820, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7245) loss tensor(0.1293, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.1376, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7226) loss tensor(0.1792, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.1951, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.2288, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.1829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1670, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1550, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1558, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1644, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7271) loss tensor(0.1791, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1953, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1739, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.2045, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7231) loss tensor(0.1886, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.1537, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.2164, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1057, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7299) loss tensor(0.1400, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1521, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1027, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1387, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1617, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1938, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.2039, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1276, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7317) loss tensor(0.1609, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7302) loss tensor(0.1607, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.2025, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1566, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.1863, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1750, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1228, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1390, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1347, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7318) loss tensor(0.1263, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7320) loss tensor(0.1345, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1464, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7311) loss tensor(0.2215, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7297) loss tensor(0.1320, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7303) loss tensor(0.1444, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7313) loss tensor(0.2148, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7312) loss tensor(0.1915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1612, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1407, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7286) loss tensor(0.1250, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1367, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1925, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1718, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1717, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1708, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7267) loss tensor(0.1611, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7274) loss tensor(0.1899, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1868, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1580, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.1529, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1292, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.2145, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7270) loss tensor(0.1910, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2030, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.2091, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1844, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7306) loss tensor(0.1552, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7307) loss tensor(0.1381, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7309) loss tensor(0.1929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1719, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7275) loss tensor(0.1303, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1936, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7255) loss tensor(0.1430, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7242) loss tensor(0.1903, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.1356, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.3022, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1625, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7295) loss tensor(0.1261, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.2814, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1436, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1920, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1900, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7282) loss tensor(0.1499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2025, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1295, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.2187, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.2201, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1453, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1446, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2026, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.1548, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7258) loss tensor(0.2023, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2018, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7259) loss tensor(0.2447, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7234) loss tensor(0.1652, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7237) loss tensor(0.1016, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7208) loss tensor(0.1581, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7223) loss tensor(0.1527, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7239) loss tensor(0.1627, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7221) loss tensor(0.2102, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7218) loss tensor(0.1842, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7225) loss tensor(0.2204, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7230) loss tensor(0.1829, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7240) loss tensor(0.1990, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1484, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.2003, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.2556, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7253) loss tensor(0.1641, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7257) loss tensor(0.1746, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7256) loss tensor(0.1971, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.2309, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.2331, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1879, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.1570, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7241) loss tensor(0.1643, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7248) loss tensor(0.1825, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1474, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7251) loss tensor(0.1503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7269) loss tensor(0.1255, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.2279, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.1631, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7283) loss tensor(0.1386, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1265, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1929, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1667, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: tensor(0.7273) loss tensor(0.2845, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.1431, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7293) loss tensor(0.1468, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7290) loss tensor(0.2093, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1473, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7279) loss tensor(0.1503, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7289) loss tensor(0.1665, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7294) loss tensor(0.1605, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7287) loss tensor(0.1129, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7298) loss tensor(0.2591, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7280) loss tensor(0.1921, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7285) loss tensor(0.1892, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1485, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7265) loss tensor(0.2273, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1841, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7278) loss tensor(0.1335, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7271) loss tensor(0.1274, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7247) loss tensor(0.1266, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.0915, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7273) loss tensor(0.2499, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7288) loss tensor(0.1824, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1439, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7266) loss tensor(0.2206, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7291) loss tensor(0.1949, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7281) loss tensor(0.1994, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7277) loss tensor(0.1636, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7262) loss tensor(0.1504, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7254) loss tensor(0.1974, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7261) loss tensor(0.1781, grad_fn=<NllLossBackward>)\n",
      "accuracy: tensor(0.7263) loss tensor(0.2696, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "test_accurucy_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(\"epoch:\", epoch)\n",
    "    order = np.random.permutation(len(X_train))\n",
    "    for start_index in range(0, len(X_train), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_indexes = order[start_index:start_index+batch_size]\n",
    "        \n",
    "        X_batch = X_train[batch_indexes].to(device)\n",
    "        y_batch = y_train[batch_indexes].to(device)\n",
    "\n",
    "        preds = model.forward(X_batch)\n",
    "        \n",
    "        loss_value = loss(preds, y_batch)\n",
    "        loss_value.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        test_preds = model.forward(X_test)\n",
    "        test_loss_history.append(loss(test_preds, y_test).data.cpu())\n",
    "        \n",
    "        accuracy = (test_preds.argmax(dim=1) == y_test).float().mean().data.cpu()\n",
    "        test_accurucy_history.append(accuracy)\n",
    "        \n",
    "        print(\"accuracy:\", accuracy, \"loss\", loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_accurucy_history)\n",
    "plt.title(\"History\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_loss_history)\n",
    "plt.title(\"LOST\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"lost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Convert number to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_type_dict_T = {}\n",
    "for key, value in article_type_dict.items():\n",
    "    article_type_dict_T[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flip Flops'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_type_dict_T[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_type_dict['Flip Flops']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flip Flops The number is 11\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X_test[1][0])\n",
    "plt.show()\n",
    "print(article_type_dict_T[int(y_test[1].numpy())], \"The number is\",y_test[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = X_test[1]\n",
    "test = test.unsqueeze_(0)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.forward(test)\n",
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How we see we get same example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./LeNet5.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LeNet5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = torch.nn.CrossEntropyLoss()\n",
    "optimizer2 = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint2 = torch.load(PATH)\n",
    "model2.load_state_dict(checkpoint2['model_state_dict'])\n",
    "optimizer2.load_state_dict(checkpoint2['optimizer_state_dict'])\n",
    "epoch2 = checkpoint2['epoch']\n",
    "loss2 = checkpoint2['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.forward(test).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
